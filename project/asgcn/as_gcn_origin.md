## origin train cross_sub

```
/root/anaconda3/envs/asgcn/bin/python /root/AS-GCN/main.py recognition -c config/as_gcn/ntu-xsub/train.yaml --device 1 --batch_size 4
/root/AS-GCN/processor/io.py:34: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  default_arg = yaml.load(f)
/root/AS-GCN/net/utils/adj_learn.py:18: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)
  offdiag_indices = (ones - eye).nonzero().t()
[06.22.21|09:19:58] Load weights from ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch9_model1.pt.
[06.22.21|09:19:58] Load weights [A].
[06.22.21|09:19:58] Load weights [data_bn.weight].
[06.22.21|09:19:58] Load weights [data_bn.bias].
[06.22.21|09:19:58] Load weights [data_bn.running_mean].
[06.22.21|09:19:58] Load weights [data_bn.running_var].
[06.22.21|09:19:58] Load weights [data_bn.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_0.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [class_layer_0.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [class_layer_0.tcn.0.weight].
[06.22.21|09:19:58] Load weights [class_layer_0.tcn.0.bias].
[06.22.21|09:19:58] Load weights [class_layer_0.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_0.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [class_layer_0.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_0.tcn.2.weight].
[06.22.21|09:19:58] Load weights [class_layer_0.tcn.2.bias].
[06.22.21|09:19:58] Load weights [class_layer_0.tcn.3.weight].
[06.22.21|09:19:58] Load weights [class_layer_0.tcn.3.bias].
[06.22.21|09:19:58] Load weights [class_layer_0.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_0.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [class_layer_0.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_1.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [class_layer_1.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [class_layer_1.tcn.0.weight].
[06.22.21|09:19:58] Load weights [class_layer_1.tcn.0.bias].
[06.22.21|09:19:58] Load weights [class_layer_1.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_1.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [class_layer_1.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_1.tcn.2.weight].
[06.22.21|09:19:58] Load weights [class_layer_1.tcn.2.bias].
[06.22.21|09:19:58] Load weights [class_layer_1.tcn.3.weight].
[06.22.21|09:19:58] Load weights [class_layer_1.tcn.3.bias].
[06.22.21|09:19:58] Load weights [class_layer_1.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_1.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [class_layer_1.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_2.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [class_layer_2.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [class_layer_2.tcn.0.weight].
[06.22.21|09:19:58] Load weights [class_layer_2.tcn.0.bias].
[06.22.21|09:19:58] Load weights [class_layer_2.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_2.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [class_layer_2.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_2.tcn.2.weight].
[06.22.21|09:19:58] Load weights [class_layer_2.tcn.2.bias].
[06.22.21|09:19:58] Load weights [class_layer_2.tcn.3.weight].
[06.22.21|09:19:58] Load weights [class_layer_2.tcn.3.bias].
[06.22.21|09:19:58] Load weights [class_layer_2.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_2.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [class_layer_2.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_3.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [class_layer_3.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [class_layer_3.tcn.0.weight].
[06.22.21|09:19:58] Load weights [class_layer_3.tcn.0.bias].
[06.22.21|09:19:58] Load weights [class_layer_3.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_3.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [class_layer_3.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_3.tcn.2.weight].
[06.22.21|09:19:58] Load weights [class_layer_3.tcn.2.bias].
[06.22.21|09:19:58] Load weights [class_layer_3.tcn.3.weight].
[06.22.21|09:19:58] Load weights [class_layer_3.tcn.3.bias].
[06.22.21|09:19:58] Load weights [class_layer_3.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_3.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [class_layer_3.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_3.residual.0.weight].
[06.22.21|09:19:58] Load weights [class_layer_3.residual.0.bias].
[06.22.21|09:19:58] Load weights [class_layer_3.residual.1.weight].
[06.22.21|09:19:58] Load weights [class_layer_3.residual.1.bias].
[06.22.21|09:19:58] Load weights [class_layer_3.residual.1.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_3.residual.1.running_var].
[06.22.21|09:19:58] Load weights [class_layer_3.residual.1.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_4.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [class_layer_4.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [class_layer_4.tcn.0.weight].
[06.22.21|09:19:58] Load weights [class_layer_4.tcn.0.bias].
[06.22.21|09:19:58] Load weights [class_layer_4.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_4.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [class_layer_4.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_4.tcn.2.weight].
[06.22.21|09:19:58] Load weights [class_layer_4.tcn.2.bias].
[06.22.21|09:19:58] Load weights [class_layer_4.tcn.3.weight].
[06.22.21|09:19:58] Load weights [class_layer_4.tcn.3.bias].
[06.22.21|09:19:58] Load weights [class_layer_4.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_4.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [class_layer_4.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_5.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [class_layer_5.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [class_layer_5.tcn.0.weight].
[06.22.21|09:19:58] Load weights [class_layer_5.tcn.0.bias].
[06.22.21|09:19:58] Load weights [class_layer_5.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_5.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [class_layer_5.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_5.tcn.2.weight].
[06.22.21|09:19:58] Load weights [class_layer_5.tcn.2.bias].
[06.22.21|09:19:58] Load weights [class_layer_5.tcn.3.weight].
[06.22.21|09:19:58] Load weights [class_layer_5.tcn.3.bias].
[06.22.21|09:19:58] Load weights [class_layer_5.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_5.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [class_layer_5.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_6.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [class_layer_6.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [class_layer_6.tcn.0.weight].
[06.22.21|09:19:58] Load weights [class_layer_6.tcn.0.bias].
[06.22.21|09:19:58] Load weights [class_layer_6.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_6.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [class_layer_6.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_6.tcn.2.weight].
[06.22.21|09:19:58] Load weights [class_layer_6.tcn.2.bias].
[06.22.21|09:19:58] Load weights [class_layer_6.tcn.3.weight].
[06.22.21|09:19:58] Load weights [class_layer_6.tcn.3.bias].
[06.22.21|09:19:58] Load weights [class_layer_6.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_6.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [class_layer_6.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_6.residual.0.weight].
[06.22.21|09:19:58] Load weights [class_layer_6.residual.0.bias].
[06.22.21|09:19:58] Load weights [class_layer_6.residual.1.weight].
[06.22.21|09:19:58] Load weights [class_layer_6.residual.1.bias].
[06.22.21|09:19:58] Load weights [class_layer_6.residual.1.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_6.residual.1.running_var].
[06.22.21|09:19:58] Load weights [class_layer_6.residual.1.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_7.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [class_layer_7.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [class_layer_7.tcn.0.weight].
[06.22.21|09:19:58] Load weights [class_layer_7.tcn.0.bias].
[06.22.21|09:19:58] Load weights [class_layer_7.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_7.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [class_layer_7.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_7.tcn.2.weight].
[06.22.21|09:19:58] Load weights [class_layer_7.tcn.2.bias].
[06.22.21|09:19:58] Load weights [class_layer_7.tcn.3.weight].
[06.22.21|09:19:58] Load weights [class_layer_7.tcn.3.bias].
[06.22.21|09:19:58] Load weights [class_layer_7.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_7.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [class_layer_7.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_8.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [class_layer_8.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [class_layer_8.tcn.0.weight].
[06.22.21|09:19:58] Load weights [class_layer_8.tcn.0.bias].
[06.22.21|09:19:58] Load weights [class_layer_8.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_8.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [class_layer_8.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [class_layer_8.tcn.2.weight].
[06.22.21|09:19:58] Load weights [class_layer_8.tcn.2.bias].
[06.22.21|09:19:58] Load weights [class_layer_8.tcn.3.weight].
[06.22.21|09:19:58] Load weights [class_layer_8.tcn.3.bias].
[06.22.21|09:19:58] Load weights [class_layer_8.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [class_layer_8.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [class_layer_8.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_0.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [recon_layer_0.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [recon_layer_0.tcn.0.weight].
[06.22.21|09:19:58] Load weights [recon_layer_0.tcn.0.bias].
[06.22.21|09:19:58] Load weights [recon_layer_0.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_0.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_0.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_0.tcn.2.weight].
[06.22.21|09:19:58] Load weights [recon_layer_0.tcn.2.bias].
[06.22.21|09:19:58] Load weights [recon_layer_0.tcn.3.weight].
[06.22.21|09:19:58] Load weights [recon_layer_0.tcn.3.bias].
[06.22.21|09:19:58] Load weights [recon_layer_0.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_0.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_0.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_0.residual.0.weight].
[06.22.21|09:19:58] Load weights [recon_layer_0.residual.0.bias].
[06.22.21|09:19:58] Load weights [recon_layer_0.residual.1.weight].
[06.22.21|09:19:58] Load weights [recon_layer_0.residual.1.bias].
[06.22.21|09:19:58] Load weights [recon_layer_0.residual.1.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_0.residual.1.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_0.residual.1.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_1.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [recon_layer_1.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [recon_layer_1.tcn.0.weight].
[06.22.21|09:19:58] Load weights [recon_layer_1.tcn.0.bias].
[06.22.21|09:19:58] Load weights [recon_layer_1.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_1.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_1.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_1.tcn.2.weight].
[06.22.21|09:19:58] Load weights [recon_layer_1.tcn.2.bias].
[06.22.21|09:19:58] Load weights [recon_layer_1.tcn.3.weight].
[06.22.21|09:19:58] Load weights [recon_layer_1.tcn.3.bias].
[06.22.21|09:19:58] Load weights [recon_layer_1.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_1.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_1.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_1.residual.0.weight].
[06.22.21|09:19:58] Load weights [recon_layer_1.residual.0.bias].
[06.22.21|09:19:58] Load weights [recon_layer_1.residual.1.weight].
[06.22.21|09:19:58] Load weights [recon_layer_1.residual.1.bias].
[06.22.21|09:19:58] Load weights [recon_layer_1.residual.1.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_1.residual.1.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_1.residual.1.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_2.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [recon_layer_2.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [recon_layer_2.tcn.0.weight].
[06.22.21|09:19:58] Load weights [recon_layer_2.tcn.0.bias].
[06.22.21|09:19:58] Load weights [recon_layer_2.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_2.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_2.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_2.tcn.2.weight].
[06.22.21|09:19:58] Load weights [recon_layer_2.tcn.2.bias].
[06.22.21|09:19:58] Load weights [recon_layer_2.tcn.3.weight].
[06.22.21|09:19:58] Load weights [recon_layer_2.tcn.3.bias].
[06.22.21|09:19:58] Load weights [recon_layer_2.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_2.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_2.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_2.residual.0.weight].
[06.22.21|09:19:58] Load weights [recon_layer_2.residual.0.bias].
[06.22.21|09:19:58] Load weights [recon_layer_2.residual.1.weight].
[06.22.21|09:19:58] Load weights [recon_layer_2.residual.1.bias].
[06.22.21|09:19:58] Load weights [recon_layer_2.residual.1.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_2.residual.1.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_2.residual.1.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_3.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [recon_layer_3.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [recon_layer_3.tcn.0.weight].
[06.22.21|09:19:58] Load weights [recon_layer_3.tcn.0.bias].
[06.22.21|09:19:58] Load weights [recon_layer_3.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_3.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_3.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_3.tcn.2.weight].
[06.22.21|09:19:58] Load weights [recon_layer_3.tcn.2.bias].
[06.22.21|09:19:58] Load weights [recon_layer_3.tcn.3.weight].
[06.22.21|09:19:58] Load weights [recon_layer_3.tcn.3.bias].
[06.22.21|09:19:58] Load weights [recon_layer_3.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_3.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_3.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_3.residual.0.weight].
[06.22.21|09:19:58] Load weights [recon_layer_3.residual.0.bias].
[06.22.21|09:19:58] Load weights [recon_layer_3.residual.1.weight].
[06.22.21|09:19:58] Load weights [recon_layer_3.residual.1.bias].
[06.22.21|09:19:58] Load weights [recon_layer_3.residual.1.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_3.residual.1.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_3.residual.1.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_4.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [recon_layer_4.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [recon_layer_4.tcn.0.weight].
[06.22.21|09:19:58] Load weights [recon_layer_4.tcn.0.bias].
[06.22.21|09:19:58] Load weights [recon_layer_4.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_4.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_4.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_4.tcn.2.weight].
[06.22.21|09:19:58] Load weights [recon_layer_4.tcn.2.bias].
[06.22.21|09:19:58] Load weights [recon_layer_4.tcn.3.weight].
[06.22.21|09:19:58] Load weights [recon_layer_4.tcn.3.bias].
[06.22.21|09:19:58] Load weights [recon_layer_4.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_4.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_4.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_4.residual.0.weight].
[06.22.21|09:19:58] Load weights [recon_layer_4.residual.0.bias].
[06.22.21|09:19:58] Load weights [recon_layer_4.residual.1.weight].
[06.22.21|09:19:58] Load weights [recon_layer_4.residual.1.bias].
[06.22.21|09:19:58] Load weights [recon_layer_4.residual.1.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_4.residual.1.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_4.residual.1.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_5.gcn.conv.weight].
[06.22.21|09:19:58] Load weights [recon_layer_5.gcn.conv.bias].
[06.22.21|09:19:58] Load weights [recon_layer_5.tcn.0.weight].
[06.22.21|09:19:58] Load weights [recon_layer_5.tcn.0.bias].
[06.22.21|09:19:58] Load weights [recon_layer_5.tcn.0.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_5.tcn.0.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_5.tcn.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_5.tcn.2.weight].
[06.22.21|09:19:58] Load weights [recon_layer_5.tcn.2.bias].
[06.22.21|09:19:58] Load weights [recon_layer_5.tcn.3.weight].
[06.22.21|09:19:58] Load weights [recon_layer_5.tcn.3.bias].
[06.22.21|09:19:58] Load weights [recon_layer_5.tcn.3.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_5.tcn.3.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_5.tcn.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_6.gcn_recon.deconv.weight].
[06.22.21|09:19:58] Load weights [recon_layer_6.gcn_recon.deconv.bias].
[06.22.21|09:19:58] Load weights [recon_layer_6.tcn_recon.0.weight].
[06.22.21|09:19:58] Load weights [recon_layer_6.tcn_recon.0.bias].
[06.22.21|09:19:58] Load weights [recon_layer_6.tcn_recon.0.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_6.tcn_recon.0.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_6.tcn_recon.0.num_batches_tracked].
[06.22.21|09:19:58] Load weights [recon_layer_6.tcn_recon.2.weight].
[06.22.21|09:19:58] Load weights [recon_layer_6.tcn_recon.2.bias].
[06.22.21|09:19:58] Load weights [recon_layer_6.tcn_recon.3.weight].
[06.22.21|09:19:58] Load weights [recon_layer_6.tcn_recon.3.bias].
[06.22.21|09:19:58] Load weights [recon_layer_6.tcn_recon.3.running_mean].
[06.22.21|09:19:58] Load weights [recon_layer_6.tcn_recon.3.running_var].
[06.22.21|09:19:58] Load weights [recon_layer_6.tcn_recon.3.num_batches_tracked].
[06.22.21|09:19:58] Load weights [edge_importance.0].
[06.22.21|09:19:58] Load weights [edge_importance.1].
[06.22.21|09:19:58] Load weights [edge_importance.2].
[06.22.21|09:19:58] Load weights [edge_importance.3].
[06.22.21|09:19:58] Load weights [edge_importance.4].
[06.22.21|09:19:58] Load weights [edge_importance.5].
[06.22.21|09:19:58] Load weights [edge_importance.6].
[06.22.21|09:19:58] Load weights [edge_importance.7].
[06.22.21|09:19:58] Load weights [edge_importance.8].
[06.22.21|09:19:58] Load weights [edge_importance_recon.0].
[06.22.21|09:19:58] Load weights [edge_importance_recon.1].
[06.22.21|09:19:58] Load weights [edge_importance_recon.2].
[06.22.21|09:19:58] Load weights [edge_importance_recon.3].
[06.22.21|09:19:58] Load weights [edge_importance_recon.4].
[06.22.21|09:19:58] Load weights [edge_importance_recon.5].
[06.22.21|09:19:58] Load weights [edge_importance_recon.6].
[06.22.21|09:19:58] Load weights [edge_importance_recon.7].
[06.22.21|09:19:58] Load weights [edge_importance_recon.8].
[06.22.21|09:19:58] Load weights [fcn.weight].
[06.22.21|09:19:58] Load weights [fcn.bias].
[06.22.21|09:19:58] Load weights from ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch9_model2.pt.
[06.22.21|09:19:58] Load weights [encoder.mlp1.fc1.weight].
[06.22.21|09:19:58] Load weights [encoder.mlp1.fc1.bias].
[06.22.21|09:19:58] Load weights [encoder.mlp1.fc2.weight].
[06.22.21|09:19:58] Load weights [encoder.mlp1.fc2.bias].
[06.22.21|09:19:58] Load weights [encoder.mlp1.bn.weight].
[06.22.21|09:19:58] Load weights [encoder.mlp1.bn.bias].
[06.22.21|09:19:58] Load weights [encoder.mlp1.bn.running_mean].
[06.22.21|09:19:58] Load weights [encoder.mlp1.bn.running_var].
[06.22.21|09:19:58] Load weights [encoder.mlp1.bn.num_batches_tracked].
[06.22.21|09:19:58] Load weights [encoder.mlp2.fc1.weight].
[06.22.21|09:19:58] Load weights [encoder.mlp2.fc1.bias].
[06.22.21|09:19:58] Load weights [encoder.mlp2.fc2.weight].
[06.22.21|09:19:58] Load weights [encoder.mlp2.fc2.bias].
[06.22.21|09:19:58] Load weights [encoder.mlp2.bn.weight].
[06.22.21|09:19:58] Load weights [encoder.mlp2.bn.bias].
[06.22.21|09:19:58] Load weights [encoder.mlp2.bn.running_mean].
[06.22.21|09:19:58] Load weights [encoder.mlp2.bn.running_var].
[06.22.21|09:19:58] Load weights [encoder.mlp2.bn.num_batches_tracked].
[06.22.21|09:19:58] Load weights [encoder.mlp3.fc1.weight].
[06.22.21|09:19:58] Load weights [encoder.mlp3.fc1.bias].
[06.22.21|09:19:58] Load weights [encoder.mlp3.fc2.weight].
[06.22.21|09:19:58] Load weights [encoder.mlp3.fc2.bias].
[06.22.21|09:19:58] Load weights [encoder.mlp3.bn.weight].
[06.22.21|09:19:58] Load weights [encoder.mlp3.bn.bias].
[06.22.21|09:19:58] Load weights [encoder.mlp3.bn.running_mean].
[06.22.21|09:19:58] Load weights [encoder.mlp3.bn.running_var].
[06.22.21|09:19:58] Load weights [encoder.mlp3.bn.num_batches_tracked].
[06.22.21|09:19:58] Load weights [encoder.mlp4.fc1.weight].
[06.22.21|09:19:58] Load weights [encoder.mlp4.fc1.bias].
[06.22.21|09:19:58] Load weights [encoder.mlp4.fc2.weight].
[06.22.21|09:19:58] Load weights [encoder.mlp4.fc2.bias].
[06.22.21|09:19:58] Load weights [encoder.mlp4.bn.weight].
[06.22.21|09:19:58] Load weights [encoder.mlp4.bn.bias].
[06.22.21|09:19:58] Load weights [encoder.mlp4.bn.running_mean].
[06.22.21|09:19:58] Load weights [encoder.mlp4.bn.running_var].
[06.22.21|09:19:58] Load weights [encoder.mlp4.bn.num_batches_tracked].
[06.22.21|09:19:58] Load weights [encoder.fc_out.weight].
[06.22.21|09:19:58] Load weights [encoder.fc_out.bias].
[06.22.21|09:19:58] Load weights [decoder.msg_fc1.0.weight].
[06.22.21|09:19:58] Load weights [decoder.msg_fc1.0.bias].
[06.22.21|09:19:58] Load weights [decoder.msg_fc1.1.weight].
[06.22.21|09:19:58] Load weights [decoder.msg_fc1.1.bias].
[06.22.21|09:19:58] Load weights [decoder.msg_fc1.2.weight].
[06.22.21|09:19:58] Load weights [decoder.msg_fc1.2.bias].
[06.22.21|09:19:58] Load weights [decoder.msg_fc2.0.weight].
[06.22.21|09:19:58] Load weights [decoder.msg_fc2.0.bias].
[06.22.21|09:19:58] Load weights [decoder.msg_fc2.1.weight].
[06.22.21|09:19:58] Load weights [decoder.msg_fc2.1.bias].
[06.22.21|09:19:58] Load weights [decoder.msg_fc2.2.weight].
[06.22.21|09:19:58] Load weights [decoder.msg_fc2.2.bias].
[06.22.21|09:19:58] Load weights [decoder.hidden_r.weight].
[06.22.21|09:19:58] Load weights [decoder.hidden_i.weight].
[06.22.21|09:19:58] Load weights [decoder.hidden_n.weight].
[06.22.21|09:19:58] Load weights [decoder.input_r.weight].
[06.22.21|09:19:58] Load weights [decoder.input_r.bias].
[06.22.21|09:19:58] Load weights [decoder.input_i.weight].
[06.22.21|09:19:58] Load weights [decoder.input_i.bias].
[06.22.21|09:19:58] Load weights [decoder.input_n.weight].
[06.22.21|09:19:58] Load weights [decoder.input_n.bias].
[06.22.21|09:19:58] Load weights [decoder.out_fc1.weight].
[06.22.21|09:19:58] Load weights [decoder.out_fc1.bias].
[06.22.21|09:19:58] Load weights [decoder.out_fc2.weight].
[06.22.21|09:19:58] Load weights [decoder.out_fc2.bias].
[06.22.21|09:19:58] Load weights [decoder.out_fc3.weight].
[06.22.21|09:19:58] Load weights [decoder.out_fc3.bias].
[06.22.21|09:20:01] Parameters:
{'work_dir': './work_dir/recognition/ntu-xsub/AS_GCN', 'config': 'config/as_gcn/ntu-xsub/train.yaml', 'phase': 'train', 'save_result': False, 'start_epoch': 10, 'num_epoch': 100, 'use_gpu': True, 'device': [1], 'log_interval': 100, 'save_interval': 1, 'eval_interval': 5, 'save_log': True, 'print_log': True, 'pavi_log': False, 'feeder': 'feeder.feeder.Feeder', 'num_worker': 4, 'train_feeder_args': {'data_path': './data/nturgb_d/xsub/train_data_joint_pad.npy', 'label_path': './data/nturgb_d/xsub/train_label.pkl', 'random_move': True, 'repeat_pad': True, 'down_sample': True, 'debug': False}, 'test_feeder_args': {'data_path': './data/nturgb_d/xsub/val_data_joint_pad.npy', 'label_path': './data/nturgb_d/xsub/val_label.pkl', 'random_move': False, 'repeat_pad': True, 'down_sample': True}, 'batch_size': 4, 'test_batch_size': 32, 'debug': False, 'model1': 'net.as_gcn.Model', 'model2': 'net.utils.adj_learn.AdjacencyLearn', 'model1_args': {'in_channels': 3, 'num_class': 60, 'dropout': 0.5, 'edge_importance_weighting': True, 'graph_args': {'layout': 'ntu-rgb+d', 'strategy': 'spatial', 'max_hop': 4}}, 'model2_args': {'n_in_enc': 150, 'n_hid_enc': 128, 'edge_types': 3, 'n_in_dec': 3, 'n_hid_dec': 128, 'node_num': 25}, 'weights1': './work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch9_model1.pt', 'weights2': './work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch9_model2.pt', 'ignore_weights': [], 'show_topk': [1, 5], 'base_lr1': 0.1, 'base_lr2': 0.0005, 'step': [50, 70, 90], 'optimizer': 'SGD', 'nesterov': True, 'weight_decay': 0.0001, 'max_hop_dir': 'max_hop_4', 'lamda_act': 0.5, 'lamda_act_dir': 'lamda_05'}

[06.22.21|09:20:01] Training epoch: 10
/root/anaconda3/envs/asgcn/lib/python3.6/site-packages/torch/nn/modules/container.py:435: UserWarning: Setting attributes on ParameterList is not supported.
  warnings.warn("Setting attributes on ParameterList is not supported.")
/root/AS-GCN/net/utils/adj_learn.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  soft_max_1d = F.softmax(trans_input)
[06.22.21|09:20:02] 	Iter 0 Done. | loss1: 4.0646 | loss_class: 4.0562 | loss_recon: 0.0084 | lr: 0.100000
[06.22.21|09:20:54] 	Iter 100 Done. | loss1: 4.2628 | loss_class: 4.2610 | loss_recon: 0.0017 | lr: 0.100000
[06.22.21|09:21:44] 	Iter 200 Done. | loss1: 4.0211 | loss_class: 4.0195 | loss_recon: 0.0015 | lr: 0.100000
[06.22.21|09:22:36] 	Iter 300 Done. | loss1: 4.4422 | loss_class: 4.4406 | loss_recon: 0.0016 | lr: 0.100000
[06.22.21|09:23:27] 	Iter 400 Done. | loss1: 4.3386 | loss_class: 4.3374 | loss_recon: 0.0012 | lr: 0.100000
[06.22.21|09:24:18] 	Iter 500 Done. | loss1: 3.9202 | loss_class: 3.9194 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|09:25:08] 	Iter 600 Done. | loss1: 3.9102 | loss_class: 3.9090 | loss_recon: 0.0013 | lr: 0.100000
[06.22.21|09:26:00] 	Iter 700 Done. | loss1: 4.3347 | loss_class: 4.3294 | loss_recon: 0.0052 | lr: 0.100000
[06.22.21|09:26:51] 	Iter 800 Done. | loss1: 3.7319 | loss_class: 3.7310 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|09:27:42] 	Iter 900 Done. | loss1: 3.7967 | loss_class: 3.7959 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|09:28:34] 	Iter 1000 Done. | loss1: 4.0492 | loss_class: 4.0483 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|09:29:26] 	Iter 1100 Done. | loss1: 3.4970 | loss_class: 3.4959 | loss_recon: 0.0012 | lr: 0.100000
[06.22.21|09:30:17] 	Iter 1200 Done. | loss1: 3.7649 | loss_class: 3.7641 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|09:31:09] 	Iter 1300 Done. | loss1: 3.9772 | loss_class: 3.9761 | loss_recon: 0.0011 | lr: 0.100000
[06.22.21|09:32:01] 	Iter 1400 Done. | loss1: 4.2881 | loss_class: 4.2874 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|09:32:52] 	Iter 1500 Done. | loss1: 3.4202 | loss_class: 3.4192 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|09:33:43] 	Iter 1600 Done. | loss1: 4.2647 | loss_class: 4.2639 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|09:34:35] 	Iter 1700 Done. | loss1: 4.5805 | loss_class: 4.5799 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|09:35:27] 	Iter 1800 Done. | loss1: 4.0580 | loss_class: 4.0571 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|09:36:18] 	Iter 1900 Done. | loss1: 4.0683 | loss_class: 4.0676 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|09:37:10] 	Iter 2000 Done. | loss1: 3.8921 | loss_class: 3.8911 | loss_recon: 0.0011 | lr: 0.100000
[06.22.21|09:38:01] 	Iter 2100 Done. | loss1: 3.7186 | loss_class: 3.7177 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|09:38:53] 	Iter 2200 Done. | loss1: 4.4453 | loss_class: 4.4443 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|09:39:44] 	Iter 2300 Done. | loss1: 3.6580 | loss_class: 3.6567 | loss_recon: 0.0013 | lr: 0.100000
[06.22.21|09:40:36] 	Iter 2400 Done. | loss1: 3.3243 | loss_class: 3.3237 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|09:41:27] 	Iter 2500 Done. | loss1: 3.8976 | loss_class: 3.8967 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|09:42:20] 	Iter 2600 Done. | loss1: 3.5674 | loss_class: 3.5667 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|09:43:11] 	Iter 2700 Done. | loss1: 4.0008 | loss_class: 3.9964 | loss_recon: 0.0044 | lr: 0.100000
[06.22.21|09:44:03] 	Iter 2800 Done. | loss1: 3.9507 | loss_class: 3.9499 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|09:44:54] 	Iter 2900 Done. | loss1: 3.3905 | loss_class: 3.3895 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|09:45:47] 	Iter 3000 Done. | loss1: 4.2546 | loss_class: 4.2539 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|09:46:38] 	Iter 3100 Done. | loss1: 3.7974 | loss_class: 3.7968 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|09:47:30] 	Iter 3200 Done. | loss1: 3.3842 | loss_class: 3.3836 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|09:48:21] 	Iter 3300 Done. | loss1: 4.3468 | loss_class: 4.3463 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|09:49:12] 	Iter 3400 Done. | loss1: 3.8002 | loss_class: 3.7994 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|09:50:04] 	Iter 3500 Done. | loss1: 3.7086 | loss_class: 3.7079 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|09:50:56] 	Iter 3600 Done. | loss1: 3.9508 | loss_class: 3.9502 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|09:51:48] 	Iter 3700 Done. | loss1: 3.5736 | loss_class: 3.5729 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|09:52:39] 	Iter 3800 Done. | loss1: 3.6290 | loss_class: 3.6283 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|09:53:31] 	Iter 3900 Done. | loss1: 3.4076 | loss_class: 3.4066 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|09:54:22] 	Iter 4000 Done. | loss1: 3.7935 | loss_class: 3.7917 | loss_recon: 0.0018 | lr: 0.100000
[06.22.21|09:55:13] 	Iter 4100 Done. | loss1: 4.0257 | loss_class: 4.0250 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|09:56:05] 	Iter 4200 Done. | loss1: 5.5132 | loss_class: 5.5124 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|09:56:56] 	Iter 4300 Done. | loss1: 3.0884 | loss_class: 3.0877 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|09:57:48] 	Iter 4400 Done. | loss1: 3.5635 | loss_class: 3.5629 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|09:58:39] 	Iter 4500 Done. | loss1: 3.5267 | loss_class: 3.5262 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|09:59:30] 	Iter 4600 Done. | loss1: 4.3565 | loss_class: 4.3557 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|10:00:21] 	Iter 4700 Done. | loss1: 3.9800 | loss_class: 3.9794 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:01:12] 	Iter 4800 Done. | loss1: 4.6249 | loss_class: 4.6241 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|10:02:04] 	Iter 4900 Done. | loss1: 3.6305 | loss_class: 3.6297 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|10:02:56] 	Iter 5000 Done. | loss1: 4.2640 | loss_class: 4.2634 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:03:48] 	Iter 5100 Done. | loss1: 3.3504 | loss_class: 3.3498 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:04:40] 	Iter 5200 Done. | loss1: 4.0746 | loss_class: 4.0740 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:05:32] 	Iter 5300 Done. | loss1: 3.5067 | loss_class: 3.5062 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|10:06:23] 	Iter 5400 Done. | loss1: 4.5267 | loss_class: 4.5259 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|10:07:14] 	Iter 5500 Done. | loss1: 2.7823 | loss_class: 2.7816 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:08:05] 	Iter 5600 Done. | loss1: 4.3400 | loss_class: 4.3395 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|10:08:56] 	Iter 5700 Done. | loss1: 3.4189 | loss_class: 3.4183 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:09:47] 	Iter 5800 Done. | loss1: 3.3536 | loss_class: 3.3530 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:10:38] 	Iter 5900 Done. | loss1: 4.4223 | loss_class: 4.4218 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:11:30] 	Iter 6000 Done. | loss1: 2.5198 | loss_class: 2.5190 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|10:12:22] 	Iter 6100 Done. | loss1: 3.7254 | loss_class: 3.7248 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:13:14] 	Iter 6200 Done. | loss1: 3.5997 | loss_class: 3.5990 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:14:04] 	Iter 6300 Done. | loss1: 3.0844 | loss_class: 3.0837 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:14:56] 	Iter 6400 Done. | loss1: 3.6182 | loss_class: 3.6176 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:15:47] 	Iter 6500 Done. | loss1: 3.5038 | loss_class: 3.5033 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:16:39] 	Iter 6600 Done. | loss1: 3.4609 | loss_class: 3.4600 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|10:17:30] 	Iter 6700 Done. | loss1: 3.8626 | loss_class: 3.8621 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|10:18:22] 	Iter 6800 Done. | loss1: 3.4608 | loss_class: 3.4600 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|10:19:13] 	Iter 6900 Done. | loss1: 3.9280 | loss_class: 3.9271 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|10:20:05] 	Iter 7000 Done. | loss1: 2.6512 | loss_class: 2.6506 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:20:56] 	Iter 7100 Done. | loss1: 3.3212 | loss_class: 3.3204 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|10:21:48] 	Iter 7200 Done. | loss1: 3.1385 | loss_class: 3.1379 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:22:40] 	Iter 7300 Done. | loss1: 4.6234 | loss_class: 4.6228 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:23:32] 	Iter 7400 Done. | loss1: 3.2527 | loss_class: 3.2521 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:24:24] 	Iter 7500 Done. | loss1: 3.0626 | loss_class: 3.0620 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:25:17] 	Iter 7600 Done. | loss1: 2.9685 | loss_class: 2.9680 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:26:08] 	Iter 7700 Done. | loss1: 3.6643 | loss_class: 3.6635 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|10:27:00] 	Iter 7800 Done. | loss1: 3.0400 | loss_class: 3.0390 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|10:27:52] 	Iter 7900 Done. | loss1: 2.2488 | loss_class: 2.2482 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:28:44] 	Iter 8000 Done. | loss1: 4.0257 | loss_class: 4.0251 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:29:35] 	Iter 8100 Done. | loss1: 3.4745 | loss_class: 3.4738 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:30:27] 	Iter 8200 Done. | loss1: 3.7469 | loss_class: 3.7462 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:31:19] 	Iter 8300 Done. | loss1: 2.4295 | loss_class: 2.4286 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|10:32:10] 	Iter 8400 Done. | loss1: 2.8812 | loss_class: 2.8807 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|10:33:02] 	Iter 8500 Done. | loss1: 3.1236 | loss_class: 3.1229 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:33:54] 	Iter 8600 Done. | loss1: 3.4122 | loss_class: 3.4116 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:34:46] 	Iter 8700 Done. | loss1: 3.1511 | loss_class: 3.1504 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:35:37] 	Iter 8800 Done. | loss1: 2.7058 | loss_class: 2.7053 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:36:28] 	Iter 8900 Done. | loss1: 3.5570 | loss_class: 3.5564 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:37:20] 	Iter 9000 Done. | loss1: 2.9009 | loss_class: 2.9002 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:38:11] 	Iter 9100 Done. | loss1: 2.9259 | loss_class: 2.9252 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:39:02] 	Iter 9200 Done. | loss1: 2.8473 | loss_class: 2.8467 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:39:53] 	Iter 9300 Done. | loss1: 4.0954 | loss_class: 4.0948 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:40:44] 	Iter 9400 Done. | loss1: 2.7135 | loss_class: 2.7128 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:41:36] 	Iter 9500 Done. | loss1: 3.3302 | loss_class: 3.3297 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:42:27] 	Iter 9600 Done. | loss1: 2.9626 | loss_class: 2.9620 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:43:18] 	Iter 9700 Done. | loss1: 2.9747 | loss_class: 2.9743 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|10:44:10] 	Iter 9800 Done. | loss1: 2.9964 | loss_class: 2.9958 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:45:01] 	Iter 9900 Done. | loss1: 3.6582 | loss_class: 3.6575 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:45:52] 	Iter 10000 Done. | loss1: 2.7183 | loss_class: 2.7177 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:46:03] 	mean_loss1: 3.6475319340423633
[06.22.21|10:46:03] 	mean_loss_class: 3.646481150687276
[06.22.21|10:46:03] 	mean_loss_recon: 0.0010507834818369553
[06.22.21|10:46:03] Time consumption:
[06.22.21|10:46:03] Done.
[06.22.21|10:46:03] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch10_model1.pt.
[06.22.21|10:46:03] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch10_model2.pt.
[06.22.21|10:46:03] Training epoch: 11
[06.22.21|10:46:44] 	Iter 10100 Done. | loss1: 3.1425 | loss_class: 3.1420 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|10:47:35] 	Iter 10200 Done. | loss1: 3.2132 | loss_class: 3.2124 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|10:48:27] 	Iter 10300 Done. | loss1: 3.3865 | loss_class: 3.3855 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|10:49:19] 	Iter 10400 Done. | loss1: 3.2874 | loss_class: 3.2868 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:50:10] 	Iter 10500 Done. | loss1: 3.8332 | loss_class: 3.8323 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|10:51:01] 	Iter 10600 Done. | loss1: 4.3357 | loss_class: 4.3351 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:51:53] 	Iter 10700 Done. | loss1: 2.2254 | loss_class: 2.2248 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:52:44] 	Iter 10800 Done. | loss1: 2.0238 | loss_class: 2.0229 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|10:53:35] 	Iter 10900 Done. | loss1: 2.3806 | loss_class: 2.3800 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:54:27] 	Iter 11000 Done. | loss1: 3.3174 | loss_class: 3.3168 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:55:19] 	Iter 11100 Done. | loss1: 3.9650 | loss_class: 3.9644 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|10:56:10] 	Iter 11200 Done. | loss1: 2.1473 | loss_class: 2.1465 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|10:57:03] 	Iter 11300 Done. | loss1: 1.7279 | loss_class: 1.7271 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:57:55] 	Iter 11400 Done. | loss1: 2.6494 | loss_class: 2.6488 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|10:58:46] 	Iter 11500 Done. | loss1: 2.8292 | loss_class: 2.8288 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|10:59:38] 	Iter 11600 Done. | loss1: 3.3478 | loss_class: 3.3470 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:00:29] 	Iter 11700 Done. | loss1: 3.7043 | loss_class: 3.7038 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:01:21] 	Iter 11800 Done. | loss1: 2.5876 | loss_class: 2.5869 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:02:13] 	Iter 11900 Done. | loss1: 3.5965 | loss_class: 3.5959 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:03:05] 	Iter 12000 Done. | loss1: 3.5240 | loss_class: 3.5235 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|11:03:56] 	Iter 12100 Done. | loss1: 3.1254 | loss_class: 3.1249 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:04:48] 	Iter 12200 Done. | loss1: 2.7901 | loss_class: 2.7896 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:05:41] 	Iter 12300 Done. | loss1: 1.9780 | loss_class: 1.9774 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:06:33] 	Iter 12400 Done. | loss1: 2.6696 | loss_class: 2.6689 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|11:07:24] 	Iter 12500 Done. | loss1: 2.2938 | loss_class: 2.2929 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|11:08:15] 	Iter 12600 Done. | loss1: 5.5889 | loss_class: 5.5881 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|11:09:08] 	Iter 12700 Done. | loss1: 2.9493 | loss_class: 2.9488 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:09:59] 	Iter 12800 Done. | loss1: 2.2291 | loss_class: 2.2283 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|11:10:51] 	Iter 12900 Done. | loss1: 2.5762 | loss_class: 2.5755 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:11:42] 	Iter 13000 Done. | loss1: 2.2078 | loss_class: 2.2071 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:12:34] 	Iter 13100 Done. | loss1: 2.1392 | loss_class: 2.1385 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:13:26] 	Iter 13200 Done. | loss1: 1.3071 | loss_class: 1.3066 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:14:18] 	Iter 13300 Done. | loss1: 4.0473 | loss_class: 4.0467 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:15:08] 	Iter 13400 Done. | loss1: 1.6669 | loss_class: 1.6664 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:16:00] 	Iter 13500 Done. | loss1: 3.6031 | loss_class: 3.6025 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:16:51] 	Iter 13600 Done. | loss1: 2.4067 | loss_class: 2.4061 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:17:42] 	Iter 13700 Done. | loss1: 3.1981 | loss_class: 3.1972 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|11:18:34] 	Iter 13800 Done. | loss1: 2.0922 | loss_class: 2.0915 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:19:26] 	Iter 13900 Done. | loss1: 3.2100 | loss_class: 3.2091 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|11:20:18] 	Iter 14000 Done. | loss1: 2.5015 | loss_class: 2.5009 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:21:10] 	Iter 14100 Done. | loss1: 3.5709 | loss_class: 3.5704 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:22:03] 	Iter 14200 Done. | loss1: 2.3749 | loss_class: 2.3742 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:22:55] 	Iter 14300 Done. | loss1: 3.5519 | loss_class: 3.5512 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:23:46] 	Iter 14400 Done. | loss1: 3.6893 | loss_class: 3.6884 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|11:24:39] 	Iter 14500 Done. | loss1: 2.9468 | loss_class: 2.9461 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:25:31] 	Iter 14600 Done. | loss1: 2.0187 | loss_class: 2.0181 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:26:23] 	Iter 14700 Done. | loss1: 2.1455 | loss_class: 2.1448 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|11:27:14] 	Iter 14800 Done. | loss1: 3.3326 | loss_class: 3.3320 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:28:06] 	Iter 14900 Done. | loss1: 2.1990 | loss_class: 2.1981 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|11:28:58] 	Iter 15000 Done. | loss1: 3.0550 | loss_class: 3.0545 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:29:50] 	Iter 15100 Done. | loss1: 2.5916 | loss_class: 2.5910 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:30:41] 	Iter 15200 Done. | loss1: 2.7984 | loss_class: 2.7977 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:31:32] 	Iter 15300 Done. | loss1: 2.7288 | loss_class: 2.7283 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:32:24] 	Iter 15400 Done. | loss1: 2.3335 | loss_class: 2.3328 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:33:15] 	Iter 15500 Done. | loss1: 1.5846 | loss_class: 1.5840 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:34:06] 	Iter 15600 Done. | loss1: 2.3495 | loss_class: 2.3489 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:34:58] 	Iter 15700 Done. | loss1: 3.2694 | loss_class: 3.2688 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:35:50] 	Iter 15800 Done. | loss1: 3.1226 | loss_class: 3.1219 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:36:41] 	Iter 15900 Done. | loss1: 4.1445 | loss_class: 4.1436 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|11:37:33] 	Iter 16000 Done. | loss1: 2.8964 | loss_class: 2.8957 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:38:24] 	Iter 16100 Done. | loss1: 1.8139 | loss_class: 1.8133 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:39:15] 	Iter 16200 Done. | loss1: 1.8083 | loss_class: 1.8078 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:40:07] 	Iter 16300 Done. | loss1: 4.6626 | loss_class: 4.6619 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:40:58] 	Iter 16400 Done. | loss1: 3.9853 | loss_class: 3.9846 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:41:50] 	Iter 16500 Done. | loss1: 3.5922 | loss_class: 3.5915 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:42:41] 	Iter 16600 Done. | loss1: 2.4945 | loss_class: 2.4937 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|11:43:33] 	Iter 16700 Done. | loss1: 2.7126 | loss_class: 2.7120 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:44:24] 	Iter 16800 Done. | loss1: 1.9234 | loss_class: 1.9226 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|11:45:16] 	Iter 16900 Done. | loss1: 2.5518 | loss_class: 2.5510 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|11:46:08] 	Iter 17000 Done. | loss1: 2.5343 | loss_class: 2.5338 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:46:59] 	Iter 17100 Done. | loss1: 2.5136 | loss_class: 2.5131 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:47:51] 	Iter 17200 Done. | loss1: 2.7081 | loss_class: 2.7077 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|11:48:42] 	Iter 17300 Done. | loss1: 3.3442 | loss_class: 3.3434 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|11:49:33] 	Iter 17400 Done. | loss1: 3.4288 | loss_class: 3.4281 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:50:25] 	Iter 17500 Done. | loss1: 1.7533 | loss_class: 1.7526 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:51:17] 	Iter 17600 Done. | loss1: 3.9045 | loss_class: 3.9038 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:52:09] 	Iter 17700 Done. | loss1: 2.3839 | loss_class: 2.3834 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:53:00] 	Iter 17800 Done. | loss1: 2.8041 | loss_class: 2.8036 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:53:52] 	Iter 17900 Done. | loss1: 2.6051 | loss_class: 2.6047 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:54:44] 	Iter 18000 Done. | loss1: 2.1045 | loss_class: 2.1039 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:55:36] 	Iter 18100 Done. | loss1: 1.8165 | loss_class: 1.8158 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|11:56:27] 	Iter 18200 Done. | loss1: 1.4518 | loss_class: 1.4513 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|11:57:19] 	Iter 18300 Done. | loss1: 2.2523 | loss_class: 2.2517 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:58:09] 	Iter 18400 Done. | loss1: 2.0227 | loss_class: 2.0221 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|11:59:01] 	Iter 18500 Done. | loss1: 3.2442 | loss_class: 3.2434 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|11:59:52] 	Iter 18600 Done. | loss1: 2.6406 | loss_class: 2.6399 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:00:43] 	Iter 18700 Done. | loss1: 2.8585 | loss_class: 2.8577 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|12:01:34] 	Iter 18800 Done. | loss1: 2.5829 | loss_class: 2.5822 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:02:26] 	Iter 18900 Done. | loss1: 2.7265 | loss_class: 2.7260 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|12:03:17] 	Iter 19000 Done. | loss1: 2.1242 | loss_class: 2.1237 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|12:04:09] 	Iter 19100 Done. | loss1: 2.5292 | loss_class: 2.5285 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:05:01] 	Iter 19200 Done. | loss1: 2.8220 | loss_class: 2.8212 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|12:05:53] 	Iter 19300 Done. | loss1: 2.5521 | loss_class: 2.5514 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:06:44] 	Iter 19400 Done. | loss1: 2.7487 | loss_class: 2.7480 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:07:35] 	Iter 19500 Done. | loss1: 2.2704 | loss_class: 2.2699 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|12:08:26] 	Iter 19600 Done. | loss1: 2.9007 | loss_class: 2.9001 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:09:18] 	Iter 19700 Done. | loss1: 1.3102 | loss_class: 1.3095 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|12:10:09] 	Iter 19800 Done. | loss1: 3.1742 | loss_class: 3.1737 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|12:11:01] 	Iter 19900 Done. | loss1: 3.7299 | loss_class: 3.7293 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:11:52] 	Iter 20000 Done. | loss1: 2.0561 | loss_class: 2.0555 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:12:14] 	mean_loss1: 2.787145144287352
[06.22.21|12:12:14] 	mean_loss_class: 2.786494454535104
[06.22.21|12:12:14] 	mean_loss_recon: 0.0006506900354770814
[06.22.21|12:12:14] Time consumption:
[06.22.21|12:12:14] Done.
[06.22.21|12:12:14] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch11_model1.pt.
[06.22.21|12:12:14] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch11_model2.pt.
[06.22.21|12:12:14] Training epoch: 12
[06.22.21|12:12:45] 	Iter 20100 Done. | loss1: 3.6693 | loss_class: 3.6687 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|12:13:35] 	Iter 20200 Done. | loss1: 0.9855 | loss_class: 0.9849 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:14:25] 	Iter 20300 Done. | loss1: 2.9358 | loss_class: 2.9353 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|12:15:15] 	Iter 20400 Done. | loss1: 3.3877 | loss_class: 3.3869 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|12:16:07] 	Iter 20500 Done. | loss1: 1.8314 | loss_class: 1.8307 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|12:17:00] 	Iter 20600 Done. | loss1: 2.2142 | loss_class: 2.2135 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:17:51] 	Iter 20700 Done. | loss1: 3.5569 | loss_class: 3.5562 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:18:41] 	Iter 20800 Done. | loss1: 3.2071 | loss_class: 3.2065 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:19:31] 	Iter 20900 Done. | loss1: 3.0884 | loss_class: 3.0878 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:20:21] 	Iter 21000 Done. | loss1: 2.3080 | loss_class: 2.3074 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:21:15] 	Iter 21100 Done. | loss1: 2.7599 | loss_class: 2.7592 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:22:05] 	Iter 21200 Done. | loss1: 2.7611 | loss_class: 2.7604 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:22:56] 	Iter 21300 Done. | loss1: 1.2432 | loss_class: 1.2425 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:23:46] 	Iter 21400 Done. | loss1: 1.8967 | loss_class: 1.8962 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|12:24:36] 	Iter 21500 Done. | loss1: 3.0571 | loss_class: 3.0565 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:25:26] 	Iter 21600 Done. | loss1: 1.8126 | loss_class: 1.8120 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:26:17] 	Iter 21700 Done. | loss1: 2.4674 | loss_class: 2.4668 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:27:10] 	Iter 21800 Done. | loss1: 2.2606 | loss_class: 2.2602 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|12:28:00] 	Iter 21900 Done. | loss1: 2.6029 | loss_class: 2.6021 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|12:28:51] 	Iter 22000 Done. | loss1: 1.2578 | loss_class: 1.2570 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|12:29:44] 	Iter 22100 Done. | loss1: 2.4450 | loss_class: 2.4441 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|12:30:35] 	Iter 22200 Done. | loss1: 1.6572 | loss_class: 1.6566 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:31:25] 	Iter 22300 Done. | loss1: 3.8604 | loss_class: 3.8597 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:32:16] 	Iter 22400 Done. | loss1: 1.6934 | loss_class: 1.6927 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:33:06] 	Iter 22500 Done. | loss1: 2.6325 | loss_class: 2.6319 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:33:57] 	Iter 22600 Done. | loss1: 2.9855 | loss_class: 2.9850 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|12:34:47] 	Iter 22700 Done. | loss1: 1.8389 | loss_class: 1.8383 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:35:37] 	Iter 22800 Done. | loss1: 1.5846 | loss_class: 1.5837 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|12:36:27] 	Iter 22900 Done. | loss1: 2.9467 | loss_class: 2.9461 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:37:17] 	Iter 23000 Done. | loss1: 1.7969 | loss_class: 1.7962 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:38:07] 	Iter 23100 Done. | loss1: 1.7725 | loss_class: 1.7719 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:38:58] 	Iter 23200 Done. | loss1: 2.3750 | loss_class: 2.3744 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:39:48] 	Iter 23300 Done. | loss1: 1.6528 | loss_class: 1.6522 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:40:39] 	Iter 23400 Done. | loss1: 1.7483 | loss_class: 1.7473 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|12:41:30] 	Iter 23500 Done. | loss1: 2.6983 | loss_class: 2.6977 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:42:20] 	Iter 23600 Done. | loss1: 1.9685 | loss_class: 1.9680 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:43:10] 	Iter 23700 Done. | loss1: 3.1667 | loss_class: 3.1662 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:44:00] 	Iter 23800 Done. | loss1: 3.4520 | loss_class: 3.4513 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:44:50] 	Iter 23900 Done. | loss1: 1.9073 | loss_class: 1.9068 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|12:45:40] 	Iter 24000 Done. | loss1: 3.0151 | loss_class: 3.0144 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:46:30] 	Iter 24100 Done. | loss1: 1.5867 | loss_class: 1.5862 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|12:47:20] 	Iter 24200 Done. | loss1: 3.6045 | loss_class: 3.6039 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:48:10] 	Iter 24300 Done. | loss1: 1.5725 | loss_class: 1.5720 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|12:49:00] 	Iter 24400 Done. | loss1: 2.1204 | loss_class: 2.1196 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|12:49:50] 	Iter 24500 Done. | loss1: 1.8726 | loss_class: 1.8719 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:50:40] 	Iter 24600 Done. | loss1: 2.4211 | loss_class: 2.4204 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:51:29] 	Iter 24700 Done. | loss1: 1.5106 | loss_class: 1.5100 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|12:52:19] 	Iter 24800 Done. | loss1: 3.7812 | loss_class: 3.7806 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:53:09] 	Iter 24900 Done. | loss1: 1.8806 | loss_class: 1.8800 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:54:00] 	Iter 25000 Done. | loss1: 1.8716 | loss_class: 1.8708 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:54:49] 	Iter 25100 Done. | loss1: 2.4846 | loss_class: 2.4840 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:55:39] 	Iter 25200 Done. | loss1: 1.7875 | loss_class: 1.7867 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:56:29] 	Iter 25300 Done. | loss1: 1.6255 | loss_class: 1.6249 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:57:19] 	Iter 25400 Done. | loss1: 3.5216 | loss_class: 3.5210 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|12:58:08] 	Iter 25500 Done. | loss1: 1.9037 | loss_class: 1.9030 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:58:58] 	Iter 25600 Done. | loss1: 2.9039 | loss_class: 2.9032 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|12:59:47] 	Iter 25700 Done. | loss1: 3.5275 | loss_class: 3.5268 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:00:37] 	Iter 25800 Done. | loss1: 2.8178 | loss_class: 2.8170 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:01:27] 	Iter 25900 Done. | loss1: 3.0292 | loss_class: 3.0287 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|13:02:17] 	Iter 26000 Done. | loss1: 2.1979 | loss_class: 2.1973 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:03:07] 	Iter 26100 Done. | loss1: 4.0107 | loss_class: 4.0102 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|13:03:57] 	Iter 26200 Done. | loss1: 2.6032 | loss_class: 2.6025 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:04:47] 	Iter 26300 Done. | loss1: 4.1189 | loss_class: 4.1183 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:05:38] 	Iter 26400 Done. | loss1: 1.8796 | loss_class: 1.8788 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:06:28] 	Iter 26500 Done. | loss1: 2.7247 | loss_class: 2.7241 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:07:18] 	Iter 26600 Done. | loss1: 1.6832 | loss_class: 1.6826 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:08:08] 	Iter 26700 Done. | loss1: 1.2329 | loss_class: 1.2323 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:08:58] 	Iter 26800 Done. | loss1: 2.5252 | loss_class: 2.5246 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:09:48] 	Iter 26900 Done. | loss1: 1.8403 | loss_class: 1.8396 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:10:38] 	Iter 27000 Done. | loss1: 2.0947 | loss_class: 2.0940 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:11:29] 	Iter 27100 Done. | loss1: 2.0064 | loss_class: 2.0057 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:12:20] 	Iter 27200 Done. | loss1: 2.4171 | loss_class: 2.4164 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:13:10] 	Iter 27300 Done. | loss1: 1.3934 | loss_class: 1.3926 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:14:00] 	Iter 27400 Done. | loss1: 2.3735 | loss_class: 2.3731 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|13:14:51] 	Iter 27500 Done. | loss1: 1.5102 | loss_class: 1.5095 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:15:41] 	Iter 27600 Done. | loss1: 1.5614 | loss_class: 1.5607 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:16:32] 	Iter 27700 Done. | loss1: 3.5802 | loss_class: 3.5796 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:17:22] 	Iter 27800 Done. | loss1: 2.0397 | loss_class: 2.0391 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:18:13] 	Iter 27900 Done. | loss1: 2.0777 | loss_class: 2.0770 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:19:03] 	Iter 28000 Done. | loss1: 2.5134 | loss_class: 2.5127 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:19:53] 	Iter 28100 Done. | loss1: 2.0801 | loss_class: 2.0795 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|13:20:43] 	Iter 28200 Done. | loss1: 3.4877 | loss_class: 3.4872 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:21:33] 	Iter 28300 Done. | loss1: 2.6572 | loss_class: 2.6564 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:22:24] 	Iter 28400 Done. | loss1: 2.0607 | loss_class: 2.0599 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:23:14] 	Iter 28500 Done. | loss1: 2.2106 | loss_class: 2.2101 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:24:04] 	Iter 28600 Done. | loss1: 2.0531 | loss_class: 2.0527 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|13:24:54] 	Iter 28700 Done. | loss1: 1.3078 | loss_class: 1.3071 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:25:44] 	Iter 28800 Done. | loss1: 2.6657 | loss_class: 2.6652 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|13:26:34] 	Iter 28900 Done. | loss1: 0.7613 | loss_class: 0.7605 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:27:25] 	Iter 29000 Done. | loss1: 2.2988 | loss_class: 2.2982 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:28:15] 	Iter 29100 Done. | loss1: 1.1914 | loss_class: 1.1904 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|13:29:05] 	Iter 29200 Done. | loss1: 1.9854 | loss_class: 1.9850 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|13:29:55] 	Iter 29300 Done. | loss1: 2.5569 | loss_class: 2.5561 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:30:45] 	Iter 29400 Done. | loss1: 2.6112 | loss_class: 2.6107 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|13:31:36] 	Iter 29500 Done. | loss1: 1.7742 | loss_class: 1.7735 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:32:26] 	Iter 29600 Done. | loss1: 1.2283 | loss_class: 1.2279 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|13:33:16] 	Iter 29700 Done. | loss1: 1.0186 | loss_class: 1.0178 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:34:06] 	Iter 29800 Done. | loss1: 2.0423 | loss_class: 2.0415 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:34:56] 	Iter 29900 Done. | loss1: 2.3574 | loss_class: 2.3568 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:35:47] 	Iter 30000 Done. | loss1: 1.0944 | loss_class: 1.0939 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:36:19] 	mean_loss1: 2.313653604260725
[06.22.21|13:36:19] 	mean_loss_class: 2.3130109438970403
[06.22.21|13:36:19] 	mean_loss_recon: 0.00064266054382363
[06.22.21|13:36:19] Time consumption:
[06.22.21|13:36:19] Done.
[06.22.21|13:36:20] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch12_model1.pt.
[06.22.21|13:36:20] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch12_model2.pt.
[06.22.21|13:36:20] Training epoch: 13
[06.22.21|13:36:38] 	Iter 30100 Done. | loss1: 1.1376 | loss_class: 1.1371 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|13:37:28] 	Iter 30200 Done. | loss1: 1.3619 | loss_class: 1.3612 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:38:19] 	Iter 30300 Done. | loss1: 1.3873 | loss_class: 1.3866 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:39:09] 	Iter 30400 Done. | loss1: 2.5029 | loss_class: 2.5024 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|13:39:59] 	Iter 30500 Done. | loss1: 1.3272 | loss_class: 1.3266 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|13:40:50] 	Iter 30600 Done. | loss1: 1.2025 | loss_class: 1.2019 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|13:41:41] 	Iter 30700 Done. | loss1: 2.2748 | loss_class: 2.2743 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:42:33] 	Iter 30800 Done. | loss1: 2.0426 | loss_class: 2.0419 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:43:24] 	Iter 30900 Done. | loss1: 2.2251 | loss_class: 2.2247 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|13:44:14] 	Iter 31000 Done. | loss1: 2.8963 | loss_class: 2.8958 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:45:05] 	Iter 31100 Done. | loss1: 1.0778 | loss_class: 1.0771 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:45:56] 	Iter 31200 Done. | loss1: 1.8507 | loss_class: 1.8503 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|13:46:46] 	Iter 31300 Done. | loss1: 2.5672 | loss_class: 2.5665 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:47:37] 	Iter 31400 Done. | loss1: 1.8199 | loss_class: 1.8192 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:48:30] 	Iter 31500 Done. | loss1: 1.7019 | loss_class: 1.7011 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:49:21] 	Iter 31600 Done. | loss1: 3.5348 | loss_class: 3.5342 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:50:12] 	Iter 31700 Done. | loss1: 2.7086 | loss_class: 2.7080 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:51:03] 	Iter 31800 Done. | loss1: 1.9704 | loss_class: 1.9698 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:51:53] 	Iter 31900 Done. | loss1: 0.6817 | loss_class: 0.6809 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|13:52:45] 	Iter 32000 Done. | loss1: 2.2648 | loss_class: 2.2642 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:53:35] 	Iter 32100 Done. | loss1: 3.8709 | loss_class: 3.8699 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|13:54:26] 	Iter 32200 Done. | loss1: 0.9954 | loss_class: 0.9948 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:55:17] 	Iter 32300 Done. | loss1: 2.6178 | loss_class: 2.6170 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:56:08] 	Iter 32400 Done. | loss1: 0.9164 | loss_class: 0.9156 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:56:58] 	Iter 32500 Done. | loss1: 2.4806 | loss_class: 2.4800 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:57:48] 	Iter 32600 Done. | loss1: 2.3801 | loss_class: 2.3795 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|13:58:38] 	Iter 32700 Done. | loss1: 1.8777 | loss_class: 1.8769 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|13:59:29] 	Iter 32800 Done. | loss1: 1.7278 | loss_class: 1.7271 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:00:19] 	Iter 32900 Done. | loss1: 2.3043 | loss_class: 2.3036 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:01:10] 	Iter 33000 Done. | loss1: 1.2085 | loss_class: 1.2079 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:02:00] 	Iter 33100 Done. | loss1: 2.4266 | loss_class: 2.4260 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:02:51] 	Iter 33200 Done. | loss1: 1.2792 | loss_class: 1.2784 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:03:43] 	Iter 33300 Done. | loss1: 2.0531 | loss_class: 2.0526 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:04:33] 	Iter 33400 Done. | loss1: 2.7661 | loss_class: 2.7657 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|14:05:24] 	Iter 33500 Done. | loss1: 1.0593 | loss_class: 1.0585 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|14:06:15] 	Iter 33600 Done. | loss1: 1.6772 | loss_class: 1.6765 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:07:05] 	Iter 33700 Done. | loss1: 1.1118 | loss_class: 1.1114 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:07:56] 	Iter 33800 Done. | loss1: 1.3418 | loss_class: 1.3414 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|14:08:47] 	Iter 33900 Done. | loss1: 1.2336 | loss_class: 1.2329 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:09:38] 	Iter 34000 Done. | loss1: 1.9943 | loss_class: 1.9937 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:10:29] 	Iter 34100 Done. | loss1: 1.5618 | loss_class: 1.5610 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|14:11:19] 	Iter 34200 Done. | loss1: 2.2623 | loss_class: 2.2614 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|14:12:10] 	Iter 34300 Done. | loss1: 1.9996 | loss_class: 1.9988 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|14:13:00] 	Iter 34400 Done. | loss1: 2.2484 | loss_class: 2.2478 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:13:51] 	Iter 34500 Done. | loss1: 2.4515 | loss_class: 2.4508 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|14:14:41] 	Iter 34600 Done. | loss1: 1.9611 | loss_class: 1.9606 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:15:32] 	Iter 34700 Done. | loss1: 2.2071 | loss_class: 2.2064 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:16:22] 	Iter 34800 Done. | loss1: 0.9154 | loss_class: 0.9149 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:17:13] 	Iter 34900 Done. | loss1: 1.8519 | loss_class: 1.8515 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:18:03] 	Iter 35000 Done. | loss1: 1.8339 | loss_class: 1.8335 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:18:54] 	Iter 35100 Done. | loss1: 2.7936 | loss_class: 2.7931 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:19:44] 	Iter 35200 Done. | loss1: 1.4865 | loss_class: 1.4859 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:20:34] 	Iter 35300 Done. | loss1: 2.1750 | loss_class: 2.1742 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:21:25] 	Iter 35400 Done. | loss1: 2.4565 | loss_class: 2.4559 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:22:16] 	Iter 35500 Done. | loss1: 3.0926 | loss_class: 3.0921 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:23:06] 	Iter 35600 Done. | loss1: 0.9146 | loss_class: 0.9141 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:23:57] 	Iter 35700 Done. | loss1: 1.2152 | loss_class: 1.2145 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:24:47] 	Iter 35800 Done. | loss1: 0.9405 | loss_class: 0.9399 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:25:38] 	Iter 35900 Done. | loss1: 1.9544 | loss_class: 1.9537 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:26:28] 	Iter 36000 Done. | loss1: 2.5426 | loss_class: 2.5418 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:27:19] 	Iter 36100 Done. | loss1: 3.1918 | loss_class: 3.1912 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:28:09] 	Iter 36200 Done. | loss1: 1.5986 | loss_class: 1.5978 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|14:29:00] 	Iter 36300 Done. | loss1: 1.2964 | loss_class: 1.2958 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:29:50] 	Iter 36400 Done. | loss1: 3.0426 | loss_class: 3.0421 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:30:41] 	Iter 36500 Done. | loss1: 1.1209 | loss_class: 1.1202 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|14:31:32] 	Iter 36600 Done. | loss1: 1.1964 | loss_class: 1.1957 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:32:23] 	Iter 36700 Done. | loss1: 1.6910 | loss_class: 1.6904 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:33:13] 	Iter 36800 Done. | loss1: 1.5139 | loss_class: 1.5132 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:34:04] 	Iter 36900 Done. | loss1: 1.6723 | loss_class: 1.6718 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:34:54] 	Iter 37000 Done. | loss1: 1.4284 | loss_class: 1.4277 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:35:45] 	Iter 37100 Done. | loss1: 1.4755 | loss_class: 1.4750 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:36:35] 	Iter 37200 Done. | loss1: 1.5310 | loss_class: 1.5304 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:37:26] 	Iter 37300 Done. | loss1: 1.7026 | loss_class: 1.7021 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:38:17] 	Iter 37400 Done. | loss1: 1.7346 | loss_class: 1.7341 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:39:08] 	Iter 37500 Done. | loss1: 1.5836 | loss_class: 1.5830 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:39:58] 	Iter 37600 Done. | loss1: 1.1122 | loss_class: 1.1117 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:40:48] 	Iter 37700 Done. | loss1: 0.6825 | loss_class: 0.6820 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:41:38] 	Iter 37800 Done. | loss1: 1.7280 | loss_class: 1.7275 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:42:28] 	Iter 37900 Done. | loss1: 2.3994 | loss_class: 2.3986 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|14:43:20] 	Iter 38000 Done. | loss1: 2.1510 | loss_class: 2.1501 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|14:44:10] 	Iter 38100 Done. | loss1: 0.8020 | loss_class: 0.8015 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:45:00] 	Iter 38200 Done. | loss1: 2.1591 | loss_class: 2.1586 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:45:50] 	Iter 38300 Done. | loss1: 2.2395 | loss_class: 2.2389 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:46:41] 	Iter 38400 Done. | loss1: 1.5026 | loss_class: 1.5019 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:47:32] 	Iter 38500 Done. | loss1: 2.2255 | loss_class: 2.2248 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:48:22] 	Iter 38600 Done. | loss1: 2.2846 | loss_class: 2.2839 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:49:13] 	Iter 38700 Done. | loss1: 1.6552 | loss_class: 1.6547 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:50:03] 	Iter 38800 Done. | loss1: 2.4954 | loss_class: 2.4950 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|14:50:54] 	Iter 38900 Done. | loss1: 2.8299 | loss_class: 2.8290 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|14:51:44] 	Iter 39000 Done. | loss1: 1.6431 | loss_class: 1.6424 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:52:35] 	Iter 39100 Done. | loss1: 1.4180 | loss_class: 1.4173 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:53:25] 	Iter 39200 Done. | loss1: 1.6418 | loss_class: 1.6412 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:54:16] 	Iter 39300 Done. | loss1: 1.2880 | loss_class: 1.2875 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|14:55:06] 	Iter 39400 Done. | loss1: 1.0357 | loss_class: 1.0349 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:55:56] 	Iter 39500 Done. | loss1: 2.0524 | loss_class: 2.0517 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:56:47] 	Iter 39600 Done. | loss1: 1.3327 | loss_class: 1.3320 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|14:57:37] 	Iter 39700 Done. | loss1: 3.3598 | loss_class: 3.3591 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|14:58:28] 	Iter 39800 Done. | loss1: 2.6345 | loss_class: 2.6339 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|14:59:18] 	Iter 39900 Done. | loss1: 0.6712 | loss_class: 0.6704 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|15:00:09] 	Iter 40000 Done. | loss1: 2.0866 | loss_class: 2.0860 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:00:53] 	mean_loss1: 1.9588647985132457
[06.22.21|15:00:53] 	mean_loss_class: 1.958221244090297
[06.22.21|15:00:53] 	mean_loss_recon: 0.0006435545175324221
[06.22.21|15:00:53] Time consumption:
[06.22.21|15:00:53] Done.
[06.22.21|15:00:53] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch13_model1.pt.
[06.22.21|15:00:53] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch13_model2.pt.
[06.22.21|15:00:53] Training epoch: 14
[06.22.21|15:01:00] 	Iter 40100 Done. | loss1: 0.7762 | loss_class: 0.7757 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:01:50] 	Iter 40200 Done. | loss1: 1.3649 | loss_class: 1.3641 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:02:40] 	Iter 40300 Done. | loss1: 2.0236 | loss_class: 2.0229 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:03:31] 	Iter 40400 Done. | loss1: 1.2977 | loss_class: 1.2971 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:04:21] 	Iter 40500 Done. | loss1: 2.3347 | loss_class: 2.3341 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:05:12] 	Iter 40600 Done. | loss1: 1.8899 | loss_class: 1.8894 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|15:06:02] 	Iter 40700 Done. | loss1: 1.8232 | loss_class: 1.8226 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:06:52] 	Iter 40800 Done. | loss1: 1.6889 | loss_class: 1.6884 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|15:07:43] 	Iter 40900 Done. | loss1: 0.4293 | loss_class: 0.4286 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:08:33] 	Iter 41000 Done. | loss1: 0.9276 | loss_class: 0.9270 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:09:24] 	Iter 41100 Done. | loss1: 0.3702 | loss_class: 0.3694 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|15:10:14] 	Iter 41200 Done. | loss1: 0.6505 | loss_class: 0.6498 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:11:05] 	Iter 41300 Done. | loss1: 0.7898 | loss_class: 0.7891 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:11:56] 	Iter 41400 Done. | loss1: 2.2949 | loss_class: 2.2943 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|15:12:46] 	Iter 41500 Done. | loss1: 0.6041 | loss_class: 0.6034 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:13:35] 	Iter 41600 Done. | loss1: 1.5514 | loss_class: 1.5509 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|15:14:25] 	Iter 41700 Done. | loss1: 2.6031 | loss_class: 2.6024 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:15:15] 	Iter 41800 Done. | loss1: 2.1292 | loss_class: 2.1286 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:16:05] 	Iter 41900 Done. | loss1: 2.6845 | loss_class: 2.6838 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:16:55] 	Iter 42000 Done. | loss1: 1.2288 | loss_class: 1.2282 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:17:45] 	Iter 42100 Done. | loss1: 1.8210 | loss_class: 1.8203 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:18:35] 	Iter 42200 Done. | loss1: 1.1537 | loss_class: 1.1530 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:19:29] 	Iter 42300 Done. | loss1: 1.9601 | loss_class: 1.9595 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:20:24] 	Iter 42400 Done. | loss1: 1.2371 | loss_class: 1.2364 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:21:20] 	Iter 42500 Done. | loss1: 1.4440 | loss_class: 1.4433 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:22:14] 	Iter 42600 Done. | loss1: 1.7374 | loss_class: 1.7369 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:23:05] 	Iter 42700 Done. | loss1: 1.5361 | loss_class: 1.5356 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:23:54] 	Iter 42800 Done. | loss1: 1.3767 | loss_class: 1.3758 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|15:24:44] 	Iter 42900 Done. | loss1: 1.8893 | loss_class: 1.8886 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:25:34] 	Iter 43000 Done. | loss1: 1.9492 | loss_class: 1.9486 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|15:26:24] 	Iter 43100 Done. | loss1: 2.4672 | loss_class: 2.4666 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:27:14] 	Iter 43200 Done. | loss1: 0.6662 | loss_class: 0.6657 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|15:28:04] 	Iter 43300 Done. | loss1: 2.4192 | loss_class: 2.4186 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:28:53] 	Iter 43400 Done. | loss1: 1.8330 | loss_class: 1.8322 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|15:29:43] 	Iter 43500 Done. | loss1: 0.5390 | loss_class: 0.5382 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:30:33] 	Iter 43600 Done. | loss1: 1.6863 | loss_class: 1.6857 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:31:23] 	Iter 43700 Done. | loss1: 2.0199 | loss_class: 2.0191 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|15:32:13] 	Iter 43800 Done. | loss1: 0.6086 | loss_class: 0.6078 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|15:33:03] 	Iter 43900 Done. | loss1: 1.2297 | loss_class: 1.2291 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:33:53] 	Iter 44000 Done. | loss1: 2.9813 | loss_class: 2.9805 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|15:34:43] 	Iter 44100 Done. | loss1: 1.7507 | loss_class: 1.7501 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:35:36] 	Iter 44200 Done. | loss1: 1.1475 | loss_class: 1.1467 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|15:36:27] 	Iter 44300 Done. | loss1: 2.2730 | loss_class: 2.2725 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|15:37:17] 	Iter 44400 Done. | loss1: 1.0625 | loss_class: 1.0619 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:38:07] 	Iter 44500 Done. | loss1: 1.1629 | loss_class: 1.1624 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|15:38:57] 	Iter 44600 Done. | loss1: 1.1335 | loss_class: 1.1329 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:39:47] 	Iter 44700 Done. | loss1: 2.0969 | loss_class: 2.0961 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|15:40:37] 	Iter 44800 Done. | loss1: 1.0176 | loss_class: 1.0169 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:41:29] 	Iter 44900 Done. | loss1: 0.8134 | loss_class: 0.8127 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:42:22] 	Iter 45000 Done. | loss1: 1.5002 | loss_class: 1.4996 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:43:12] 	Iter 45100 Done. | loss1: 1.6890 | loss_class: 1.6885 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|15:44:03] 	Iter 45200 Done. | loss1: 1.4613 | loss_class: 1.4606 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:44:53] 	Iter 45300 Done. | loss1: 0.2044 | loss_class: 0.2038 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:45:43] 	Iter 45400 Done. | loss1: 2.0863 | loss_class: 2.0857 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:46:33] 	Iter 45500 Done. | loss1: 1.6636 | loss_class: 1.6629 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:47:22] 	Iter 45600 Done. | loss1: 2.1942 | loss_class: 2.1934 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|15:48:13] 	Iter 45700 Done. | loss1: 1.9073 | loss_class: 1.9067 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:49:03] 	Iter 45800 Done. | loss1: 2.2116 | loss_class: 2.2109 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:49:53] 	Iter 45900 Done. | loss1: 2.6040 | loss_class: 2.6034 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:50:47] 	Iter 46000 Done. | loss1: 1.3588 | loss_class: 1.3584 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|15:51:37] 	Iter 46100 Done. | loss1: 1.2102 | loss_class: 1.2096 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:52:30] 	Iter 46200 Done. | loss1: 0.7839 | loss_class: 0.7834 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|15:53:21] 	Iter 46300 Done. | loss1: 1.4776 | loss_class: 1.4768 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:54:11] 	Iter 46400 Done. | loss1: 1.1367 | loss_class: 1.1360 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:55:01] 	Iter 46500 Done. | loss1: 2.1220 | loss_class: 2.1212 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|15:55:52] 	Iter 46600 Done. | loss1: 1.3978 | loss_class: 1.3971 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|15:56:42] 	Iter 46700 Done. | loss1: 2.7914 | loss_class: 2.7910 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|15:57:32] 	Iter 46800 Done. | loss1: 1.3074 | loss_class: 1.3069 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|15:58:23] 	Iter 46900 Done. | loss1: 3.8321 | loss_class: 3.8312 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|15:59:13] 	Iter 47000 Done. | loss1: 1.7607 | loss_class: 1.7601 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:00:03] 	Iter 47100 Done. | loss1: 3.1356 | loss_class: 3.1348 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|16:00:53] 	Iter 47200 Done. | loss1: 1.0507 | loss_class: 1.0501 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:01:43] 	Iter 47300 Done. | loss1: 1.3884 | loss_class: 1.3878 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:02:33] 	Iter 47400 Done. | loss1: 0.4241 | loss_class: 0.4231 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|16:03:25] 	Iter 47500 Done. | loss1: 1.7933 | loss_class: 1.7927 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:04:15] 	Iter 47600 Done. | loss1: 1.5917 | loss_class: 1.5912 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|16:05:06] 	Iter 47700 Done. | loss1: 1.4419 | loss_class: 1.4412 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:05:56] 	Iter 47800 Done. | loss1: 5.1168 | loss_class: 5.1162 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:06:47] 	Iter 47900 Done. | loss1: 1.1892 | loss_class: 1.1886 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:07:37] 	Iter 48000 Done. | loss1: 1.6084 | loss_class: 1.6078 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:08:28] 	Iter 48100 Done. | loss1: 1.2936 | loss_class: 1.2930 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:09:18] 	Iter 48200 Done. | loss1: 1.7763 | loss_class: 1.7757 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:10:12] 	Iter 48300 Done. | loss1: 1.5419 | loss_class: 1.5410 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|16:11:03] 	Iter 48400 Done. | loss1: 2.1699 | loss_class: 2.1693 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|16:11:53] 	Iter 48500 Done. | loss1: 2.5513 | loss_class: 2.5505 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|16:12:43] 	Iter 48600 Done. | loss1: 1.9983 | loss_class: 1.9976 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:13:33] 	Iter 48700 Done. | loss1: 2.0012 | loss_class: 2.0005 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:14:24] 	Iter 48800 Done. | loss1: 2.7903 | loss_class: 2.7896 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:15:15] 	Iter 48900 Done. | loss1: 1.7908 | loss_class: 1.7900 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|16:16:06] 	Iter 49000 Done. | loss1: 3.1774 | loss_class: 3.1770 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|16:16:56] 	Iter 49100 Done. | loss1: 1.2046 | loss_class: 1.2041 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:17:46] 	Iter 49200 Done. | loss1: 1.9806 | loss_class: 1.9799 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:18:36] 	Iter 49300 Done. | loss1: 1.6411 | loss_class: 1.6405 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|16:19:25] 	Iter 49400 Done. | loss1: 1.2603 | loss_class: 1.2597 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:20:15] 	Iter 49500 Done. | loss1: 0.5578 | loss_class: 0.5573 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|16:21:06] 	Iter 49600 Done. | loss1: 0.9314 | loss_class: 0.9307 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:22:00] 	Iter 49700 Done. | loss1: 2.4169 | loss_class: 2.4162 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:22:50] 	Iter 49800 Done. | loss1: 2.6795 | loss_class: 2.6790 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|16:23:41] 	Iter 49900 Done. | loss1: 1.7529 | loss_class: 1.7522 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:24:32] 	Iter 50000 Done. | loss1: 3.6808 | loss_class: 3.6802 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:25:22] 	Iter 50100 Done. | loss1: 1.3286 | loss_class: 1.3280 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:25:27] 	mean_loss1: 1.7612805409188566
[06.22.21|16:25:27] 	mean_loss_class: 1.7606348555071927
[06.22.21|16:25:27] 	mean_loss_recon: 0.0006456857473825331
[06.22.21|16:25:27] Time consumption:
[06.22.21|16:25:27] Done.
[06.22.21|16:25:27] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch14_model1.pt.
[06.22.21|16:25:27] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch14_model2.pt.
[06.22.21|16:25:27] Eval epoch: 14
[06.22.21|16:32:01] 	mean_loss1: 1.6175128412108088
[06.22.21|16:32:01] 	mean_loss_class: 1.6171692533548487
[06.22.21|16:32:01] 	mean_loss_recon: 0.03435886180923544
[06.22.21|16:32:01] 

[06.22.21|16:32:01] 	Top1: 51.98%
[06.22.21|16:32:01] 

[06.22.21|16:32:01] 	Top5: 85.84%
[06.22.21|16:32:01] Done.
[06.22.21|16:32:01] Training epoch: 15
[06.22.21|16:32:47] 	Iter 50200 Done. | loss1: 1.3757 | loss_class: 1.3750 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:33:37] 	Iter 50300 Done. | loss1: 1.4993 | loss_class: 1.4984 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|16:34:28] 	Iter 50400 Done. | loss1: 0.7594 | loss_class: 0.7589 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|16:35:18] 	Iter 50500 Done. | loss1: 3.4600 | loss_class: 3.4593 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:36:10] 	Iter 50600 Done. | loss1: 2.5894 | loss_class: 2.5888 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:37:01] 	Iter 50700 Done. | loss1: 1.0131 | loss_class: 1.0124 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:37:52] 	Iter 50800 Done. | loss1: 1.6107 | loss_class: 1.6101 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:38:42] 	Iter 50900 Done. | loss1: 1.3738 | loss_class: 1.3733 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|16:39:33] 	Iter 51000 Done. | loss1: 1.0391 | loss_class: 1.0381 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|16:40:24] 	Iter 51100 Done. | loss1: 1.8255 | loss_class: 1.8249 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:41:17] 	Iter 51200 Done. | loss1: 1.1169 | loss_class: 1.1162 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:42:07] 	Iter 51300 Done. | loss1: 2.0754 | loss_class: 2.0748 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:42:58] 	Iter 51400 Done. | loss1: 0.6502 | loss_class: 0.6493 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|16:43:49] 	Iter 51500 Done. | loss1: 0.5592 | loss_class: 0.5584 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|16:44:39] 	Iter 51600 Done. | loss1: 0.4994 | loss_class: 0.4986 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|16:45:29] 	Iter 51700 Done. | loss1: 0.7469 | loss_class: 0.7462 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:46:19] 	Iter 51800 Done. | loss1: 1.5778 | loss_class: 1.5773 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|16:47:10] 	Iter 51900 Done. | loss1: 0.5775 | loss_class: 0.5767 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|16:48:00] 	Iter 52000 Done. | loss1: 2.0905 | loss_class: 2.0899 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:48:51] 	Iter 52100 Done. | loss1: 0.2108 | loss_class: 0.2101 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:49:41] 	Iter 52200 Done. | loss1: 2.4510 | loss_class: 2.4504 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:50:32] 	Iter 52300 Done. | loss1: 1.7036 | loss_class: 1.7028 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|16:51:24] 	Iter 52400 Done. | loss1: 1.4087 | loss_class: 1.4081 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:52:16] 	Iter 52500 Done. | loss1: 2.3563 | loss_class: 2.3557 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:53:07] 	Iter 52600 Done. | loss1: 1.6497 | loss_class: 1.6489 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|16:53:57] 	Iter 52700 Done. | loss1: 0.6746 | loss_class: 0.6740 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|16:54:47] 	Iter 52800 Done. | loss1: 2.2274 | loss_class: 2.2267 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:55:37] 	Iter 52900 Done. | loss1: 1.3292 | loss_class: 1.3284 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|16:56:27] 	Iter 53000 Done. | loss1: 1.6572 | loss_class: 1.6563 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|16:57:17] 	Iter 53100 Done. | loss1: 1.8608 | loss_class: 1.8601 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:58:08] 	Iter 53200 Done. | loss1: 1.3696 | loss_class: 1.3689 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|16:58:58] 	Iter 53300 Done. | loss1: 0.3587 | loss_class: 0.3582 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|16:59:49] 	Iter 53400 Done. | loss1: 1.0933 | loss_class: 1.0925 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:00:40] 	Iter 53500 Done. | loss1: 1.4458 | loss_class: 1.4453 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:01:30] 	Iter 53600 Done. | loss1: 2.8656 | loss_class: 2.8648 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|17:02:20] 	Iter 53700 Done. | loss1: 1.1158 | loss_class: 1.1151 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:03:12] 	Iter 53800 Done. | loss1: 1.6300 | loss_class: 1.6293 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:04:02] 	Iter 53900 Done. | loss1: 2.6263 | loss_class: 2.6259 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:04:54] 	Iter 54000 Done. | loss1: 0.5298 | loss_class: 0.5290 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|17:05:45] 	Iter 54100 Done. | loss1: 3.2538 | loss_class: 3.2531 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:06:38] 	Iter 54200 Done. | loss1: 1.4916 | loss_class: 1.4910 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:07:29] 	Iter 54300 Done. | loss1: 2.1513 | loss_class: 2.1507 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:08:20] 	Iter 54400 Done. | loss1: 0.6927 | loss_class: 0.6920 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:09:10] 	Iter 54500 Done. | loss1: 2.1034 | loss_class: 2.1028 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:10:00] 	Iter 54600 Done. | loss1: 1.4238 | loss_class: 1.4233 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:10:53] 	Iter 54700 Done. | loss1: 1.3446 | loss_class: 1.3441 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:11:43] 	Iter 54800 Done. | loss1: 1.9979 | loss_class: 1.9974 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:12:34] 	Iter 54900 Done. | loss1: 2.7392 | loss_class: 2.7387 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:13:24] 	Iter 55000 Done. | loss1: 2.5849 | loss_class: 2.5844 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:14:15] 	Iter 55100 Done. | loss1: 0.8609 | loss_class: 0.8603 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:15:07] 	Iter 55200 Done. | loss1: 1.2686 | loss_class: 1.2681 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:15:59] 	Iter 55300 Done. | loss1: 1.1013 | loss_class: 1.1006 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:16:50] 	Iter 55400 Done. | loss1: 2.1612 | loss_class: 2.1608 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|17:17:41] 	Iter 55500 Done. | loss1: 1.7405 | loss_class: 1.7399 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:18:30] 	Iter 55600 Done. | loss1: 2.8296 | loss_class: 2.8289 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:19:20] 	Iter 55700 Done. | loss1: 1.0390 | loss_class: 1.0381 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|17:20:10] 	Iter 55800 Done. | loss1: 0.8025 | loss_class: 0.8017 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|17:21:00] 	Iter 55900 Done. | loss1: 1.9040 | loss_class: 1.9032 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|17:21:51] 	Iter 56000 Done. | loss1: 0.9943 | loss_class: 0.9937 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:22:41] 	Iter 56100 Done. | loss1: 2.8856 | loss_class: 2.8851 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:23:36] 	Iter 56200 Done. | loss1: 2.8495 | loss_class: 2.8488 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|17:24:27] 	Iter 56300 Done. | loss1: 1.4422 | loss_class: 1.4416 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:25:17] 	Iter 56400 Done. | loss1: 1.8577 | loss_class: 1.8571 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:26:07] 	Iter 56500 Done. | loss1: 1.0758 | loss_class: 1.0751 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:26:58] 	Iter 56600 Done. | loss1: 1.5557 | loss_class: 1.5549 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|17:27:48] 	Iter 56700 Done. | loss1: 1.1016 | loss_class: 1.1009 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:28:40] 	Iter 56800 Done. | loss1: 0.4677 | loss_class: 0.4670 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:29:31] 	Iter 56900 Done. | loss1: 0.8058 | loss_class: 0.8052 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:30:27] 	Iter 57000 Done. | loss1: 1.0656 | loss_class: 1.0650 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:31:20] 	Iter 57100 Done. | loss1: 1.5797 | loss_class: 1.5790 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:32:11] 	Iter 57200 Done. | loss1: 2.2105 | loss_class: 2.2099 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:33:04] 	Iter 57300 Done. | loss1: 1.1712 | loss_class: 1.1705 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:33:54] 	Iter 57400 Done. | loss1: 3.1064 | loss_class: 3.1059 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:34:45] 	Iter 57500 Done. | loss1: 2.6462 | loss_class: 2.6457 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:35:36] 	Iter 57600 Done. | loss1: 0.5908 | loss_class: 0.5901 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:36:28] 	Iter 57700 Done. | loss1: 1.8732 | loss_class: 1.8725 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:37:19] 	Iter 57800 Done. | loss1: 3.4104 | loss_class: 3.4098 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:38:09] 	Iter 57900 Done. | loss1: 2.0854 | loss_class: 2.0848 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:39:00] 	Iter 58000 Done. | loss1: 1.9204 | loss_class: 1.9198 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:39:50] 	Iter 58100 Done. | loss1: 0.8426 | loss_class: 0.8418 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:40:41] 	Iter 58200 Done. | loss1: 0.4174 | loss_class: 0.4168 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:41:33] 	Iter 58300 Done. | loss1: 1.2039 | loss_class: 1.2032 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:42:25] 	Iter 58400 Done. | loss1: 1.0132 | loss_class: 1.0125 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:43:17] 	Iter 58500 Done. | loss1: 0.6866 | loss_class: 0.6860 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:44:08] 	Iter 58600 Done. | loss1: 0.7592 | loss_class: 0.7585 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:45:01] 	Iter 58700 Done. | loss1: 1.2596 | loss_class: 1.2589 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:45:52] 	Iter 58800 Done. | loss1: 0.9998 | loss_class: 0.9990 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|17:46:42] 	Iter 58900 Done. | loss1: 0.9673 | loss_class: 0.9667 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:47:33] 	Iter 59000 Done. | loss1: 0.8545 | loss_class: 0.8539 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:48:23] 	Iter 59100 Done. | loss1: 1.3855 | loss_class: 1.3849 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:49:13] 	Iter 59200 Done. | loss1: 2.6484 | loss_class: 2.6479 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:50:03] 	Iter 59300 Done. | loss1: 2.5118 | loss_class: 2.5112 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:50:53] 	Iter 59400 Done. | loss1: 1.3890 | loss_class: 1.3884 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:51:43] 	Iter 59500 Done. | loss1: 2.9061 | loss_class: 2.9056 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:52:34] 	Iter 59600 Done. | loss1: 2.0889 | loss_class: 2.0881 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|17:53:24] 	Iter 59700 Done. | loss1: 1.2397 | loss_class: 1.2391 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:54:15] 	Iter 59800 Done. | loss1: 1.9338 | loss_class: 1.9333 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:55:05] 	Iter 59900 Done. | loss1: 1.1612 | loss_class: 1.1606 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:55:55] 	Iter 60000 Done. | loss1: 1.9314 | loss_class: 1.9307 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|17:56:45] 	Iter 60100 Done. | loss1: 0.8548 | loss_class: 0.8542 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|17:57:01] 	mean_loss1: 1.6420399826469774
[06.22.21|17:57:01] 	mean_loss_class: 1.641390726734135
[06.22.21|17:57:01] 	mean_loss_recon: 0.0006492556051844204
[06.22.21|17:57:01] Time consumption:
[06.22.21|17:57:01] Done.
[06.22.21|17:57:01] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch15_model1.pt.
[06.22.21|17:57:01] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch15_model2.pt.
[06.22.21|17:57:01] Training epoch: 16
[06.22.21|17:57:36] 	Iter 60200 Done. | loss1: 0.8741 | loss_class: 0.8735 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|17:58:27] 	Iter 60300 Done. | loss1: 3.5651 | loss_class: 3.5643 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|17:59:17] 	Iter 60400 Done. | loss1: 0.6642 | loss_class: 0.6637 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|18:00:08] 	Iter 60500 Done. | loss1: 1.4468 | loss_class: 1.4462 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:00:58] 	Iter 60600 Done. | loss1: 0.7444 | loss_class: 0.7437 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:01:49] 	Iter 60700 Done. | loss1: 3.0277 | loss_class: 3.0270 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:02:38] 	Iter 60800 Done. | loss1: 1.7607 | loss_class: 1.7601 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|18:03:30] 	Iter 60900 Done. | loss1: 1.1860 | loss_class: 1.1855 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|18:04:24] 	Iter 61000 Done. | loss1: 1.7272 | loss_class: 1.7266 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|18:05:14] 	Iter 61100 Done. | loss1: 1.9911 | loss_class: 1.9904 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:06:04] 	Iter 61200 Done. | loss1: 0.8929 | loss_class: 0.8922 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:06:54] 	Iter 61300 Done. | loss1: 2.5924 | loss_class: 2.5918 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:07:44] 	Iter 61400 Done. | loss1: 0.8417 | loss_class: 0.8410 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|18:08:35] 	Iter 61500 Done. | loss1: 1.9692 | loss_class: 1.9687 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|18:09:25] 	Iter 61600 Done. | loss1: 0.6788 | loss_class: 0.6784 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|18:10:15] 	Iter 61700 Done. | loss1: 1.8703 | loss_class: 1.8696 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:11:05] 	Iter 61800 Done. | loss1: 3.7205 | loss_class: 3.7198 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:11:55] 	Iter 61900 Done. | loss1: 0.9854 | loss_class: 0.9850 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|18:12:45] 	Iter 62000 Done. | loss1: 0.9350 | loss_class: 0.9343 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:13:35] 	Iter 62100 Done. | loss1: 1.9039 | loss_class: 1.9030 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|18:14:25] 	Iter 62200 Done. | loss1: 1.9165 | loss_class: 1.9159 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:15:16] 	Iter 62300 Done. | loss1: 1.3214 | loss_class: 1.3208 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:16:06] 	Iter 62400 Done. | loss1: 1.6836 | loss_class: 1.6830 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:16:56] 	Iter 62500 Done. | loss1: 0.8136 | loss_class: 0.8130 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:17:46] 	Iter 62600 Done. | loss1: 1.2849 | loss_class: 1.2844 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|18:18:36] 	Iter 62700 Done. | loss1: 3.1327 | loss_class: 3.1320 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:19:26] 	Iter 62800 Done. | loss1: 1.7025 | loss_class: 1.7018 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:20:16] 	Iter 62900 Done. | loss1: 2.3845 | loss_class: 2.3838 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:21:06] 	Iter 63000 Done. | loss1: 1.9162 | loss_class: 1.9155 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:21:57] 	Iter 63100 Done. | loss1: 2.9370 | loss_class: 2.9364 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:22:47] 	Iter 63200 Done. | loss1: 0.5770 | loss_class: 0.5763 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:23:40] 	Iter 63300 Done. | loss1: 0.4970 | loss_class: 0.4964 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|18:24:30] 	Iter 63400 Done. | loss1: 1.6343 | loss_class: 1.6335 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|18:25:20] 	Iter 63500 Done. | loss1: 1.3900 | loss_class: 1.3894 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:26:11] 	Iter 63600 Done. | loss1: 1.9440 | loss_class: 1.9432 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|18:27:01] 	Iter 63700 Done. | loss1: 0.9151 | loss_class: 0.9145 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|18:27:51] 	Iter 63800 Done. | loss1: 3.0482 | loss_class: 3.0478 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|18:28:41] 	Iter 63900 Done. | loss1: 2.6324 | loss_class: 2.6319 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:29:31] 	Iter 64000 Done. | loss1: 0.5574 | loss_class: 0.5567 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:30:20] 	Iter 64100 Done. | loss1: 1.5536 | loss_class: 1.5529 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:31:10] 	Iter 64200 Done. | loss1: 2.1454 | loss_class: 2.1446 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|18:32:03] 	Iter 64300 Done. | loss1: 2.0216 | loss_class: 2.0209 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:32:55] 	Iter 64400 Done. | loss1: 1.2932 | loss_class: 1.2926 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:33:45] 	Iter 64500 Done. | loss1: 2.2017 | loss_class: 2.2011 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:34:35] 	Iter 64600 Done. | loss1: 2.5334 | loss_class: 2.5329 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|18:35:25] 	Iter 64700 Done. | loss1: 1.0073 | loss_class: 1.0064 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|18:36:15] 	Iter 64800 Done. | loss1: 1.9997 | loss_class: 1.9992 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|18:37:04] 	Iter 64900 Done. | loss1: 1.6677 | loss_class: 1.6670 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:37:54] 	Iter 65000 Done. | loss1: 1.5394 | loss_class: 1.5387 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:38:43] 	Iter 65100 Done. | loss1: 1.4212 | loss_class: 1.4203 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|18:39:32] 	Iter 65200 Done. | loss1: 1.0441 | loss_class: 1.0436 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:40:22] 	Iter 65300 Done. | loss1: 0.5240 | loss_class: 0.5235 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|18:41:13] 	Iter 65400 Done. | loss1: 1.9534 | loss_class: 1.9528 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:42:03] 	Iter 65500 Done. | loss1: 1.9061 | loss_class: 1.9052 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|18:42:54] 	Iter 65600 Done. | loss1: 3.0508 | loss_class: 3.0500 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|18:43:44] 	Iter 65700 Done. | loss1: 0.4760 | loss_class: 0.4753 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:44:34] 	Iter 65800 Done. | loss1: 0.7131 | loss_class: 0.7125 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:45:24] 	Iter 65900 Done. | loss1: 1.4518 | loss_class: 1.4512 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:46:14] 	Iter 66000 Done. | loss1: 1.5738 | loss_class: 1.5732 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:47:05] 	Iter 66100 Done. | loss1: 2.0313 | loss_class: 2.0307 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:47:55] 	Iter 66200 Done. | loss1: 2.9893 | loss_class: 2.9883 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|18:48:46] 	Iter 66300 Done. | loss1: 0.5436 | loss_class: 0.5430 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:49:35] 	Iter 66400 Done. | loss1: 2.3051 | loss_class: 2.3045 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:50:26] 	Iter 66500 Done. | loss1: 0.7224 | loss_class: 0.7217 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:51:16] 	Iter 66600 Done. | loss1: 3.8123 | loss_class: 3.8116 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:52:10] 	Iter 66700 Done. | loss1: 1.5185 | loss_class: 1.5181 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|18:53:00] 	Iter 66800 Done. | loss1: 1.3052 | loss_class: 1.3045 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:53:50] 	Iter 66900 Done. | loss1: 1.0785 | loss_class: 1.0778 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:54:41] 	Iter 67000 Done. | loss1: 0.6104 | loss_class: 0.6097 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:55:31] 	Iter 67100 Done. | loss1: 4.1036 | loss_class: 4.1030 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:56:22] 	Iter 67200 Done. | loss1: 1.0480 | loss_class: 1.0474 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|18:57:12] 	Iter 67300 Done. | loss1: 0.4035 | loss_class: 0.4028 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|18:58:02] 	Iter 67400 Done. | loss1: 2.1060 | loss_class: 2.1056 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|18:58:53] 	Iter 67500 Done. | loss1: 1.4706 | loss_class: 1.4702 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|18:59:42] 	Iter 67600 Done. | loss1: 0.8601 | loss_class: 0.8596 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|19:00:32] 	Iter 67700 Done. | loss1: 0.3925 | loss_class: 0.3918 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:01:24] 	Iter 67800 Done. | loss1: 2.8827 | loss_class: 2.8818 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|19:02:14] 	Iter 67900 Done. | loss1: 1.1340 | loss_class: 1.1335 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|19:03:04] 	Iter 68000 Done. | loss1: 2.1773 | loss_class: 2.1768 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:03:55] 	Iter 68100 Done. | loss1: 1.3735 | loss_class: 1.3729 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:04:46] 	Iter 68200 Done. | loss1: 1.7374 | loss_class: 1.7366 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|19:05:39] 	Iter 68300 Done. | loss1: 1.0539 | loss_class: 1.0531 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|19:06:29] 	Iter 68400 Done. | loss1: 1.5021 | loss_class: 1.5017 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|19:07:19] 	Iter 68500 Done. | loss1: 0.6537 | loss_class: 0.6530 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:08:10] 	Iter 68600 Done. | loss1: 2.3459 | loss_class: 2.3453 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:09:00] 	Iter 68700 Done. | loss1: 1.2734 | loss_class: 1.2728 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:09:49] 	Iter 68800 Done. | loss1: 0.6512 | loss_class: 0.6504 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|19:10:39] 	Iter 68900 Done. | loss1: 1.9944 | loss_class: 1.9935 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|19:11:29] 	Iter 69000 Done. | loss1: 3.4629 | loss_class: 3.4622 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:12:19] 	Iter 69100 Done. | loss1: 1.4643 | loss_class: 1.4637 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:13:09] 	Iter 69200 Done. | loss1: 3.5087 | loss_class: 3.5080 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:13:59] 	Iter 69300 Done. | loss1: 0.2458 | loss_class: 0.2449 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|19:14:49] 	Iter 69400 Done. | loss1: 0.7244 | loss_class: 0.7238 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:15:39] 	Iter 69500 Done. | loss1: 1.9226 | loss_class: 1.9222 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|19:16:30] 	Iter 69600 Done. | loss1: 2.0187 | loss_class: 2.0178 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|19:17:19] 	Iter 69700 Done. | loss1: 0.7154 | loss_class: 0.7148 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:18:09] 	Iter 69800 Done. | loss1: 0.9074 | loss_class: 0.9069 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:18:59] 	Iter 69900 Done. | loss1: 1.3289 | loss_class: 1.3282 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:19:49] 	Iter 70000 Done. | loss1: 2.1032 | loss_class: 2.1026 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:20:39] 	Iter 70100 Done. | loss1: 2.0146 | loss_class: 2.0139 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:21:05] 	mean_loss1: 1.5664890702364838
[06.22.21|19:21:05] 	mean_loss_class: 1.5658415269379462
[06.22.21|19:21:05] 	mean_loss_recon: 0.0006475429636441325
[06.22.21|19:21:05] Time consumption:
[06.22.21|19:21:05] Done.
[06.22.21|19:21:05] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch16_model1.pt.
[06.22.21|19:21:05] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch16_model2.pt.
[06.22.21|19:21:05] Training epoch: 17
[06.22.21|19:21:29] 	Iter 70200 Done. | loss1: 0.9136 | loss_class: 0.9130 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:22:19] 	Iter 70300 Done. | loss1: 1.0195 | loss_class: 1.0188 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:23:10] 	Iter 70400 Done. | loss1: 1.0047 | loss_class: 1.0040 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:24:04] 	Iter 70500 Done. | loss1: 2.8671 | loss_class: 2.8667 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|19:24:58] 	Iter 70600 Done. | loss1: 2.1028 | loss_class: 2.1021 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:25:48] 	Iter 70700 Done. | loss1: 0.5637 | loss_class: 0.5631 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:26:38] 	Iter 70800 Done. | loss1: 1.4842 | loss_class: 1.4834 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|19:27:27] 	Iter 70900 Done. | loss1: 1.1206 | loss_class: 1.1201 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|19:28:19] 	Iter 71000 Done. | loss1: 0.9731 | loss_class: 0.9724 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:29:09] 	Iter 71100 Done. | loss1: 3.9752 | loss_class: 3.9746 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:30:00] 	Iter 71200 Done. | loss1: 1.3371 | loss_class: 1.3362 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|19:30:53] 	Iter 71300 Done. | loss1: 2.8300 | loss_class: 2.8293 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|19:31:44] 	Iter 71400 Done. | loss1: 0.6389 | loss_class: 0.6382 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:32:35] 	Iter 71500 Done. | loss1: 0.6556 | loss_class: 0.6551 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|19:33:25] 	Iter 71600 Done. | loss1: 1.1001 | loss_class: 1.0994 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:34:15] 	Iter 71700 Done. | loss1: 2.3149 | loss_class: 2.3143 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:35:05] 	Iter 71800 Done. | loss1: 1.7927 | loss_class: 1.7922 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|19:35:55] 	Iter 71900 Done. | loss1: 1.1435 | loss_class: 1.1429 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:36:45] 	Iter 72000 Done. | loss1: 2.1293 | loss_class: 2.1287 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:37:36] 	Iter 72100 Done. | loss1: 0.7648 | loss_class: 0.7640 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|19:38:26] 	Iter 72200 Done. | loss1: 1.5237 | loss_class: 1.5231 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:39:16] 	Iter 72300 Done. | loss1: 2.0007 | loss_class: 2.0001 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:40:06] 	Iter 72400 Done. | loss1: 0.9924 | loss_class: 0.9919 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|19:40:57] 	Iter 72500 Done. | loss1: 1.0021 | loss_class: 1.0015 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:41:46] 	Iter 72600 Done. | loss1: 2.4296 | loss_class: 2.4291 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|19:42:37] 	Iter 72700 Done. | loss1: 0.7296 | loss_class: 0.7290 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:43:27] 	Iter 72800 Done. | loss1: 2.1162 | loss_class: 2.1156 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:44:17] 	Iter 72900 Done. | loss1: 0.8917 | loss_class: 0.8912 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|19:45:08] 	Iter 73000 Done. | loss1: 2.4283 | loss_class: 2.4276 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:45:58] 	Iter 73100 Done. | loss1: 0.8836 | loss_class: 0.8830 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:46:48] 	Iter 73200 Done. | loss1: 0.6423 | loss_class: 0.6417 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:47:40] 	Iter 73300 Done. | loss1: 1.2066 | loss_class: 1.2061 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|19:48:30] 	Iter 73400 Done. | loss1: 1.3531 | loss_class: 1.3524 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:49:22] 	Iter 73500 Done. | loss1: 1.4113 | loss_class: 1.4108 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|19:50:14] 	Iter 73600 Done. | loss1: 3.0784 | loss_class: 3.0778 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:51:04] 	Iter 73700 Done. | loss1: 0.2012 | loss_class: 0.2003 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|19:51:54] 	Iter 73800 Done. | loss1: 1.6530 | loss_class: 1.6524 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:52:45] 	Iter 73900 Done. | loss1: 0.8884 | loss_class: 0.8876 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|19:53:35] 	Iter 74000 Done. | loss1: 3.3945 | loss_class: 3.3939 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|19:54:26] 	Iter 74100 Done. | loss1: 2.1784 | loss_class: 2.1776 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|19:55:15] 	Iter 74200 Done. | loss1: 1.7369 | loss_class: 1.7362 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:56:05] 	Iter 74300 Done. | loss1: 0.6195 | loss_class: 0.6187 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|19:56:55] 	Iter 74400 Done. | loss1: 0.8121 | loss_class: 0.8114 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:57:49] 	Iter 74500 Done. | loss1: 1.5585 | loss_class: 1.5578 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:58:40] 	Iter 74600 Done. | loss1: 1.8324 | loss_class: 1.8317 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|19:59:31] 	Iter 74700 Done. | loss1: 0.9487 | loss_class: 0.9482 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:00:23] 	Iter 74800 Done. | loss1: 1.5725 | loss_class: 1.5719 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:01:13] 	Iter 74900 Done. | loss1: 1.7222 | loss_class: 1.7216 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:02:04] 	Iter 75000 Done. | loss1: 1.2198 | loss_class: 1.2192 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|20:02:55] 	Iter 75100 Done. | loss1: 1.8766 | loss_class: 1.8758 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|20:03:46] 	Iter 75200 Done. | loss1: 1.5169 | loss_class: 1.5162 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:04:42] 	Iter 75300 Done. | loss1: 1.6658 | loss_class: 1.6650 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|20:05:37] 	Iter 75400 Done. | loss1: 0.9714 | loss_class: 0.9709 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|20:06:29] 	Iter 75500 Done. | loss1: 2.4256 | loss_class: 2.4244 | loss_recon: 0.0012 | lr: 0.100000
[06.22.21|20:07:20] 	Iter 75600 Done. | loss1: 1.5737 | loss_class: 1.5728 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|20:08:10] 	Iter 75700 Done. | loss1: 2.8508 | loss_class: 2.8503 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|20:09:00] 	Iter 75800 Done. | loss1: 0.2720 | loss_class: 0.2713 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:09:50] 	Iter 75900 Done. | loss1: 0.4253 | loss_class: 0.4246 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:10:40] 	Iter 76000 Done. | loss1: 2.0906 | loss_class: 2.0900 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|20:11:30] 	Iter 76100 Done. | loss1: 1.5118 | loss_class: 1.5114 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|20:12:21] 	Iter 76200 Done. | loss1: 3.9819 | loss_class: 3.9815 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|20:13:14] 	Iter 76300 Done. | loss1: 3.0409 | loss_class: 3.0403 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:14:05] 	Iter 76400 Done. | loss1: 0.8516 | loss_class: 0.8507 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|20:14:56] 	Iter 76500 Done. | loss1: 1.7759 | loss_class: 1.7752 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|20:15:48] 	Iter 76600 Done. | loss1: 0.9928 | loss_class: 0.9921 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:16:38] 	Iter 76700 Done. | loss1: 2.5670 | loss_class: 2.5665 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|20:17:28] 	Iter 76800 Done. | loss1: 1.7056 | loss_class: 1.7050 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:18:19] 	Iter 76900 Done. | loss1: 1.9290 | loss_class: 1.9284 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:19:09] 	Iter 77000 Done. | loss1: 1.4806 | loss_class: 1.4799 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:19:59] 	Iter 77100 Done. | loss1: 0.1563 | loss_class: 0.1555 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:20:50] 	Iter 77200 Done. | loss1: 0.5447 | loss_class: 0.5437 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|20:21:45] 	Iter 77300 Done. | loss1: 0.5145 | loss_class: 0.5138 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:22:38] 	Iter 77400 Done. | loss1: 0.3803 | loss_class: 0.3796 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:23:28] 	Iter 77500 Done. | loss1: 1.2263 | loss_class: 1.2257 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:24:19] 	Iter 77600 Done. | loss1: 0.6155 | loss_class: 0.6147 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:25:13] 	Iter 77700 Done. | loss1: 3.7448 | loss_class: 3.7441 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:26:03] 	Iter 77800 Done. | loss1: 0.7633 | loss_class: 0.7628 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|20:26:53] 	Iter 77900 Done. | loss1: 3.0765 | loss_class: 3.0758 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:27:44] 	Iter 78000 Done. | loss1: 0.7511 | loss_class: 0.7504 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:28:36] 	Iter 78100 Done. | loss1: 1.1163 | loss_class: 1.1156 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:29:26] 	Iter 78200 Done. | loss1: 0.4128 | loss_class: 0.4123 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|20:30:17] 	Iter 78300 Done. | loss1: 0.8833 | loss_class: 0.8825 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|20:31:07] 	Iter 78400 Done. | loss1: 1.2521 | loss_class: 1.2513 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|20:31:58] 	Iter 78500 Done. | loss1: 2.7335 | loss_class: 2.7329 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:32:48] 	Iter 78600 Done. | loss1: 1.6912 | loss_class: 1.6907 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|20:33:38] 	Iter 78700 Done. | loss1: 0.5615 | loss_class: 0.5610 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:34:28] 	Iter 78800 Done. | loss1: 1.9813 | loss_class: 1.9807 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:35:19] 	Iter 78900 Done. | loss1: 1.8486 | loss_class: 1.8477 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|20:36:09] 	Iter 79000 Done. | loss1: 0.6167 | loss_class: 0.6161 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:36:59] 	Iter 79100 Done. | loss1: 2.2813 | loss_class: 2.2809 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|20:37:50] 	Iter 79200 Done. | loss1: 1.3274 | loss_class: 1.3268 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:38:40] 	Iter 79300 Done. | loss1: 1.8797 | loss_class: 1.8790 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:39:30] 	Iter 79400 Done. | loss1: 0.6565 | loss_class: 0.6558 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:40:20] 	Iter 79500 Done. | loss1: 0.2933 | loss_class: 0.2926 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:41:10] 	Iter 79600 Done. | loss1: 1.5643 | loss_class: 1.5637 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:42:02] 	Iter 79700 Done. | loss1: 0.6810 | loss_class: 0.6804 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:42:57] 	Iter 79800 Done. | loss1: 1.9943 | loss_class: 1.9935 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|20:43:47] 	Iter 79900 Done. | loss1: 1.3714 | loss_class: 1.3710 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|20:44:38] 	Iter 80000 Done. | loss1: 0.5631 | loss_class: 0.5623 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|20:45:29] 	Iter 80100 Done. | loss1: 1.5680 | loss_class: 1.5675 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|20:46:07] 	mean_loss1: 1.4887745614539285
[06.22.21|20:46:07] 	mean_loss_class: 1.4881261784274902
[06.22.21|20:46:07] 	mean_loss_recon: 0.0006483828866803386
[06.22.21|20:46:07] Time consumption:
[06.22.21|20:46:07] Done.
[06.22.21|20:46:07] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch17_model1.pt.
[06.22.21|20:46:07] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch17_model2.pt.
[06.22.21|20:46:07] Training epoch: 18
[06.22.21|20:46:20] 	Iter 80200 Done. | loss1: 1.7882 | loss_class: 1.7874 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|20:47:11] 	Iter 80300 Done. | loss1: 1.1474 | loss_class: 1.1466 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|20:48:02] 	Iter 80400 Done. | loss1: 0.9236 | loss_class: 0.9231 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|20:48:53] 	Iter 80500 Done. | loss1: 2.8641 | loss_class: 2.8635 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:49:46] 	Iter 80600 Done. | loss1: 0.8613 | loss_class: 0.8607 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:50:37] 	Iter 80700 Done. | loss1: 1.9119 | loss_class: 1.9112 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:51:31] 	Iter 80800 Done. | loss1: 1.0888 | loss_class: 1.0881 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:52:22] 	Iter 80900 Done. | loss1: 1.1476 | loss_class: 1.1469 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:53:12] 	Iter 81000 Done. | loss1: 1.7302 | loss_class: 1.7296 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:54:02] 	Iter 81100 Done. | loss1: 1.4008 | loss_class: 1.4000 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|20:54:52] 	Iter 81200 Done. | loss1: 0.4933 | loss_class: 0.4929 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|20:55:46] 	Iter 81300 Done. | loss1: 1.8780 | loss_class: 1.8774 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:56:37] 	Iter 81400 Done. | loss1: 1.6131 | loss_class: 1.6124 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:57:28] 	Iter 81500 Done. | loss1: 2.3923 | loss_class: 2.3917 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|20:58:18] 	Iter 81600 Done. | loss1: 1.0093 | loss_class: 1.0087 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|20:59:10] 	Iter 81700 Done. | loss1: 1.5651 | loss_class: 1.5646 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|21:00:01] 	Iter 81800 Done. | loss1: 1.3962 | loss_class: 1.3956 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|21:00:51] 	Iter 81900 Done. | loss1: 0.0966 | loss_class: 0.0961 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|21:01:42] 	Iter 82000 Done. | loss1: 1.3628 | loss_class: 1.3621 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:02:37] 	Iter 82100 Done. | loss1: 0.9883 | loss_class: 0.9875 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|21:03:28] 	Iter 82200 Done. | loss1: 1.2471 | loss_class: 1.2465 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:04:22] 	Iter 82300 Done. | loss1: 1.0260 | loss_class: 1.0255 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:05:13] 	Iter 82400 Done. | loss1: 1.0702 | loss_class: 1.0694 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:06:06] 	Iter 82500 Done. | loss1: 2.9751 | loss_class: 2.9745 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:07:00] 	Iter 82600 Done. | loss1: 0.5369 | loss_class: 0.5362 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|21:07:54] 	Iter 82700 Done. | loss1: 2.6432 | loss_class: 2.6426 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:08:46] 	Iter 82800 Done. | loss1: 0.8189 | loss_class: 0.8184 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|21:09:38] 	Iter 82900 Done. | loss1: 0.4659 | loss_class: 0.4652 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:10:29] 	Iter 83000 Done. | loss1: 1.5549 | loss_class: 1.5544 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:11:21] 	Iter 83100 Done. | loss1: 2.2217 | loss_class: 2.2212 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|21:12:14] 	Iter 83200 Done. | loss1: 0.1889 | loss_class: 0.1883 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:13:05] 	Iter 83300 Done. | loss1: 1.1538 | loss_class: 1.1530 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|21:13:57] 	Iter 83400 Done. | loss1: 0.0110 | loss_class: 0.0102 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|21:14:51] 	Iter 83500 Done. | loss1: 0.8778 | loss_class: 0.8772 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:15:45] 	Iter 83600 Done. | loss1: 2.3706 | loss_class: 2.3698 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|21:16:40] 	Iter 83700 Done. | loss1: 2.0373 | loss_class: 2.0369 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|21:17:34] 	Iter 83800 Done. | loss1: 0.6284 | loss_class: 0.6277 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:18:26] 	Iter 83900 Done. | loss1: 1.9567 | loss_class: 1.9561 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:19:16] 	Iter 84000 Done. | loss1: 0.3337 | loss_class: 0.3329 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|21:20:09] 	Iter 84100 Done. | loss1: 2.9998 | loss_class: 2.9991 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:21:02] 	Iter 84200 Done. | loss1: 1.2086 | loss_class: 1.2077 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|21:21:54] 	Iter 84300 Done. | loss1: 0.5110 | loss_class: 0.5102 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|21:22:46] 	Iter 84400 Done. | loss1: 3.0788 | loss_class: 3.0781 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:23:37] 	Iter 84500 Done. | loss1: 0.4808 | loss_class: 0.4801 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:24:29] 	Iter 84600 Done. | loss1: 2.2395 | loss_class: 2.2388 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:25:20] 	Iter 84700 Done. | loss1: 1.5228 | loss_class: 1.5221 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:26:12] 	Iter 84800 Done. | loss1: 1.0121 | loss_class: 1.0115 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:27:04] 	Iter 84900 Done. | loss1: 1.8283 | loss_class: 1.8276 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:27:55] 	Iter 85000 Done. | loss1: 1.0334 | loss_class: 1.0327 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:28:48] 	Iter 85100 Done. | loss1: 1.2289 | loss_class: 1.2281 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:29:39] 	Iter 85200 Done. | loss1: 1.9632 | loss_class: 1.9626 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:30:31] 	Iter 85300 Done. | loss1: 0.6638 | loss_class: 0.6631 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:31:23] 	Iter 85400 Done. | loss1: 0.9257 | loss_class: 0.9251 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:32:14] 	Iter 85500 Done. | loss1: 1.5141 | loss_class: 1.5134 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:33:06] 	Iter 85600 Done. | loss1: 3.1380 | loss_class: 3.1373 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:33:58] 	Iter 85700 Done. | loss1: 0.5604 | loss_class: 0.5599 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|21:34:50] 	Iter 85800 Done. | loss1: 2.0142 | loss_class: 2.0139 | loss_recon: 0.0003 | lr: 0.100000
[06.22.21|21:35:41] 	Iter 85900 Done. | loss1: 1.2468 | loss_class: 1.2462 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:36:32] 	Iter 86000 Done. | loss1: 1.4666 | loss_class: 1.4659 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:37:24] 	Iter 86100 Done. | loss1: 0.5688 | loss_class: 0.5683 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|21:38:16] 	Iter 86200 Done. | loss1: 1.4442 | loss_class: 1.4435 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:39:07] 	Iter 86300 Done. | loss1: 2.4623 | loss_class: 2.4618 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|21:39:59] 	Iter 86400 Done. | loss1: 0.8995 | loss_class: 0.8989 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:40:50] 	Iter 86500 Done. | loss1: 2.3255 | loss_class: 2.3247 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|21:41:42] 	Iter 86600 Done. | loss1: 0.9059 | loss_class: 0.9051 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|21:42:34] 	Iter 86700 Done. | loss1: 1.1019 | loss_class: 1.1012 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:43:25] 	Iter 86800 Done. | loss1: 0.9921 | loss_class: 0.9915 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:44:17] 	Iter 86900 Done. | loss1: 0.6599 | loss_class: 0.6592 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:45:10] 	Iter 87000 Done. | loss1: 1.5975 | loss_class: 1.5968 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:46:02] 	Iter 87100 Done. | loss1: 0.3452 | loss_class: 0.3444 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|21:46:53] 	Iter 87200 Done. | loss1: 0.3187 | loss_class: 0.3182 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|21:47:45] 	Iter 87300 Done. | loss1: 0.6837 | loss_class: 0.6829 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|21:48:37] 	Iter 87400 Done. | loss1: 1.0629 | loss_class: 1.0624 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|21:49:28] 	Iter 87500 Done. | loss1: 5.2929 | loss_class: 5.2925 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|21:50:21] 	Iter 87600 Done. | loss1: 0.6015 | loss_class: 0.6008 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:51:13] 	Iter 87700 Done. | loss1: 1.0464 | loss_class: 1.0456 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|21:52:04] 	Iter 87800 Done. | loss1: 1.4653 | loss_class: 1.4647 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:52:56] 	Iter 87900 Done. | loss1: 1.9440 | loss_class: 1.9435 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:53:47] 	Iter 88000 Done. | loss1: 1.0048 | loss_class: 1.0043 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|21:54:39] 	Iter 88100 Done. | loss1: 1.5874 | loss_class: 1.5868 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:55:30] 	Iter 88200 Done. | loss1: 1.5346 | loss_class: 1.5340 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:56:22] 	Iter 88300 Done. | loss1: 1.6512 | loss_class: 1.6506 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:57:14] 	Iter 88400 Done. | loss1: 0.6470 | loss_class: 0.6463 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|21:58:06] 	Iter 88500 Done. | loss1: 0.8522 | loss_class: 0.8517 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|21:58:57] 	Iter 88600 Done. | loss1: 0.8584 | loss_class: 0.8578 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|21:59:48] 	Iter 88700 Done. | loss1: 0.9311 | loss_class: 0.9304 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:00:40] 	Iter 88800 Done. | loss1: 1.5757 | loss_class: 1.5750 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:01:32] 	Iter 88900 Done. | loss1: 1.4284 | loss_class: 1.4279 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:02:24] 	Iter 89000 Done. | loss1: 1.3248 | loss_class: 1.3243 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:03:15] 	Iter 89100 Done. | loss1: 1.8204 | loss_class: 1.8199 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:04:07] 	Iter 89200 Done. | loss1: 0.5836 | loss_class: 0.5830 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:04:58] 	Iter 89300 Done. | loss1: 0.7260 | loss_class: 0.7256 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|22:05:50] 	Iter 89400 Done. | loss1: 1.7175 | loss_class: 1.7168 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:06:42] 	Iter 89500 Done. | loss1: 2.3133 | loss_class: 2.3126 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:07:34] 	Iter 89600 Done. | loss1: 0.5982 | loss_class: 0.5973 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:08:26] 	Iter 89700 Done. | loss1: 3.4286 | loss_class: 3.4281 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:09:18] 	Iter 89800 Done. | loss1: 1.4846 | loss_class: 1.4839 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:10:10] 	Iter 89900 Done. | loss1: 2.0412 | loss_class: 2.0406 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:11:01] 	Iter 90000 Done. | loss1: 0.9348 | loss_class: 0.9342 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:11:53] 	Iter 90100 Done. | loss1: 1.7917 | loss_class: 1.7911 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:12:43] 	mean_loss1: 1.4302045970706696
[06.22.21|22:12:43] 	mean_loss_class: 1.4295557660932203
[06.22.21|22:12:43] 	mean_loss_recon: 0.000648830908981998
[06.22.21|22:12:43] Time consumption:
[06.22.21|22:12:43] Done.
[06.22.21|22:12:44] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch18_model1.pt.
[06.22.21|22:12:44] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch18_model2.pt.
[06.22.21|22:12:44] Training epoch: 19
[06.22.21|22:12:46] 	Iter 90200 Done. | loss1: 2.0231 | loss_class: 2.0223 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:13:38] 	Iter 90300 Done. | loss1: 2.8261 | loss_class: 2.8254 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:14:30] 	Iter 90400 Done. | loss1: 0.7149 | loss_class: 0.7143 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:15:22] 	Iter 90500 Done. | loss1: 3.6476 | loss_class: 3.6471 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:16:13] 	Iter 90600 Done. | loss1: 2.3792 | loss_class: 2.3787 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:17:05] 	Iter 90700 Done. | loss1: 1.0178 | loss_class: 1.0172 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:17:56] 	Iter 90800 Done. | loss1: 1.0670 | loss_class: 1.0664 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:18:48] 	Iter 90900 Done. | loss1: 1.3119 | loss_class: 1.3111 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:19:40] 	Iter 91000 Done. | loss1: 1.0122 | loss_class: 1.0116 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:20:32] 	Iter 91100 Done. | loss1: 2.2416 | loss_class: 2.2410 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:21:23] 	Iter 91200 Done. | loss1: 0.4659 | loss_class: 0.4651 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:22:16] 	Iter 91300 Done. | loss1: 0.4791 | loss_class: 0.4785 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:23:08] 	Iter 91400 Done. | loss1: 2.0099 | loss_class: 2.0094 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|22:23:59] 	Iter 91500 Done. | loss1: 0.6265 | loss_class: 0.6261 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:24:51] 	Iter 91600 Done. | loss1: 0.7271 | loss_class: 0.7263 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:25:42] 	Iter 91700 Done. | loss1: 1.3606 | loss_class: 1.3601 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:26:34] 	Iter 91800 Done. | loss1: 0.4267 | loss_class: 0.4261 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:27:26] 	Iter 91900 Done. | loss1: 2.0001 | loss_class: 1.9993 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:28:18] 	Iter 92000 Done. | loss1: 1.0234 | loss_class: 1.0229 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:29:09] 	Iter 92100 Done. | loss1: 0.8468 | loss_class: 0.8459 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|22:30:01] 	Iter 92200 Done. | loss1: 1.4019 | loss_class: 1.4013 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:30:52] 	Iter 92300 Done. | loss1: 1.1671 | loss_class: 1.1663 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:31:44] 	Iter 92400 Done. | loss1: 1.1614 | loss_class: 1.1609 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:32:36] 	Iter 92500 Done. | loss1: 1.9846 | loss_class: 1.9838 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:33:28] 	Iter 92600 Done. | loss1: 1.7746 | loss_class: 1.7740 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:34:20] 	Iter 92700 Done. | loss1: 1.0998 | loss_class: 1.0992 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:35:11] 	Iter 92800 Done. | loss1: 3.1828 | loss_class: 3.1820 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:36:03] 	Iter 92900 Done. | loss1: 1.1144 | loss_class: 1.1136 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:36:55] 	Iter 93000 Done. | loss1: 0.9402 | loss_class: 0.9395 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:37:47] 	Iter 93100 Done. | loss1: 3.1155 | loss_class: 3.1147 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:38:39] 	Iter 93200 Done. | loss1: 0.7011 | loss_class: 0.7004 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:39:30] 	Iter 93300 Done. | loss1: 1.0533 | loss_class: 1.0526 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:40:22] 	Iter 93400 Done. | loss1: 2.0170 | loss_class: 2.0163 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:41:13] 	Iter 93500 Done. | loss1: 1.1954 | loss_class: 1.1948 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:42:05] 	Iter 93600 Done. | loss1: 1.6369 | loss_class: 1.6364 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:42:56] 	Iter 93700 Done. | loss1: 0.3925 | loss_class: 0.3920 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:43:47] 	Iter 93800 Done. | loss1: 1.1421 | loss_class: 1.1416 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:44:41] 	Iter 93900 Done. | loss1: 0.2386 | loss_class: 0.2381 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|22:45:32] 	Iter 94000 Done. | loss1: 0.8065 | loss_class: 0.8057 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:46:24] 	Iter 94100 Done. | loss1: 0.9928 | loss_class: 0.9922 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:47:16] 	Iter 94200 Done. | loss1: 1.4852 | loss_class: 1.4845 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:48:07] 	Iter 94300 Done. | loss1: 1.1159 | loss_class: 1.1153 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:48:59] 	Iter 94400 Done. | loss1: 3.2348 | loss_class: 3.2340 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:49:50] 	Iter 94500 Done. | loss1: 1.5538 | loss_class: 1.5532 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:50:41] 	Iter 94600 Done. | loss1: 1.2903 | loss_class: 1.2897 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:51:33] 	Iter 94700 Done. | loss1: 1.3730 | loss_class: 1.3722 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|22:52:24] 	Iter 94800 Done. | loss1: 1.6708 | loss_class: 1.6701 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:53:16] 	Iter 94900 Done. | loss1: 1.1930 | loss_class: 1.1922 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|22:54:08] 	Iter 95000 Done. | loss1: 1.9528 | loss_class: 1.9517 | loss_recon: 0.0012 | lr: 0.100000
[06.22.21|22:54:58] 	Iter 95100 Done. | loss1: 1.2731 | loss_class: 1.2724 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:55:49] 	Iter 95200 Done. | loss1: 2.1279 | loss_class: 2.1272 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|22:56:39] 	Iter 95300 Done. | loss1: 0.8196 | loss_class: 0.8190 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:57:31] 	Iter 95400 Done. | loss1: 1.6176 | loss_class: 1.6173 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|22:58:22] 	Iter 95500 Done. | loss1: 1.4679 | loss_class: 1.4673 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|22:59:12] 	Iter 95600 Done. | loss1: 1.7477 | loss_class: 1.7470 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:00:03] 	Iter 95700 Done. | loss1: 0.7320 | loss_class: 0.7313 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:00:53] 	Iter 95800 Done. | loss1: 0.6079 | loss_class: 0.6072 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:01:44] 	Iter 95900 Done. | loss1: 0.4009 | loss_class: 0.4002 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:02:35] 	Iter 96000 Done. | loss1: 1.2641 | loss_class: 1.2633 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:03:26] 	Iter 96100 Done. | loss1: 1.4978 | loss_class: 1.4971 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:04:17] 	Iter 96200 Done. | loss1: 1.1346 | loss_class: 1.1341 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|23:05:08] 	Iter 96300 Done. | loss1: 2.6946 | loss_class: 2.6941 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|23:05:58] 	Iter 96400 Done. | loss1: 0.4906 | loss_class: 0.4900 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:06:49] 	Iter 96500 Done. | loss1: 0.4470 | loss_class: 0.4465 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|23:07:40] 	Iter 96600 Done. | loss1: 1.9042 | loss_class: 1.9035 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:08:30] 	Iter 96700 Done. | loss1: 0.4962 | loss_class: 0.4955 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:09:21] 	Iter 96800 Done. | loss1: 1.9244 | loss_class: 1.9236 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|23:10:11] 	Iter 96900 Done. | loss1: 1.6577 | loss_class: 1.6571 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:11:02] 	Iter 97000 Done. | loss1: 0.6442 | loss_class: 0.6434 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|23:11:52] 	Iter 97100 Done. | loss1: 0.7287 | loss_class: 0.7281 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:12:42] 	Iter 97200 Done. | loss1: 1.9261 | loss_class: 1.9255 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:13:33] 	Iter 97300 Done. | loss1: 0.8714 | loss_class: 0.8706 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:14:25] 	Iter 97400 Done. | loss1: 1.9301 | loss_class: 1.9296 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|23:15:16] 	Iter 97500 Done. | loss1: 2.7627 | loss_class: 2.7621 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:16:06] 	Iter 97600 Done. | loss1: 0.9859 | loss_class: 0.9852 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:16:57] 	Iter 97700 Done. | loss1: 1.3327 | loss_class: 1.3322 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|23:17:48] 	Iter 97800 Done. | loss1: 0.6926 | loss_class: 0.6920 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:18:38] 	Iter 97900 Done. | loss1: 0.6571 | loss_class: 0.6565 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|23:19:28] 	Iter 98000 Done. | loss1: 0.7162 | loss_class: 0.7158 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|23:20:19] 	Iter 98100 Done. | loss1: 0.3745 | loss_class: 0.3736 | loss_recon: 0.0009 | lr: 0.100000
[06.22.21|23:21:09] 	Iter 98200 Done. | loss1: 2.5287 | loss_class: 2.5283 | loss_recon: 0.0004 | lr: 0.100000
[06.22.21|23:21:59] 	Iter 98300 Done. | loss1: 0.7106 | loss_class: 0.7100 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:22:49] 	Iter 98400 Done. | loss1: 1.1585 | loss_class: 1.1577 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|23:23:41] 	Iter 98500 Done. | loss1: 0.2990 | loss_class: 0.2982 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|23:24:31] 	Iter 98600 Done. | loss1: 0.8249 | loss_class: 0.8243 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:25:22] 	Iter 98700 Done. | loss1: 3.2555 | loss_class: 3.2548 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:26:12] 	Iter 98800 Done. | loss1: 0.7636 | loss_class: 0.7628 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:27:02] 	Iter 98900 Done. | loss1: 1.1425 | loss_class: 1.1416 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|23:27:53] 	Iter 99000 Done. | loss1: 3.2230 | loss_class: 3.2224 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:28:44] 	Iter 99100 Done. | loss1: 0.5348 | loss_class: 0.5342 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:29:34] 	Iter 99200 Done. | loss1: 0.6341 | loss_class: 0.6335 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|23:30:24] 	Iter 99300 Done. | loss1: 1.8743 | loss_class: 1.8738 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|23:31:15] 	Iter 99400 Done. | loss1: 1.9163 | loss_class: 1.9157 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|23:32:05] 	Iter 99500 Done. | loss1: 0.1568 | loss_class: 0.1561 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|23:32:56] 	Iter 99600 Done. | loss1: 0.2603 | loss_class: 0.2597 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:33:46] 	Iter 99700 Done. | loss1: 1.2168 | loss_class: 1.2161 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:34:36] 	Iter 99800 Done. | loss1: 0.2785 | loss_class: 0.2780 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|23:35:26] 	Iter 99900 Done. | loss1: 2.9814 | loss_class: 2.9808 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:36:17] 	Iter 100000 Done. | loss1: 1.2064 | loss_class: 1.2056 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:37:08] 	Iter 100100 Done. | loss1: 0.2709 | loss_class: 0.2701 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|23:37:58] 	Iter 100200 Done. | loss1: 0.9682 | loss_class: 0.9675 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:38:08] 	mean_loss1: 1.3811823049062746
[06.22.21|23:38:08] 	mean_loss_class: 1.380533992317681
[06.22.21|23:38:08] 	mean_loss_recon: 0.0006483129873861396
[06.22.21|23:38:08] Time consumption:
[06.22.21|23:38:08] Done.
[06.22.21|23:38:08] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch19_model1.pt.
[06.22.21|23:38:09] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch19_model2.pt.
[06.22.21|23:38:09] Eval epoch: 19
[06.22.21|23:44:43] 	mean_loss1: 1.456104460728261
[06.22.21|23:44:43] 	mean_loss_class: 1.4557348401971566
[06.22.21|23:44:43] 	mean_loss_recon: 0.03696227966006412
[06.22.21|23:44:43] 

[06.22.21|23:44:43] 	Top1: 56.82%
[06.22.21|23:44:43] 

[06.22.21|23:44:43] 	Top5: 88.85%
[06.22.21|23:44:43] Done.
[06.22.21|23:44:43] Training epoch: 20
[06.22.21|23:45:24] 	Iter 100300 Done. | loss1: 1.0948 | loss_class: 1.0943 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|23:46:15] 	Iter 100400 Done. | loss1: 2.4558 | loss_class: 2.4551 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:47:08] 	Iter 100500 Done. | loss1: 0.9567 | loss_class: 0.9560 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:48:01] 	Iter 100600 Done. | loss1: 1.8692 | loss_class: 1.8683 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|23:48:51] 	Iter 100700 Done. | loss1: 0.7888 | loss_class: 0.7881 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:49:43] 	Iter 100800 Done. | loss1: 1.4559 | loss_class: 1.4553 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:50:34] 	Iter 100900 Done. | loss1: 1.6592 | loss_class: 1.6584 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|23:51:25] 	Iter 101000 Done. | loss1: 1.3435 | loss_class: 1.3428 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:52:18] 	Iter 101100 Done. | loss1: 0.7246 | loss_class: 0.7236 | loss_recon: 0.0010 | lr: 0.100000
[06.22.21|23:53:11] 	Iter 101200 Done. | loss1: 1.8683 | loss_class: 1.8675 | loss_recon: 0.0008 | lr: 0.100000
[06.22.21|23:54:03] 	Iter 101300 Done. | loss1: 1.3695 | loss_class: 1.3689 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:54:55] 	Iter 101400 Done. | loss1: 1.3171 | loss_class: 1.3165 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:55:46] 	Iter 101500 Done. | loss1: 1.2267 | loss_class: 1.2260 | loss_recon: 0.0007 | lr: 0.100000
[06.22.21|23:56:39] 	Iter 101600 Done. | loss1: 1.2898 | loss_class: 1.2892 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:57:30] 	Iter 101700 Done. | loss1: 1.0531 | loss_class: 1.0525 | loss_recon: 0.0006 | lr: 0.100000
[06.22.21|23:58:21] 	Iter 101800 Done. | loss1: 0.4060 | loss_class: 0.4055 | loss_recon: 0.0005 | lr: 0.100000
[06.22.21|23:59:12] 	Iter 101900 Done. | loss1: 2.0809 | loss_class: 2.0803 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:00:03] 	Iter 102000 Done. | loss1: 3.0809 | loss_class: 3.0801 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:00:54] 	Iter 102100 Done. | loss1: 1.0892 | loss_class: 1.0885 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:01:44] 	Iter 102200 Done. | loss1: 1.0149 | loss_class: 1.0144 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|00:02:36] 	Iter 102300 Done. | loss1: 2.0984 | loss_class: 2.0977 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:03:28] 	Iter 102400 Done. | loss1: 1.3478 | loss_class: 1.3472 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:04:21] 	Iter 102500 Done. | loss1: 1.2898 | loss_class: 1.2892 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:05:13] 	Iter 102600 Done. | loss1: 1.1493 | loss_class: 1.1487 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:06:04] 	Iter 102700 Done. | loss1: 0.9175 | loss_class: 0.9168 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:06:57] 	Iter 102800 Done. | loss1: 1.4438 | loss_class: 1.4433 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|00:07:48] 	Iter 102900 Done. | loss1: 0.2414 | loss_class: 0.2409 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|00:08:39] 	Iter 103000 Done. | loss1: 0.5004 | loss_class: 0.4997 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:09:30] 	Iter 103100 Done. | loss1: 1.4985 | loss_class: 1.4980 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|00:10:22] 	Iter 103200 Done. | loss1: 2.3826 | loss_class: 2.3820 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:11:14] 	Iter 103300 Done. | loss1: 0.5675 | loss_class: 0.5667 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:12:07] 	Iter 103400 Done. | loss1: 1.8389 | loss_class: 1.8382 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:12:59] 	Iter 103500 Done. | loss1: 2.7994 | loss_class: 2.7987 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:13:51] 	Iter 103600 Done. | loss1: 0.6381 | loss_class: 0.6375 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:14:42] 	Iter 103700 Done. | loss1: 0.7256 | loss_class: 0.7247 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|00:15:34] 	Iter 103800 Done. | loss1: 2.3239 | loss_class: 2.3231 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:16:26] 	Iter 103900 Done. | loss1: 0.2662 | loss_class: 0.2655 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:17:17] 	Iter 104000 Done. | loss1: 2.2743 | loss_class: 2.2735 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:18:09] 	Iter 104100 Done. | loss1: 2.0327 | loss_class: 2.0321 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:19:00] 	Iter 104200 Done. | loss1: 1.0922 | loss_class: 1.0915 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:19:52] 	Iter 104300 Done. | loss1: 1.0969 | loss_class: 1.0962 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:20:44] 	Iter 104400 Done. | loss1: 1.1278 | loss_class: 1.1271 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:21:35] 	Iter 104500 Done. | loss1: 1.6587 | loss_class: 1.6578 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|00:22:26] 	Iter 104600 Done. | loss1: 1.7566 | loss_class: 1.7558 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:23:17] 	Iter 104700 Done. | loss1: 2.6185 | loss_class: 2.6179 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:24:09] 	Iter 104800 Done. | loss1: 0.4115 | loss_class: 0.4107 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:25:01] 	Iter 104900 Done. | loss1: 1.4392 | loss_class: 1.4385 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:25:53] 	Iter 105000 Done. | loss1: 1.9057 | loss_class: 1.9051 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:26:45] 	Iter 105100 Done. | loss1: 1.2833 | loss_class: 1.2826 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:27:37] 	Iter 105200 Done. | loss1: 0.3782 | loss_class: 0.3774 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:28:29] 	Iter 105300 Done. | loss1: 0.8668 | loss_class: 0.8663 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:29:21] 	Iter 105400 Done. | loss1: 0.5979 | loss_class: 0.5973 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:30:13] 	Iter 105500 Done. | loss1: 1.9986 | loss_class: 1.9979 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:31:05] 	Iter 105600 Done. | loss1: 1.1641 | loss_class: 1.1635 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:31:57] 	Iter 105700 Done. | loss1: 1.0605 | loss_class: 1.0601 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|00:32:48] 	Iter 105800 Done. | loss1: 1.0066 | loss_class: 1.0060 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:33:39] 	Iter 105900 Done. | loss1: 2.3087 | loss_class: 2.3078 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|00:34:31] 	Iter 106000 Done. | loss1: 0.7008 | loss_class: 0.7002 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:35:23] 	Iter 106100 Done. | loss1: 1.4761 | loss_class: 1.4753 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:36:15] 	Iter 106200 Done. | loss1: 2.7872 | loss_class: 2.7867 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|00:37:06] 	Iter 106300 Done. | loss1: 1.9774 | loss_class: 1.9767 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:37:58] 	Iter 106400 Done. | loss1: 0.3461 | loss_class: 0.3455 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:38:51] 	Iter 106500 Done. | loss1: 1.0558 | loss_class: 1.0551 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:39:43] 	Iter 106600 Done. | loss1: 2.4362 | loss_class: 2.4356 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|00:40:35] 	Iter 106700 Done. | loss1: 0.4188 | loss_class: 0.4179 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:41:27] 	Iter 106800 Done. | loss1: 0.9992 | loss_class: 0.9985 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:42:19] 	Iter 106900 Done. | loss1: 0.2752 | loss_class: 0.2744 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:43:11] 	Iter 107000 Done. | loss1: 1.3491 | loss_class: 1.3486 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:44:03] 	Iter 107100 Done. | loss1: 1.0100 | loss_class: 1.0092 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:44:55] 	Iter 107200 Done. | loss1: 1.8998 | loss_class: 1.8991 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:45:46] 	Iter 107300 Done. | loss1: 0.7781 | loss_class: 0.7775 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:46:38] 	Iter 107400 Done. | loss1: 1.1127 | loss_class: 1.1122 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|00:47:30] 	Iter 107500 Done. | loss1: 1.1047 | loss_class: 1.1041 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:48:22] 	Iter 107600 Done. | loss1: 3.2673 | loss_class: 3.2667 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:49:14] 	Iter 107700 Done. | loss1: 0.2463 | loss_class: 0.2456 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:50:05] 	Iter 107800 Done. | loss1: 1.1996 | loss_class: 1.1990 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:50:57] 	Iter 107900 Done. | loss1: 2.3968 | loss_class: 2.3960 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:51:49] 	Iter 108000 Done. | loss1: 0.2899 | loss_class: 0.2892 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:52:41] 	Iter 108100 Done. | loss1: 0.2821 | loss_class: 0.2814 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|00:53:32] 	Iter 108200 Done. | loss1: 2.1712 | loss_class: 2.1706 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:54:24] 	Iter 108300 Done. | loss1: 1.3368 | loss_class: 1.3361 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:55:16] 	Iter 108400 Done. | loss1: 0.4220 | loss_class: 0.4214 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:56:08] 	Iter 108500 Done. | loss1: 1.1192 | loss_class: 1.1186 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:56:59] 	Iter 108600 Done. | loss1: 0.6277 | loss_class: 0.6269 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|00:57:52] 	Iter 108700 Done. | loss1: 0.4947 | loss_class: 0.4942 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:58:43] 	Iter 108800 Done. | loss1: 0.5981 | loss_class: 0.5974 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|00:59:35] 	Iter 108900 Done. | loss1: 1.6986 | loss_class: 1.6980 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:00:26] 	Iter 109000 Done. | loss1: 1.3997 | loss_class: 1.3990 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:01:18] 	Iter 109100 Done. | loss1: 1.5974 | loss_class: 1.5967 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:02:10] 	Iter 109200 Done. | loss1: 0.6266 | loss_class: 0.6260 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:03:01] 	Iter 109300 Done. | loss1: 1.4911 | loss_class: 1.4904 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:03:53] 	Iter 109400 Done. | loss1: 2.1997 | loss_class: 2.1992 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|01:04:44] 	Iter 109500 Done. | loss1: 1.6607 | loss_class: 1.6598 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|01:05:36] 	Iter 109600 Done. | loss1: 2.0364 | loss_class: 2.0356 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|01:06:28] 	Iter 109700 Done. | loss1: 1.3341 | loss_class: 1.3333 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|01:07:20] 	Iter 109800 Done. | loss1: 0.9087 | loss_class: 0.9080 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:08:11] 	Iter 109900 Done. | loss1: 1.3760 | loss_class: 1.3755 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:09:03] 	Iter 110000 Done. | loss1: 2.2156 | loss_class: 2.2147 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|01:09:55] 	Iter 110100 Done. | loss1: 0.3863 | loss_class: 0.3856 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:10:46] 	Iter 110200 Done. | loss1: 0.4880 | loss_class: 0.4873 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:11:08] 	mean_loss1: 1.3420063450980269
[06.23.21|01:11:08] 	mean_loss_class: 1.341356821256448
[06.23.21|01:11:08] 	mean_loss_recon: 0.0006495244425132862
[06.23.21|01:11:08] Time consumption:
[06.23.21|01:11:08] Done.
[06.23.21|01:11:08] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch20_model1.pt.
[06.23.21|01:11:08] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch20_model2.pt.
[06.23.21|01:11:08] Training epoch: 21
[06.23.21|01:11:40] 	Iter 110300 Done. | loss1: 0.3262 | loss_class: 0.3256 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:12:31] 	Iter 110400 Done. | loss1: 0.5578 | loss_class: 0.5569 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|01:13:23] 	Iter 110500 Done. | loss1: 0.9619 | loss_class: 0.9612 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:14:14] 	Iter 110600 Done. | loss1: 0.5375 | loss_class: 0.5368 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:15:06] 	Iter 110700 Done. | loss1: 1.1733 | loss_class: 1.1726 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:15:56] 	Iter 110800 Done. | loss1: 1.1126 | loss_class: 1.1117 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|01:16:47] 	Iter 110900 Done. | loss1: 0.5689 | loss_class: 0.5683 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:17:38] 	Iter 111000 Done. | loss1: 1.0305 | loss_class: 1.0300 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:18:29] 	Iter 111100 Done. | loss1: 2.2895 | loss_class: 2.2885 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|01:19:20] 	Iter 111200 Done. | loss1: 2.6988 | loss_class: 2.6980 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|01:20:11] 	Iter 111300 Done. | loss1: 1.8301 | loss_class: 1.8292 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|01:21:02] 	Iter 111400 Done. | loss1: 0.3939 | loss_class: 0.3932 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:21:53] 	Iter 111500 Done. | loss1: 0.9844 | loss_class: 0.9838 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:22:43] 	Iter 111600 Done. | loss1: 2.0295 | loss_class: 2.0289 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:23:34] 	Iter 111700 Done. | loss1: 2.0009 | loss_class: 2.0003 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:24:26] 	Iter 111800 Done. | loss1: 2.1165 | loss_class: 2.1158 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:25:16] 	Iter 111900 Done. | loss1: 1.5393 | loss_class: 1.5389 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|01:26:07] 	Iter 112000 Done. | loss1: 1.1855 | loss_class: 1.1847 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|01:26:58] 	Iter 112100 Done. | loss1: 0.5034 | loss_class: 0.5028 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:27:49] 	Iter 112200 Done. | loss1: 0.9641 | loss_class: 0.9636 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|01:28:40] 	Iter 112300 Done. | loss1: 0.9968 | loss_class: 0.9960 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|01:29:31] 	Iter 112400 Done. | loss1: 1.4614 | loss_class: 1.4609 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|01:30:22] 	Iter 112500 Done. | loss1: 2.1410 | loss_class: 2.1404 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:31:13] 	Iter 112600 Done. | loss1: 0.8999 | loss_class: 0.8993 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:32:04] 	Iter 112700 Done. | loss1: 1.2774 | loss_class: 1.2768 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:32:56] 	Iter 112800 Done. | loss1: 1.0571 | loss_class: 1.0564 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:33:47] 	Iter 112900 Done. | loss1: 1.8540 | loss_class: 1.8533 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:34:38] 	Iter 113000 Done. | loss1: 0.6320 | loss_class: 0.6315 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|01:35:29] 	Iter 113100 Done. | loss1: 0.9403 | loss_class: 0.9396 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:36:20] 	Iter 113200 Done. | loss1: 0.9165 | loss_class: 0.9160 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|01:37:12] 	Iter 113300 Done. | loss1: 1.4423 | loss_class: 1.4415 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|01:38:03] 	Iter 113400 Done. | loss1: 0.9871 | loss_class: 0.9865 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:38:54] 	Iter 113500 Done. | loss1: 2.2328 | loss_class: 2.2320 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|01:39:45] 	Iter 113600 Done. | loss1: 1.0136 | loss_class: 1.0130 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:40:35] 	Iter 113700 Done. | loss1: 2.7614 | loss_class: 2.7608 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:41:26] 	Iter 113800 Done. | loss1: 1.4947 | loss_class: 1.4941 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:42:16] 	Iter 113900 Done. | loss1: 0.4281 | loss_class: 0.4276 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|01:43:08] 	Iter 114000 Done. | loss1: 0.8107 | loss_class: 0.8102 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|01:43:59] 	Iter 114100 Done. | loss1: 1.7090 | loss_class: 1.7081 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|01:44:51] 	Iter 114200 Done. | loss1: 1.6000 | loss_class: 1.5991 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|01:45:42] 	Iter 114300 Done. | loss1: 0.7840 | loss_class: 0.7835 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|01:46:33] 	Iter 114400 Done. | loss1: 1.6577 | loss_class: 1.6571 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:47:24] 	Iter 114500 Done. | loss1: 0.7995 | loss_class: 0.7988 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:48:15] 	Iter 114600 Done. | loss1: 1.3641 | loss_class: 1.3636 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|01:49:07] 	Iter 114700 Done. | loss1: 0.5292 | loss_class: 0.5287 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|01:49:58] 	Iter 114800 Done. | loss1: 0.7916 | loss_class: 0.7911 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|01:50:49] 	Iter 114900 Done. | loss1: 0.1547 | loss_class: 0.1540 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:51:40] 	Iter 115000 Done. | loss1: 0.7230 | loss_class: 0.7223 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:52:32] 	Iter 115100 Done. | loss1: 3.7555 | loss_class: 3.7550 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|01:53:23] 	Iter 115200 Done. | loss1: 0.8597 | loss_class: 0.8592 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|01:54:14] 	Iter 115300 Done. | loss1: 2.3182 | loss_class: 2.3170 | loss_recon: 0.0012 | lr: 0.100000
[06.23.21|01:55:05] 	Iter 115400 Done. | loss1: 0.8334 | loss_class: 0.8325 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|01:55:57] 	Iter 115500 Done. | loss1: 1.7269 | loss_class: 1.7262 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:56:48] 	Iter 115600 Done. | loss1: 1.4313 | loss_class: 1.4306 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:57:39] 	Iter 115700 Done. | loss1: 1.0943 | loss_class: 1.0937 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|01:58:31] 	Iter 115800 Done. | loss1: 1.7363 | loss_class: 1.7356 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|01:59:22] 	Iter 115900 Done. | loss1: 0.0832 | loss_class: 0.0825 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:00:14] 	Iter 116000 Done. | loss1: 1.1244 | loss_class: 1.1238 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:01:05] 	Iter 116100 Done. | loss1: 1.1665 | loss_class: 1.1659 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:01:56] 	Iter 116200 Done. | loss1: 0.8799 | loss_class: 0.8793 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:02:48] 	Iter 116300 Done. | loss1: 1.4913 | loss_class: 1.4903 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|02:03:39] 	Iter 116400 Done. | loss1: 1.5882 | loss_class: 1.5875 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:04:31] 	Iter 116500 Done. | loss1: 2.3906 | loss_class: 2.3900 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:05:23] 	Iter 116600 Done. | loss1: 1.5342 | loss_class: 1.5335 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:06:14] 	Iter 116700 Done. | loss1: 0.3189 | loss_class: 0.3181 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|02:07:06] 	Iter 116800 Done. | loss1: 1.4101 | loss_class: 1.4093 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:07:56] 	Iter 116900 Done. | loss1: 2.5964 | loss_class: 2.5958 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:08:48] 	Iter 117000 Done. | loss1: 0.8201 | loss_class: 0.8194 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:09:39] 	Iter 117100 Done. | loss1: 1.6380 | loss_class: 1.6374 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:10:31] 	Iter 117200 Done. | loss1: 1.1604 | loss_class: 1.1595 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|02:11:22] 	Iter 117300 Done. | loss1: 1.4008 | loss_class: 1.4004 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:12:13] 	Iter 117400 Done. | loss1: 1.3980 | loss_class: 1.3973 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:13:05] 	Iter 117500 Done. | loss1: 1.6129 | loss_class: 1.6124 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:13:56] 	Iter 117600 Done. | loss1: 1.1832 | loss_class: 1.1827 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:14:48] 	Iter 117700 Done. | loss1: 1.1562 | loss_class: 1.1556 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:15:39] 	Iter 117800 Done. | loss1: 1.7493 | loss_class: 1.7488 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:16:30] 	Iter 117900 Done. | loss1: 0.7745 | loss_class: 0.7740 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:17:22] 	Iter 118000 Done. | loss1: 1.0691 | loss_class: 1.0688 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|02:18:13] 	Iter 118100 Done. | loss1: 1.4037 | loss_class: 1.4031 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:19:04] 	Iter 118200 Done. | loss1: 4.0587 | loss_class: 4.0582 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:19:56] 	Iter 118300 Done. | loss1: 0.1177 | loss_class: 0.1170 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:20:48] 	Iter 118400 Done. | loss1: 1.9545 | loss_class: 1.9540 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:21:39] 	Iter 118500 Done. | loss1: 0.5893 | loss_class: 0.5885 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|02:22:30] 	Iter 118600 Done. | loss1: 0.8256 | loss_class: 0.8249 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:23:22] 	Iter 118700 Done. | loss1: 1.1092 | loss_class: 1.1086 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:24:13] 	Iter 118800 Done. | loss1: 2.3922 | loss_class: 2.3916 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:25:04] 	Iter 118900 Done. | loss1: 0.6622 | loss_class: 0.6616 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:25:56] 	Iter 119000 Done. | loss1: 0.9107 | loss_class: 0.9100 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:26:47] 	Iter 119100 Done. | loss1: 0.4214 | loss_class: 0.4208 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:27:38] 	Iter 119200 Done. | loss1: 0.7383 | loss_class: 0.7375 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|02:28:29] 	Iter 119300 Done. | loss1: 3.5835 | loss_class: 3.5829 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:29:20] 	Iter 119400 Done. | loss1: 0.2387 | loss_class: 0.2382 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:30:11] 	Iter 119500 Done. | loss1: 1.9157 | loss_class: 1.9152 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:31:03] 	Iter 119600 Done. | loss1: 0.8316 | loss_class: 0.8309 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:31:54] 	Iter 119700 Done. | loss1: 1.2760 | loss_class: 1.2753 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:32:45] 	Iter 119800 Done. | loss1: 0.9469 | loss_class: 0.9462 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:33:36] 	Iter 119900 Done. | loss1: 1.0858 | loss_class: 1.0850 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|02:34:27] 	Iter 120000 Done. | loss1: 1.8120 | loss_class: 1.8112 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|02:35:19] 	Iter 120100 Done. | loss1: 1.4510 | loss_class: 1.4502 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|02:36:11] 	Iter 120200 Done. | loss1: 0.5792 | loss_class: 0.5787 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:36:43] 	mean_loss1: 1.3177659982572658
[06.23.21|02:36:43] 	mean_loss_class: 1.3171162447224722
[06.23.21|02:36:43] 	mean_loss_recon: 0.0006497535478276118
[06.23.21|02:36:43] Time consumption:
[06.23.21|02:36:43] Done.
[06.23.21|02:36:43] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch21_model1.pt.
[06.23.21|02:36:43] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch21_model2.pt.
[06.23.21|02:36:43] Training epoch: 22
[06.23.21|02:37:03] 	Iter 120300 Done. | loss1: 1.0859 | loss_class: 1.0852 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:37:54] 	Iter 120400 Done. | loss1: 0.7556 | loss_class: 0.7550 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:38:46] 	Iter 120500 Done. | loss1: 1.3644 | loss_class: 1.3637 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:39:38] 	Iter 120600 Done. | loss1: 2.8111 | loss_class: 2.8105 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:40:29] 	Iter 120700 Done. | loss1: 0.7772 | loss_class: 0.7766 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:41:21] 	Iter 120800 Done. | loss1: 0.8962 | loss_class: 0.8957 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:42:12] 	Iter 120900 Done. | loss1: 1.2563 | loss_class: 1.2558 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:43:04] 	Iter 121000 Done. | loss1: 2.3854 | loss_class: 2.3843 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|02:43:56] 	Iter 121100 Done. | loss1: 0.6744 | loss_class: 0.6737 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|02:44:47] 	Iter 121200 Done. | loss1: 1.8899 | loss_class: 1.8894 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:45:38] 	Iter 121300 Done. | loss1: 1.3407 | loss_class: 1.3401 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:46:30] 	Iter 121400 Done. | loss1: 0.5083 | loss_class: 0.5076 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:47:22] 	Iter 121500 Done. | loss1: 0.9420 | loss_class: 0.9413 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:48:14] 	Iter 121600 Done. | loss1: 0.9972 | loss_class: 0.9967 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:49:05] 	Iter 121700 Done. | loss1: 1.8318 | loss_class: 1.8313 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:49:56] 	Iter 121800 Done. | loss1: 0.6031 | loss_class: 0.6026 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:50:47] 	Iter 121900 Done. | loss1: 0.8840 | loss_class: 0.8831 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|02:51:38] 	Iter 122000 Done. | loss1: 0.7051 | loss_class: 0.7043 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|02:52:29] 	Iter 122100 Done. | loss1: 0.8161 | loss_class: 0.8154 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:53:21] 	Iter 122200 Done. | loss1: 0.7610 | loss_class: 0.7604 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|02:54:12] 	Iter 122300 Done. | loss1: 0.1556 | loss_class: 0.1549 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:55:04] 	Iter 122400 Done. | loss1: 1.2836 | loss_class: 1.2829 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|02:55:56] 	Iter 122500 Done. | loss1: 0.4588 | loss_class: 0.4580 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|02:56:47] 	Iter 122600 Done. | loss1: 2.2380 | loss_class: 2.2372 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|02:57:39] 	Iter 122700 Done. | loss1: 0.8838 | loss_class: 0.8833 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:58:31] 	Iter 122800 Done. | loss1: 1.8015 | loss_class: 1.8009 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|02:59:22] 	Iter 122900 Done. | loss1: 1.0465 | loss_class: 1.0458 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:00:13] 	Iter 123000 Done. | loss1: 0.8407 | loss_class: 0.8400 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:01:03] 	Iter 123100 Done. | loss1: 1.0631 | loss_class: 1.0626 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|03:01:55] 	Iter 123200 Done. | loss1: 1.5792 | loss_class: 1.5788 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|03:02:46] 	Iter 123300 Done. | loss1: 1.7580 | loss_class: 1.7576 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|03:03:38] 	Iter 123400 Done. | loss1: 1.3752 | loss_class: 1.3746 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:04:29] 	Iter 123500 Done. | loss1: 2.3606 | loss_class: 2.3600 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:05:21] 	Iter 123600 Done. | loss1: 0.8963 | loss_class: 0.8957 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:06:12] 	Iter 123700 Done. | loss1: 2.2224 | loss_class: 2.2218 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:07:04] 	Iter 123800 Done. | loss1: 1.1209 | loss_class: 1.1202 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:07:56] 	Iter 123900 Done. | loss1: 1.6945 | loss_class: 1.6939 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:08:47] 	Iter 124000 Done. | loss1: 2.1080 | loss_class: 2.1073 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:09:38] 	Iter 124100 Done. | loss1: 2.1174 | loss_class: 2.1168 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:10:30] 	Iter 124200 Done. | loss1: 1.8670 | loss_class: 1.8663 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|03:11:21] 	Iter 124300 Done. | loss1: 1.2228 | loss_class: 1.2221 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:12:13] 	Iter 124400 Done. | loss1: 1.2651 | loss_class: 1.2644 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:13:03] 	Iter 124500 Done. | loss1: 1.4589 | loss_class: 1.4583 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|03:13:54] 	Iter 124600 Done. | loss1: 0.5246 | loss_class: 0.5239 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:14:46] 	Iter 124700 Done. | loss1: 1.4829 | loss_class: 1.4823 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:15:37] 	Iter 124800 Done. | loss1: 1.2693 | loss_class: 1.2686 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:16:28] 	Iter 124900 Done. | loss1: 0.6562 | loss_class: 0.6556 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:17:20] 	Iter 125000 Done. | loss1: 0.8611 | loss_class: 0.8604 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:18:11] 	Iter 125100 Done. | loss1: 0.6447 | loss_class: 0.6440 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:19:03] 	Iter 125200 Done. | loss1: 0.8959 | loss_class: 0.8952 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:19:54] 	Iter 125300 Done. | loss1: 2.3697 | loss_class: 2.3690 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:20:45] 	Iter 125400 Done. | loss1: 1.0904 | loss_class: 1.0896 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|03:21:37] 	Iter 125500 Done. | loss1: 1.2995 | loss_class: 1.2989 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:22:28] 	Iter 125600 Done. | loss1: 0.0802 | loss_class: 0.0797 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|03:23:20] 	Iter 125700 Done. | loss1: 0.8148 | loss_class: 0.8141 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:24:13] 	Iter 125800 Done. | loss1: 1.8641 | loss_class: 1.8636 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|03:25:05] 	Iter 125900 Done. | loss1: 0.5827 | loss_class: 0.5818 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|03:25:56] 	Iter 126000 Done. | loss1: 2.0732 | loss_class: 2.0726 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:26:47] 	Iter 126100 Done. | loss1: 1.7463 | loss_class: 1.7456 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:27:39] 	Iter 126200 Done. | loss1: 0.8051 | loss_class: 0.8044 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:28:30] 	Iter 126300 Done. | loss1: 1.2981 | loss_class: 1.2974 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:29:22] 	Iter 126400 Done. | loss1: 2.1754 | loss_class: 2.1746 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|03:30:13] 	Iter 126500 Done. | loss1: 0.8337 | loss_class: 0.8332 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|03:31:05] 	Iter 126600 Done. | loss1: 1.7468 | loss_class: 1.7463 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:31:56] 	Iter 126700 Done. | loss1: 0.2536 | loss_class: 0.2529 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:32:48] 	Iter 126800 Done. | loss1: 0.4557 | loss_class: 0.4551 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:33:38] 	Iter 126900 Done. | loss1: 1.1977 | loss_class: 1.1970 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:34:31] 	Iter 127000 Done. | loss1: 1.1331 | loss_class: 1.1325 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:35:23] 	Iter 127100 Done. | loss1: 0.5552 | loss_class: 0.5545 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:36:14] 	Iter 127200 Done. | loss1: 1.1134 | loss_class: 1.1127 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:37:06] 	Iter 127300 Done. | loss1: 0.1737 | loss_class: 0.1730 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:37:58] 	Iter 127400 Done. | loss1: 1.3601 | loss_class: 1.3593 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|03:38:49] 	Iter 127500 Done. | loss1: 1.7699 | loss_class: 1.7692 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:39:41] 	Iter 127600 Done. | loss1: 0.6290 | loss_class: 0.6282 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|03:40:32] 	Iter 127700 Done. | loss1: 1.2630 | loss_class: 1.2625 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|03:41:25] 	Iter 127800 Done. | loss1: 1.1923 | loss_class: 1.1919 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|03:42:17] 	Iter 127900 Done. | loss1: 1.0504 | loss_class: 1.0496 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:43:09] 	Iter 128000 Done. | loss1: 2.1102 | loss_class: 2.1096 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:44:01] 	Iter 128100 Done. | loss1: 1.4019 | loss_class: 1.4011 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|03:44:53] 	Iter 128200 Done. | loss1: 0.5451 | loss_class: 0.5445 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:45:44] 	Iter 128300 Done. | loss1: 2.0138 | loss_class: 2.0131 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:46:36] 	Iter 128400 Done. | loss1: 1.5406 | loss_class: 1.5400 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:47:27] 	Iter 128500 Done. | loss1: 1.2059 | loss_class: 1.2054 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|03:48:19] 	Iter 128600 Done. | loss1: 0.6055 | loss_class: 0.6050 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|03:49:11] 	Iter 128700 Done. | loss1: 0.5011 | loss_class: 0.5003 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:50:03] 	Iter 128800 Done. | loss1: 0.8100 | loss_class: 0.8093 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:50:55] 	Iter 128900 Done. | loss1: 1.3507 | loss_class: 1.3500 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:51:46] 	Iter 129000 Done. | loss1: 1.9504 | loss_class: 1.9497 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:52:38] 	Iter 129100 Done. | loss1: 1.1773 | loss_class: 1.1766 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:53:29] 	Iter 129200 Done. | loss1: 1.2185 | loss_class: 1.2178 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|03:54:20] 	Iter 129300 Done. | loss1: 1.0510 | loss_class: 1.0504 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:55:12] 	Iter 129400 Done. | loss1: 0.9256 | loss_class: 0.9249 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|03:56:04] 	Iter 129500 Done. | loss1: 0.6107 | loss_class: 0.6101 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:56:56] 	Iter 129600 Done. | loss1: 0.9433 | loss_class: 0.9428 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:57:48] 	Iter 129700 Done. | loss1: 1.2558 | loss_class: 1.2551 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|03:58:39] 	Iter 129800 Done. | loss1: 1.0336 | loss_class: 1.0331 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|03:59:30] 	Iter 129900 Done. | loss1: 0.5965 | loss_class: 0.5958 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:00:22] 	Iter 130000 Done. | loss1: 2.2039 | loss_class: 2.2034 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:01:13] 	Iter 130100 Done. | loss1: 0.9335 | loss_class: 0.9328 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:02:05] 	Iter 130200 Done. | loss1: 1.2660 | loss_class: 1.2654 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:02:49] 	mean_loss1: 1.2946299220142785
[06.23.21|04:02:49] 	mean_loss_class: 1.2939806723566862
[06.23.21|04:02:49] 	mean_loss_recon: 0.0006492493709768989
[06.23.21|04:02:49] Time consumption:
[06.23.21|04:02:49] Done.
[06.23.21|04:02:49] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch22_model1.pt.
[06.23.21|04:02:49] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch22_model2.pt.
[06.23.21|04:02:49] Training epoch: 23
[06.23.21|04:02:57] 	Iter 130300 Done. | loss1: 1.0669 | loss_class: 1.0661 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|04:03:48] 	Iter 130400 Done. | loss1: 2.0397 | loss_class: 2.0391 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:04:40] 	Iter 130500 Done. | loss1: 0.0736 | loss_class: 0.0729 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:05:32] 	Iter 130600 Done. | loss1: 0.9128 | loss_class: 0.9122 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:06:24] 	Iter 130700 Done. | loss1: 0.6257 | loss_class: 0.6249 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:07:17] 	Iter 130800 Done. | loss1: 1.6790 | loss_class: 1.6785 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:08:08] 	Iter 130900 Done. | loss1: 1.4018 | loss_class: 1.4010 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|04:09:00] 	Iter 131000 Done. | loss1: 0.8010 | loss_class: 0.8000 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|04:09:52] 	Iter 131100 Done. | loss1: 1.3884 | loss_class: 1.3879 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:10:44] 	Iter 131200 Done. | loss1: 1.2148 | loss_class: 1.2141 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:11:35] 	Iter 131300 Done. | loss1: 2.1699 | loss_class: 2.1691 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|04:12:28] 	Iter 131400 Done. | loss1: 0.7349 | loss_class: 0.7345 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:13:19] 	Iter 131500 Done. | loss1: 0.3081 | loss_class: 0.3075 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:14:10] 	Iter 131600 Done. | loss1: 1.4297 | loss_class: 1.4291 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:15:01] 	Iter 131700 Done. | loss1: 2.3544 | loss_class: 2.3538 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:15:52] 	Iter 131800 Done. | loss1: 2.1897 | loss_class: 2.1891 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:16:43] 	Iter 131900 Done. | loss1: 0.8778 | loss_class: 0.8769 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|04:17:34] 	Iter 132000 Done. | loss1: 1.4216 | loss_class: 1.4210 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:18:27] 	Iter 132100 Done. | loss1: 3.0042 | loss_class: 3.0035 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:19:19] 	Iter 132200 Done. | loss1: 1.9667 | loss_class: 1.9661 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:20:10] 	Iter 132300 Done. | loss1: 1.5921 | loss_class: 1.5915 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:21:02] 	Iter 132400 Done. | loss1: 2.0206 | loss_class: 2.0200 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:21:54] 	Iter 132500 Done. | loss1: 1.5727 | loss_class: 1.5722 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:22:46] 	Iter 132600 Done. | loss1: 0.3538 | loss_class: 0.3533 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:23:37] 	Iter 132700 Done. | loss1: 1.4593 | loss_class: 1.4586 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:24:29] 	Iter 132800 Done. | loss1: 0.4443 | loss_class: 0.4436 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|04:25:20] 	Iter 132900 Done. | loss1: 0.3238 | loss_class: 0.3229 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|04:26:12] 	Iter 133000 Done. | loss1: 0.7774 | loss_class: 0.7766 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|04:27:04] 	Iter 133100 Done. | loss1: 1.6347 | loss_class: 1.6342 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:27:56] 	Iter 133200 Done. | loss1: 0.4063 | loss_class: 0.4056 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:28:48] 	Iter 133300 Done. | loss1: 3.0227 | loss_class: 3.0220 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:29:40] 	Iter 133400 Done. | loss1: 2.6597 | loss_class: 2.6589 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|04:30:31] 	Iter 133500 Done. | loss1: 0.9025 | loss_class: 0.9015 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|04:31:23] 	Iter 133600 Done. | loss1: 0.8089 | loss_class: 0.8083 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:32:14] 	Iter 133700 Done. | loss1: 0.7049 | loss_class: 0.7044 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:33:05] 	Iter 133800 Done. | loss1: 2.5910 | loss_class: 2.5905 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:33:57] 	Iter 133900 Done. | loss1: 2.3320 | loss_class: 2.3314 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:34:49] 	Iter 134000 Done. | loss1: 1.9037 | loss_class: 1.9030 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:35:40] 	Iter 134100 Done. | loss1: 1.8783 | loss_class: 1.8777 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:36:32] 	Iter 134200 Done. | loss1: 1.2986 | loss_class: 1.2978 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|04:37:24] 	Iter 134300 Done. | loss1: 0.9879 | loss_class: 0.9870 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|04:38:18] 	Iter 134400 Done. | loss1: 1.0369 | loss_class: 1.0364 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:39:10] 	Iter 134500 Done. | loss1: 1.2506 | loss_class: 1.2500 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:40:01] 	Iter 134600 Done. | loss1: 1.4006 | loss_class: 1.3999 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:40:54] 	Iter 134700 Done. | loss1: 0.8018 | loss_class: 0.8012 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:41:46] 	Iter 134800 Done. | loss1: 1.1654 | loss_class: 1.1647 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:42:38] 	Iter 134900 Done. | loss1: 1.1056 | loss_class: 1.1049 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:43:30] 	Iter 135000 Done. | loss1: 1.1046 | loss_class: 1.1041 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:44:20] 	Iter 135100 Done. | loss1: 0.6085 | loss_class: 0.6080 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:45:11] 	Iter 135200 Done. | loss1: 1.2025 | loss_class: 1.2019 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:46:02] 	Iter 135300 Done. | loss1: 0.4929 | loss_class: 0.4923 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:46:53] 	Iter 135400 Done. | loss1: 2.3566 | loss_class: 2.3560 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:47:44] 	Iter 135500 Done. | loss1: 2.7388 | loss_class: 2.7382 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:48:35] 	Iter 135600 Done. | loss1: 0.5239 | loss_class: 0.5233 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:49:26] 	Iter 135700 Done. | loss1: 1.3022 | loss_class: 1.3018 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:50:17] 	Iter 135800 Done. | loss1: 1.8371 | loss_class: 1.8364 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:51:08] 	Iter 135900 Done. | loss1: 0.4219 | loss_class: 0.4212 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:52:00] 	Iter 136000 Done. | loss1: 0.4559 | loss_class: 0.4555 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:52:50] 	Iter 136100 Done. | loss1: 0.3278 | loss_class: 0.3271 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:53:41] 	Iter 136200 Done. | loss1: 0.8368 | loss_class: 0.8360 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|04:54:32] 	Iter 136300 Done. | loss1: 0.9505 | loss_class: 0.9497 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|04:55:24] 	Iter 136400 Done. | loss1: 0.5321 | loss_class: 0.5315 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:56:14] 	Iter 136500 Done. | loss1: 1.4191 | loss_class: 1.4186 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:57:05] 	Iter 136600 Done. | loss1: 1.3594 | loss_class: 1.3588 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|04:57:56] 	Iter 136700 Done. | loss1: 1.3301 | loss_class: 1.3296 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:58:47] 	Iter 136800 Done. | loss1: 1.2657 | loss_class: 1.2653 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|04:59:38] 	Iter 136900 Done. | loss1: 0.5691 | loss_class: 0.5683 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|05:00:29] 	Iter 137000 Done. | loss1: 1.2190 | loss_class: 1.2183 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|05:01:20] 	Iter 137100 Done. | loss1: 1.4083 | loss_class: 1.4075 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|05:02:11] 	Iter 137200 Done. | loss1: 1.1511 | loss_class: 1.1504 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|05:03:02] 	Iter 137300 Done. | loss1: 0.6360 | loss_class: 0.6353 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|05:03:54] 	Iter 137400 Done. | loss1: 0.6770 | loss_class: 0.6765 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:04:45] 	Iter 137500 Done. | loss1: 0.5784 | loss_class: 0.5776 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|05:05:35] 	Iter 137600 Done. | loss1: 0.6114 | loss_class: 0.6110 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|05:06:26] 	Iter 137700 Done. | loss1: 0.7709 | loss_class: 0.7703 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:07:17] 	Iter 137800 Done. | loss1: 1.0823 | loss_class: 1.0817 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:08:08] 	Iter 137900 Done. | loss1: 1.4959 | loss_class: 1.4954 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:08:59] 	Iter 138000 Done. | loss1: 1.0365 | loss_class: 1.0357 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|05:09:50] 	Iter 138100 Done. | loss1: 1.2087 | loss_class: 1.2082 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:10:41] 	Iter 138200 Done. | loss1: 1.2134 | loss_class: 1.2129 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:11:32] 	Iter 138300 Done. | loss1: 1.3441 | loss_class: 1.3435 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:12:23] 	Iter 138400 Done. | loss1: 3.6993 | loss_class: 3.6987 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:13:13] 	Iter 138500 Done. | loss1: 1.2368 | loss_class: 1.2362 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:14:04] 	Iter 138600 Done. | loss1: 0.9772 | loss_class: 0.9765 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|05:14:55] 	Iter 138700 Done. | loss1: 1.6372 | loss_class: 1.6368 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|05:15:46] 	Iter 138800 Done. | loss1: 2.0953 | loss_class: 2.0946 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|05:16:37] 	Iter 138900 Done. | loss1: 0.4856 | loss_class: 0.4851 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:17:27] 	Iter 139000 Done. | loss1: 0.9599 | loss_class: 0.9593 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:18:18] 	Iter 139100 Done. | loss1: 0.0163 | loss_class: 0.0156 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|05:19:09] 	Iter 139200 Done. | loss1: 1.5472 | loss_class: 1.5466 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:20:00] 	Iter 139300 Done. | loss1: 1.0197 | loss_class: 1.0192 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:20:52] 	Iter 139400 Done. | loss1: 1.7726 | loss_class: 1.7721 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:21:43] 	Iter 139500 Done. | loss1: 1.5122 | loss_class: 1.5117 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:22:34] 	Iter 139600 Done. | loss1: 0.6358 | loss_class: 0.6350 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|05:23:24] 	Iter 139700 Done. | loss1: 1.4895 | loss_class: 1.4890 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:24:14] 	Iter 139800 Done. | loss1: 0.6949 | loss_class: 0.6944 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:25:05] 	Iter 139900 Done. | loss1: 1.9198 | loss_class: 1.9193 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:25:56] 	Iter 140000 Done. | loss1: 1.5292 | loss_class: 1.5286 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:26:47] 	Iter 140100 Done. | loss1: 0.9351 | loss_class: 0.9345 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:27:38] 	Iter 140200 Done. | loss1: 1.0483 | loss_class: 1.0476 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:28:28] 	Iter 140300 Done. | loss1: 1.8583 | loss_class: 1.8578 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:28:32] 	mean_loss1: 1.2769025224134605
[06.23.21|05:28:32] 	mean_loss_class: 1.2762538010019953
[06.23.21|05:28:32] 	mean_loss_recon: 0.0006487219306045404
[06.23.21|05:28:32] Time consumption:
[06.23.21|05:28:32] Done.
[06.23.21|05:28:32] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch23_model1.pt.
[06.23.21|05:28:32] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch23_model2.pt.
[06.23.21|05:28:32] Training epoch: 24
[06.23.21|05:29:20] 	Iter 140400 Done. | loss1: 1.2972 | loss_class: 1.2965 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|05:30:10] 	Iter 140500 Done. | loss1: 0.3738 | loss_class: 0.3733 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:31:01] 	Iter 140600 Done. | loss1: 1.3918 | loss_class: 1.3912 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:31:52] 	Iter 140700 Done. | loss1: 0.0402 | loss_class: 0.0395 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|05:32:42] 	Iter 140800 Done. | loss1: 1.7810 | loss_class: 1.7805 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:33:32] 	Iter 140900 Done. | loss1: 1.6404 | loss_class: 1.6398 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:34:24] 	Iter 141000 Done. | loss1: 1.6657 | loss_class: 1.6650 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|05:35:13] 	Iter 141100 Done. | loss1: 0.3276 | loss_class: 0.3268 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|05:36:03] 	Iter 141200 Done. | loss1: 1.5106 | loss_class: 1.5101 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:36:54] 	Iter 141300 Done. | loss1: 1.0821 | loss_class: 1.0812 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|05:37:44] 	Iter 141400 Done. | loss1: 2.0753 | loss_class: 2.0745 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|05:38:35] 	Iter 141500 Done. | loss1: 0.2729 | loss_class: 0.2722 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|05:39:24] 	Iter 141600 Done. | loss1: 0.9416 | loss_class: 0.9409 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:40:14] 	Iter 141700 Done. | loss1: 0.4309 | loss_class: 0.4303 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:41:04] 	Iter 141800 Done. | loss1: 2.9832 | loss_class: 2.9826 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:41:54] 	Iter 141900 Done. | loss1: 1.4483 | loss_class: 1.4475 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|05:42:44] 	Iter 142000 Done. | loss1: 0.9384 | loss_class: 0.9376 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|05:43:34] 	Iter 142100 Done. | loss1: 0.1895 | loss_class: 0.1885 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|05:44:24] 	Iter 142200 Done. | loss1: 1.1952 | loss_class: 1.1947 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:45:14] 	Iter 142300 Done. | loss1: 1.0421 | loss_class: 1.0415 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:46:04] 	Iter 142400 Done. | loss1: 0.9109 | loss_class: 0.9103 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:46:54] 	Iter 142500 Done. | loss1: 0.4620 | loss_class: 0.4612 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|05:47:44] 	Iter 142600 Done. | loss1: 0.7765 | loss_class: 0.7757 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|05:48:34] 	Iter 142700 Done. | loss1: 1.4989 | loss_class: 1.4978 | loss_recon: 0.0011 | lr: 0.100000
[06.23.21|05:49:25] 	Iter 142800 Done. | loss1: 1.3731 | loss_class: 1.3725 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:50:15] 	Iter 142900 Done. | loss1: 1.1389 | loss_class: 1.1383 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|05:51:05] 	Iter 143000 Done. | loss1: 1.9478 | loss_class: 1.9472 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:51:55] 	Iter 143100 Done. | loss1: 2.7379 | loss_class: 2.7372 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|05:52:46] 	Iter 143200 Done. | loss1: 1.2222 | loss_class: 1.2217 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:53:36] 	Iter 143300 Done. | loss1: 0.6896 | loss_class: 0.6891 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:54:26] 	Iter 143400 Done. | loss1: 1.2147 | loss_class: 1.2141 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:55:18] 	Iter 143500 Done. | loss1: 0.6585 | loss_class: 0.6576 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|05:56:08] 	Iter 143600 Done. | loss1: 0.4224 | loss_class: 0.4217 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|05:56:58] 	Iter 143700 Done. | loss1: 0.5618 | loss_class: 0.5611 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|05:57:48] 	Iter 143800 Done. | loss1: 1.1846 | loss_class: 1.1839 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|05:58:39] 	Iter 143900 Done. | loss1: 0.7437 | loss_class: 0.7432 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|05:59:29] 	Iter 144000 Done. | loss1: 1.6579 | loss_class: 1.6573 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:00:20] 	Iter 144100 Done. | loss1: 1.9901 | loss_class: 1.9893 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|06:01:10] 	Iter 144200 Done. | loss1: 1.5466 | loss_class: 1.5461 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|06:02:01] 	Iter 144300 Done. | loss1: 0.6369 | loss_class: 0.6364 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:02:51] 	Iter 144400 Done. | loss1: 0.3195 | loss_class: 0.3187 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:03:42] 	Iter 144500 Done. | loss1: 1.0538 | loss_class: 1.0529 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|06:04:32] 	Iter 144600 Done. | loss1: 1.0535 | loss_class: 1.0530 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|06:05:22] 	Iter 144700 Done. | loss1: 1.9001 | loss_class: 1.8996 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|06:06:13] 	Iter 144800 Done. | loss1: 0.4863 | loss_class: 0.4857 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:07:03] 	Iter 144900 Done. | loss1: 0.6650 | loss_class: 0.6645 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:07:52] 	Iter 145000 Done. | loss1: 1.4303 | loss_class: 1.4298 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|06:08:43] 	Iter 145100 Done. | loss1: 1.6786 | loss_class: 1.6777 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|06:09:35] 	Iter 145200 Done. | loss1: 1.2910 | loss_class: 1.2905 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:10:25] 	Iter 145300 Done. | loss1: 0.4635 | loss_class: 0.4625 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|06:11:15] 	Iter 145400 Done. | loss1: 2.2104 | loss_class: 2.2096 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|06:12:06] 	Iter 145500 Done. | loss1: 2.4059 | loss_class: 2.4052 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:12:56] 	Iter 145600 Done. | loss1: 1.1355 | loss_class: 1.1348 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:13:46] 	Iter 145700 Done. | loss1: 1.0370 | loss_class: 1.0363 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:14:36] 	Iter 145800 Done. | loss1: 0.6458 | loss_class: 0.6453 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|06:15:26] 	Iter 145900 Done. | loss1: 1.3144 | loss_class: 1.3138 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:16:16] 	Iter 146000 Done. | loss1: 2.8403 | loss_class: 2.8397 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:17:07] 	Iter 146100 Done. | loss1: 0.6851 | loss_class: 0.6844 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:17:57] 	Iter 146200 Done. | loss1: 0.7915 | loss_class: 0.7908 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:18:47] 	Iter 146300 Done. | loss1: 1.3099 | loss_class: 1.3093 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:19:37] 	Iter 146400 Done. | loss1: 0.0681 | loss_class: 0.0674 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:20:27] 	Iter 146500 Done. | loss1: 1.4881 | loss_class: 1.4875 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:21:17] 	Iter 146600 Done. | loss1: 0.2084 | loss_class: 0.2078 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|06:22:08] 	Iter 146700 Done. | loss1: 0.9223 | loss_class: 0.9217 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:22:58] 	Iter 146800 Done. | loss1: 2.5208 | loss_class: 2.5200 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|06:23:48] 	Iter 146900 Done. | loss1: 1.6468 | loss_class: 1.6460 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|06:24:38] 	Iter 147000 Done. | loss1: 1.8288 | loss_class: 1.8282 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:25:28] 	Iter 147100 Done. | loss1: 2.6518 | loss_class: 2.6513 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|06:26:18] 	Iter 147200 Done. | loss1: 0.9303 | loss_class: 0.9297 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:27:09] 	Iter 147300 Done. | loss1: 1.6835 | loss_class: 1.6828 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:27:59] 	Iter 147400 Done. | loss1: 0.4959 | loss_class: 0.4950 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|06:28:49] 	Iter 147500 Done. | loss1: 1.5988 | loss_class: 1.5983 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:29:40] 	Iter 147600 Done. | loss1: 2.6354 | loss_class: 2.6350 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|06:30:30] 	Iter 147700 Done. | loss1: 1.6668 | loss_class: 1.6662 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|06:31:20] 	Iter 147800 Done. | loss1: 1.4647 | loss_class: 1.4641 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:32:11] 	Iter 147900 Done. | loss1: 1.4872 | loss_class: 1.4866 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:33:02] 	Iter 148000 Done. | loss1: 2.4011 | loss_class: 2.4004 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|06:33:53] 	Iter 148100 Done. | loss1: 0.9049 | loss_class: 0.9042 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:34:43] 	Iter 148200 Done. | loss1: 1.1077 | loss_class: 1.1069 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|06:35:34] 	Iter 148300 Done. | loss1: 0.8386 | loss_class: 0.8377 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|06:36:24] 	Iter 148400 Done. | loss1: 1.5484 | loss_class: 1.5477 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:37:15] 	Iter 148500 Done. | loss1: 3.1475 | loss_class: 3.1469 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:38:05] 	Iter 148600 Done. | loss1: 1.0831 | loss_class: 1.0822 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|06:38:55] 	Iter 148700 Done. | loss1: 1.4221 | loss_class: 1.4214 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:39:46] 	Iter 148800 Done. | loss1: 0.4129 | loss_class: 0.4122 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:40:36] 	Iter 148900 Done. | loss1: 1.1005 | loss_class: 1.0997 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|06:41:26] 	Iter 149000 Done. | loss1: 1.1328 | loss_class: 1.1320 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|06:42:17] 	Iter 149100 Done. | loss1: 1.6762 | loss_class: 1.6756 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:43:08] 	Iter 149200 Done. | loss1: 1.1735 | loss_class: 1.1731 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|06:43:58] 	Iter 149300 Done. | loss1: 0.8956 | loss_class: 0.8950 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:44:49] 	Iter 149400 Done. | loss1: 0.6945 | loss_class: 0.6939 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:45:39] 	Iter 149500 Done. | loss1: 1.7953 | loss_class: 1.7948 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|06:46:29] 	Iter 149600 Done. | loss1: 1.3034 | loss_class: 1.3027 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:47:20] 	Iter 149700 Done. | loss1: 0.3491 | loss_class: 0.3485 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:48:10] 	Iter 149800 Done. | loss1: 1.1058 | loss_class: 1.1054 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|06:49:01] 	Iter 149900 Done. | loss1: 0.6068 | loss_class: 0.6061 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|06:49:52] 	Iter 150000 Done. | loss1: 2.6773 | loss_class: 2.6767 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:50:42] 	Iter 150100 Done. | loss1: 1.9032 | loss_class: 1.9025 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:51:33] 	Iter 150200 Done. | loss1: 1.2078 | loss_class: 1.2071 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|06:52:24] 	Iter 150300 Done. | loss1: 1.5188 | loss_class: 1.5183 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|06:52:39] 	mean_loss1: 1.2564419825516553
[06.23.21|06:52:39] 	mean_loss_class: 1.2557942167804874
[06.23.21|06:52:39] 	mean_loss_recon: 0.0006477654467326983
[06.23.21|06:52:39] Time consumption:
[06.23.21|06:52:39] Done.
[06.23.21|06:52:39] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch24_model1.pt.
[06.23.21|06:52:39] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch24_model2.pt.
[06.23.21|06:52:39] Eval epoch: 24
[06.23.21|06:59:10] 	mean_loss1: 1.5676805854074716
[06.23.21|06:59:10] 	mean_loss_class: 1.5673003890948702
[06.23.21|06:59:10] 	mean_loss_recon: 0.03801954996793769
[06.23.21|06:59:11] 

[06.23.21|06:59:11] 	Top1: 59.37%
[06.23.21|06:59:11] 

[06.23.21|06:59:11] 	Top5: 87.69%
[06.23.21|06:59:11] Done.
[06.23.21|06:59:11] Training epoch: 25
[06.23.21|06:59:47] 	Iter 150400 Done. | loss1: 1.4908 | loss_class: 1.4903 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:00:38] 	Iter 150500 Done. | loss1: 1.1482 | loss_class: 1.1476 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:01:28] 	Iter 150600 Done. | loss1: 0.0528 | loss_class: 0.0520 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|07:02:22] 	Iter 150700 Done. | loss1: 0.7380 | loss_class: 0.7374 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:03:13] 	Iter 150800 Done. | loss1: 2.6602 | loss_class: 2.6596 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:04:03] 	Iter 150900 Done. | loss1: 1.4869 | loss_class: 1.4862 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|07:04:52] 	Iter 151000 Done. | loss1: 1.6092 | loss_class: 1.6085 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|07:05:42] 	Iter 151100 Done. | loss1: 0.8508 | loss_class: 0.8502 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:06:33] 	Iter 151200 Done. | loss1: 2.8475 | loss_class: 2.8470 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:07:24] 	Iter 151300 Done. | loss1: 0.7450 | loss_class: 0.7444 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:08:15] 	Iter 151400 Done. | loss1: 1.0042 | loss_class: 1.0034 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|07:09:05] 	Iter 151500 Done. | loss1: 1.1295 | loss_class: 1.1289 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:09:56] 	Iter 151600 Done. | loss1: 0.6808 | loss_class: 0.6802 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:10:46] 	Iter 151700 Done. | loss1: 1.8393 | loss_class: 1.8386 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:11:36] 	Iter 151800 Done. | loss1: 2.5523 | loss_class: 2.5517 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:12:29] 	Iter 151900 Done. | loss1: 1.7771 | loss_class: 1.7765 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:13:20] 	Iter 152000 Done. | loss1: 0.6262 | loss_class: 0.6254 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|07:14:11] 	Iter 152100 Done. | loss1: 1.4805 | loss_class: 1.4799 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:15:01] 	Iter 152200 Done. | loss1: 1.6189 | loss_class: 1.6184 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:15:52] 	Iter 152300 Done. | loss1: 0.8908 | loss_class: 0.8903 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:16:42] 	Iter 152400 Done. | loss1: 1.7625 | loss_class: 1.7620 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:17:34] 	Iter 152500 Done. | loss1: 1.9807 | loss_class: 1.9799 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|07:18:23] 	Iter 152600 Done. | loss1: 0.6802 | loss_class: 0.6797 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:19:14] 	Iter 152700 Done. | loss1: 0.9431 | loss_class: 0.9424 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|07:20:05] 	Iter 152800 Done. | loss1: 0.0700 | loss_class: 0.0694 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:20:55] 	Iter 152900 Done. | loss1: 0.5544 | loss_class: 0.5538 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:21:45] 	Iter 153000 Done. | loss1: 0.4060 | loss_class: 0.4052 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|07:22:35] 	Iter 153100 Done. | loss1: 0.9215 | loss_class: 0.9206 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|07:23:26] 	Iter 153200 Done. | loss1: 0.6401 | loss_class: 0.6395 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:24:16] 	Iter 153300 Done. | loss1: 1.2634 | loss_class: 1.2629 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:25:06] 	Iter 153400 Done. | loss1: 1.4238 | loss_class: 1.4230 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|07:25:56] 	Iter 153500 Done. | loss1: 0.4143 | loss_class: 0.4137 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:26:47] 	Iter 153600 Done. | loss1: 0.9824 | loss_class: 0.9818 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:27:37] 	Iter 153700 Done. | loss1: 0.2621 | loss_class: 0.2614 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|07:28:27] 	Iter 153800 Done. | loss1: 1.3051 | loss_class: 1.3044 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:29:17] 	Iter 153900 Done. | loss1: 2.6458 | loss_class: 2.6452 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:30:10] 	Iter 154000 Done. | loss1: 0.7725 | loss_class: 0.7717 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|07:31:00] 	Iter 154100 Done. | loss1: 1.4155 | loss_class: 1.4148 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|07:31:51] 	Iter 154200 Done. | loss1: 1.1257 | loss_class: 1.1249 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|07:32:41] 	Iter 154300 Done. | loss1: 1.3705 | loss_class: 1.3698 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|07:33:31] 	Iter 154400 Done. | loss1: 0.5561 | loss_class: 0.5555 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:34:22] 	Iter 154500 Done. | loss1: 1.9124 | loss_class: 1.9119 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:35:12] 	Iter 154600 Done. | loss1: 0.4075 | loss_class: 0.4069 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:36:03] 	Iter 154700 Done. | loss1: 1.2185 | loss_class: 1.2176 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|07:36:53] 	Iter 154800 Done. | loss1: 1.2583 | loss_class: 1.2577 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:37:43] 	Iter 154900 Done. | loss1: 0.6355 | loss_class: 0.6349 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:38:34] 	Iter 155000 Done. | loss1: 0.9427 | loss_class: 0.9421 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:39:24] 	Iter 155100 Done. | loss1: 1.3734 | loss_class: 1.3726 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|07:40:15] 	Iter 155200 Done. | loss1: 2.2382 | loss_class: 2.2377 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:41:05] 	Iter 155300 Done. | loss1: 3.7405 | loss_class: 3.7400 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:41:59] 	Iter 155400 Done. | loss1: 0.4282 | loss_class: 0.4277 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:42:50] 	Iter 155500 Done. | loss1: 0.9538 | loss_class: 0.9530 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|07:43:39] 	Iter 155600 Done. | loss1: 0.4319 | loss_class: 0.4313 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:44:29] 	Iter 155700 Done. | loss1: 1.4161 | loss_class: 1.4153 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|07:45:19] 	Iter 155800 Done. | loss1: 0.7415 | loss_class: 0.7410 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:46:09] 	Iter 155900 Done. | loss1: 0.6914 | loss_class: 0.6908 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:46:59] 	Iter 156000 Done. | loss1: 1.5124 | loss_class: 1.5118 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:47:50] 	Iter 156100 Done. | loss1: 0.4486 | loss_class: 0.4478 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|07:48:41] 	Iter 156200 Done. | loss1: 0.8289 | loss_class: 0.8281 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|07:49:31] 	Iter 156300 Done. | loss1: 2.9281 | loss_class: 2.9276 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:50:23] 	Iter 156400 Done. | loss1: 1.4066 | loss_class: 1.4058 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|07:51:13] 	Iter 156500 Done. | loss1: 1.2867 | loss_class: 1.2862 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:52:04] 	Iter 156600 Done. | loss1: 0.9449 | loss_class: 0.9443 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:52:54] 	Iter 156700 Done. | loss1: 2.5418 | loss_class: 2.5412 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:53:45] 	Iter 156800 Done. | loss1: 2.2593 | loss_class: 2.2585 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|07:54:35] 	Iter 156900 Done. | loss1: 2.1240 | loss_class: 2.1235 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:55:26] 	Iter 157000 Done. | loss1: 0.1303 | loss_class: 0.1297 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|07:56:18] 	Iter 157100 Done. | loss1: 0.6628 | loss_class: 0.6621 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|07:57:09] 	Iter 157200 Done. | loss1: 1.9268 | loss_class: 1.9261 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|07:57:59] 	Iter 157300 Done. | loss1: 0.8027 | loss_class: 0.8021 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:58:49] 	Iter 157400 Done. | loss1: 1.6311 | loss_class: 1.6305 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|07:59:40] 	Iter 157500 Done. | loss1: 1.4216 | loss_class: 1.4209 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:00:31] 	Iter 157600 Done. | loss1: 1.2495 | loss_class: 1.2488 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:01:21] 	Iter 157700 Done. | loss1: 0.5549 | loss_class: 0.5541 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:02:11] 	Iter 157800 Done. | loss1: 1.8695 | loss_class: 1.8691 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|08:03:02] 	Iter 157900 Done. | loss1: 2.3767 | loss_class: 2.3759 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:03:52] 	Iter 158000 Done. | loss1: 0.9276 | loss_class: 0.9267 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|08:04:43] 	Iter 158100 Done. | loss1: 1.2908 | loss_class: 1.2900 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:05:34] 	Iter 158200 Done. | loss1: 0.8282 | loss_class: 0.8276 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:06:28] 	Iter 158300 Done. | loss1: 1.3003 | loss_class: 1.2996 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:07:19] 	Iter 158400 Done. | loss1: 0.3934 | loss_class: 0.3928 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:08:09] 	Iter 158500 Done. | loss1: 1.5682 | loss_class: 1.5676 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:08:59] 	Iter 158600 Done. | loss1: 1.2561 | loss_class: 1.2556 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|08:09:49] 	Iter 158700 Done. | loss1: 0.3065 | loss_class: 0.3059 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:10:39] 	Iter 158800 Done. | loss1: 1.3480 | loss_class: 1.3473 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:11:29] 	Iter 158900 Done. | loss1: 2.5630 | loss_class: 2.5623 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:12:19] 	Iter 159000 Done. | loss1: 1.3715 | loss_class: 1.3707 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:13:10] 	Iter 159100 Done. | loss1: 0.6279 | loss_class: 0.6271 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:13:59] 	Iter 159200 Done. | loss1: 0.8230 | loss_class: 0.8224 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:14:50] 	Iter 159300 Done. | loss1: 1.3831 | loss_class: 1.3822 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|08:15:41] 	Iter 159400 Done. | loss1: 1.3770 | loss_class: 1.3765 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|08:16:31] 	Iter 159500 Done. | loss1: 1.5443 | loss_class: 1.5435 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:17:22] 	Iter 159600 Done. | loss1: 2.5240 | loss_class: 2.5235 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:18:13] 	Iter 159700 Done. | loss1: 0.6689 | loss_class: 0.6680 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|08:19:03] 	Iter 159800 Done. | loss1: 1.5685 | loss_class: 1.5677 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:19:53] 	Iter 159900 Done. | loss1: 0.6732 | loss_class: 0.6722 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|08:20:42] 	Iter 160000 Done. | loss1: 1.0185 | loss_class: 1.0179 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|08:21:33] 	Iter 160100 Done. | loss1: 0.6290 | loss_class: 0.6283 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:22:23] 	Iter 160200 Done. | loss1: 0.8168 | loss_class: 0.8162 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:23:13] 	Iter 160300 Done. | loss1: 0.3018 | loss_class: 0.3012 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|08:23:39] 	mean_loss1: 1.2443463620887723
[06.23.21|08:23:39] 	mean_loss_class: 1.2436975452408114
[06.23.21|08:23:39] 	mean_loss_recon: 0.0006488166588786607
[06.23.21|08:23:39] Time consumption:
[06.23.21|08:23:39] Done.
[06.23.21|08:23:39] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch25_model1.pt.
[06.23.21|08:23:39] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch25_model2.pt.
[06.23.21|08:23:39] Training epoch: 26
[06.23.21|08:24:04] 	Iter 160400 Done. | loss1: 0.1973 | loss_class: 0.1965 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:24:54] 	Iter 160500 Done. | loss1: 0.8836 | loss_class: 0.8830 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:25:44] 	Iter 160600 Done. | loss1: 1.3029 | loss_class: 1.3022 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:26:36] 	Iter 160700 Done. | loss1: 0.9242 | loss_class: 0.9235 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:27:32] 	Iter 160800 Done. | loss1: 1.8254 | loss_class: 1.8247 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:28:24] 	Iter 160900 Done. | loss1: 0.3400 | loss_class: 0.3393 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:29:16] 	Iter 161000 Done. | loss1: 2.2976 | loss_class: 2.2970 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:30:06] 	Iter 161100 Done. | loss1: 1.6424 | loss_class: 1.6417 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:30:57] 	Iter 161200 Done. | loss1: 0.7008 | loss_class: 0.7001 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:31:51] 	Iter 161300 Done. | loss1: 2.0222 | loss_class: 2.0215 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:32:41] 	Iter 161400 Done. | loss1: 0.4937 | loss_class: 0.4930 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:33:32] 	Iter 161500 Done. | loss1: 0.8692 | loss_class: 0.8685 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:34:22] 	Iter 161600 Done. | loss1: 1.6656 | loss_class: 1.6649 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:35:13] 	Iter 161700 Done. | loss1: 2.4748 | loss_class: 2.4742 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:36:03] 	Iter 161800 Done. | loss1: 1.4093 | loss_class: 1.4085 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:36:56] 	Iter 161900 Done. | loss1: 2.2455 | loss_class: 2.2449 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:37:51] 	Iter 162000 Done. | loss1: 0.8913 | loss_class: 0.8907 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:38:41] 	Iter 162100 Done. | loss1: 2.0730 | loss_class: 2.0724 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:39:31] 	Iter 162200 Done. | loss1: 1.4335 | loss_class: 1.4329 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:40:22] 	Iter 162300 Done. | loss1: 0.7835 | loss_class: 0.7830 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:41:12] 	Iter 162400 Done. | loss1: 0.5121 | loss_class: 0.5113 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:42:03] 	Iter 162500 Done. | loss1: 1.9924 | loss_class: 1.9918 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:42:53] 	Iter 162600 Done. | loss1: 0.8187 | loss_class: 0.8179 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:43:43] 	Iter 162700 Done. | loss1: 1.2342 | loss_class: 1.2336 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:44:34] 	Iter 162800 Done. | loss1: 1.3517 | loss_class: 1.3513 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|08:45:24] 	Iter 162900 Done. | loss1: 1.5708 | loss_class: 1.5705 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|08:46:14] 	Iter 163000 Done. | loss1: 2.9628 | loss_class: 2.9622 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:47:05] 	Iter 163100 Done. | loss1: 1.1732 | loss_class: 1.1725 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:47:55] 	Iter 163200 Done. | loss1: 0.2857 | loss_class: 0.2850 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:48:46] 	Iter 163300 Done. | loss1: 0.9771 | loss_class: 0.9765 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:49:36] 	Iter 163400 Done. | loss1: 0.8710 | loss_class: 0.8704 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:50:27] 	Iter 163500 Done. | loss1: 0.2058 | loss_class: 0.2051 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:51:17] 	Iter 163600 Done. | loss1: 1.0357 | loss_class: 1.0351 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:52:08] 	Iter 163700 Done. | loss1: 0.4119 | loss_class: 0.4114 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|08:52:58] 	Iter 163800 Done. | loss1: 0.8544 | loss_class: 0.8539 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:53:48] 	Iter 163900 Done. | loss1: 3.6980 | loss_class: 3.6972 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:54:38] 	Iter 164000 Done. | loss1: 1.3960 | loss_class: 1.3952 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:55:28] 	Iter 164100 Done. | loss1: 2.6899 | loss_class: 2.6894 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|08:56:18] 	Iter 164200 Done. | loss1: 0.3520 | loss_class: 0.3512 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|08:57:09] 	Iter 164300 Done. | loss1: 0.6189 | loss_class: 0.6183 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|08:58:00] 	Iter 164400 Done. | loss1: 0.6850 | loss_class: 0.6842 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:58:51] 	Iter 164500 Done. | loss1: 1.8336 | loss_class: 1.8329 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|08:59:41] 	Iter 164600 Done. | loss1: 1.7254 | loss_class: 1.7248 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:00:31] 	Iter 164700 Done. | loss1: 0.3220 | loss_class: 0.3212 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|09:01:21] 	Iter 164800 Done. | loss1: 1.7451 | loss_class: 1.7444 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|09:02:11] 	Iter 164900 Done. | loss1: 0.9730 | loss_class: 0.9725 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:03:03] 	Iter 165000 Done. | loss1: 0.2762 | loss_class: 0.2756 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:03:56] 	Iter 165100 Done. | loss1: 0.7224 | loss_class: 0.7218 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:04:45] 	Iter 165200 Done. | loss1: 0.5684 | loss_class: 0.5677 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:05:34] 	Iter 165300 Done. | loss1: 0.7041 | loss_class: 0.7034 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:06:24] 	Iter 165400 Done. | loss1: 1.5088 | loss_class: 1.5081 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:07:14] 	Iter 165500 Done. | loss1: 1.1826 | loss_class: 1.1820 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:08:05] 	Iter 165600 Done. | loss1: 1.5541 | loss_class: 1.5533 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:08:55] 	Iter 165700 Done. | loss1: 0.7340 | loss_class: 0.7333 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:09:45] 	Iter 165800 Done. | loss1: 0.4738 | loss_class: 0.4730 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:10:37] 	Iter 165900 Done. | loss1: 0.7145 | loss_class: 0.7138 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:11:27] 	Iter 166000 Done. | loss1: 1.4631 | loss_class: 1.4624 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:12:17] 	Iter 166100 Done. | loss1: 1.3585 | loss_class: 1.3577 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|09:13:07] 	Iter 166200 Done. | loss1: 1.1966 | loss_class: 1.1961 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:13:56] 	Iter 166300 Done. | loss1: 0.7928 | loss_class: 0.7922 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:14:46] 	Iter 166400 Done. | loss1: 1.5072 | loss_class: 1.5065 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:15:37] 	Iter 166500 Done. | loss1: 0.8049 | loss_class: 0.8041 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|09:16:29] 	Iter 166600 Done. | loss1: 0.8976 | loss_class: 0.8969 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|09:17:19] 	Iter 166700 Done. | loss1: 1.2505 | loss_class: 1.2498 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|09:18:09] 	Iter 166800 Done. | loss1: 0.8283 | loss_class: 0.8276 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|09:18:59] 	Iter 166900 Done. | loss1: 0.5839 | loss_class: 0.5834 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:19:50] 	Iter 167000 Done. | loss1: 0.1597 | loss_class: 0.1590 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:20:40] 	Iter 167100 Done. | loss1: 1.6936 | loss_class: 1.6930 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:21:30] 	Iter 167200 Done. | loss1: 0.4218 | loss_class: 0.4212 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:22:21] 	Iter 167300 Done. | loss1: 1.3192 | loss_class: 1.3188 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|09:23:11] 	Iter 167400 Done. | loss1: 2.1994 | loss_class: 2.1987 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:24:00] 	Iter 167500 Done. | loss1: 0.4616 | loss_class: 0.4610 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:24:50] 	Iter 167600 Done. | loss1: 1.1367 | loss_class: 1.1362 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:25:40] 	Iter 167700 Done. | loss1: 0.7167 | loss_class: 0.7161 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:26:31] 	Iter 167800 Done. | loss1: 0.7324 | loss_class: 0.7319 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:27:20] 	Iter 167900 Done. | loss1: 1.1788 | loss_class: 1.1783 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:28:10] 	Iter 168000 Done. | loss1: 0.3584 | loss_class: 0.3577 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:29:01] 	Iter 168100 Done. | loss1: 0.8859 | loss_class: 0.8850 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|09:29:52] 	Iter 168200 Done. | loss1: 0.1207 | loss_class: 0.1201 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:30:43] 	Iter 168300 Done. | loss1: 2.3625 | loss_class: 2.3620 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:31:33] 	Iter 168400 Done. | loss1: 0.3150 | loss_class: 0.3144 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:32:23] 	Iter 168500 Done. | loss1: 0.5388 | loss_class: 0.5382 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:33:14] 	Iter 168600 Done. | loss1: 2.5059 | loss_class: 2.5053 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:34:04] 	Iter 168700 Done. | loss1: 0.1179 | loss_class: 0.1168 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|09:34:54] 	Iter 168800 Done. | loss1: 1.9231 | loss_class: 1.9225 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:35:45] 	Iter 168900 Done. | loss1: 0.0236 | loss_class: 0.0229 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|09:36:35] 	Iter 169000 Done. | loss1: 2.2522 | loss_class: 2.2517 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:37:26] 	Iter 169100 Done. | loss1: 1.4276 | loss_class: 1.4271 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:38:16] 	Iter 169200 Done. | loss1: 2.3958 | loss_class: 2.3951 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:39:06] 	Iter 169300 Done. | loss1: 1.2129 | loss_class: 1.2124 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:39:57] 	Iter 169400 Done. | loss1: 0.4929 | loss_class: 0.4923 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:40:47] 	Iter 169500 Done. | loss1: 1.5999 | loss_class: 1.5994 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:41:37] 	Iter 169600 Done. | loss1: 1.3933 | loss_class: 1.3926 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:42:29] 	Iter 169700 Done. | loss1: 3.0883 | loss_class: 3.0878 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:43:19] 	Iter 169800 Done. | loss1: 0.8121 | loss_class: 0.8114 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:44:10] 	Iter 169900 Done. | loss1: 1.2552 | loss_class: 1.2545 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:45:00] 	Iter 170000 Done. | loss1: 1.5249 | loss_class: 1.5244 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:45:50] 	Iter 170100 Done. | loss1: 0.8783 | loss_class: 0.8777 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:46:40] 	Iter 170200 Done. | loss1: 1.0024 | loss_class: 1.0017 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:47:31] 	Iter 170300 Done. | loss1: 0.8430 | loss_class: 0.8425 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:48:07] 	mean_loss1: 1.2354962107872454
[06.23.21|09:48:07] 	mean_loss_class: 1.234846816040989
[06.23.21|09:48:07] 	mean_loss_recon: 0.0006493952414520848
[06.23.21|09:48:07] Time consumption:
[06.23.21|09:48:07] Done.
[06.23.21|09:48:08] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch26_model1.pt.
[06.23.21|09:48:08] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch26_model2.pt.
[06.23.21|09:48:08] Training epoch: 27
[06.23.21|09:48:23] 	Iter 170400 Done. | loss1: 1.7487 | loss_class: 1.7481 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:49:13] 	Iter 170500 Done. | loss1: 2.8806 | loss_class: 2.8800 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|09:50:03] 	Iter 170600 Done. | loss1: 0.5603 | loss_class: 0.5597 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:50:53] 	Iter 170700 Done. | loss1: 0.8946 | loss_class: 0.8939 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:51:42] 	Iter 170800 Done. | loss1: 0.2584 | loss_class: 0.2578 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:52:32] 	Iter 170900 Done. | loss1: 1.1292 | loss_class: 1.1284 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|09:53:23] 	Iter 171000 Done. | loss1: 1.7474 | loss_class: 1.7465 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|09:54:13] 	Iter 171100 Done. | loss1: 0.6844 | loss_class: 0.6834 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|09:55:04] 	Iter 171200 Done. | loss1: 3.5931 | loss_class: 3.5924 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:55:55] 	Iter 171300 Done. | loss1: 0.3363 | loss_class: 0.3355 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|09:56:45] 	Iter 171400 Done. | loss1: 2.9365 | loss_class: 2.9361 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|09:57:35] 	Iter 171500 Done. | loss1: 0.9454 | loss_class: 0.9449 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|09:58:28] 	Iter 171600 Done. | loss1: 1.4222 | loss_class: 1.4216 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|09:59:18] 	Iter 171700 Done. | loss1: 1.3288 | loss_class: 1.3281 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:00:08] 	Iter 171800 Done. | loss1: 1.1900 | loss_class: 1.1894 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:00:58] 	Iter 171900 Done. | loss1: 0.5884 | loss_class: 0.5879 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|10:01:48] 	Iter 172000 Done. | loss1: 0.8289 | loss_class: 0.8284 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|10:02:38] 	Iter 172100 Done. | loss1: 1.5910 | loss_class: 1.5905 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|10:03:28] 	Iter 172200 Done. | loss1: 1.6437 | loss_class: 1.6431 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:04:17] 	Iter 172300 Done. | loss1: 1.0997 | loss_class: 1.0990 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:05:07] 	Iter 172400 Done. | loss1: 0.5750 | loss_class: 0.5744 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:05:58] 	Iter 172500 Done. | loss1: 0.8684 | loss_class: 0.8677 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:06:51] 	Iter 172600 Done. | loss1: 1.5528 | loss_class: 1.5521 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:07:42] 	Iter 172700 Done. | loss1: 0.7526 | loss_class: 0.7520 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:08:33] 	Iter 172800 Done. | loss1: 1.5205 | loss_class: 1.5198 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:09:23] 	Iter 172900 Done. | loss1: 1.1720 | loss_class: 1.1713 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:10:14] 	Iter 173000 Done. | loss1: 0.9613 | loss_class: 0.9606 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:11:07] 	Iter 173100 Done. | loss1: 1.4541 | loss_class: 1.4537 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|10:11:58] 	Iter 173200 Done. | loss1: 2.0827 | loss_class: 2.0820 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:12:49] 	Iter 173300 Done. | loss1: 2.9160 | loss_class: 2.9154 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:13:39] 	Iter 173400 Done. | loss1: 2.3939 | loss_class: 2.3932 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:14:30] 	Iter 173500 Done. | loss1: 0.2924 | loss_class: 0.2916 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|10:15:20] 	Iter 173600 Done. | loss1: 1.4395 | loss_class: 1.4388 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|10:16:11] 	Iter 173700 Done. | loss1: 1.3950 | loss_class: 1.3943 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:17:01] 	Iter 173800 Done. | loss1: 1.3734 | loss_class: 1.3725 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|10:17:52] 	Iter 173900 Done. | loss1: 2.2200 | loss_class: 2.2194 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:18:44] 	Iter 174000 Done. | loss1: 0.2406 | loss_class: 0.2400 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|10:19:34] 	Iter 174100 Done. | loss1: 0.8826 | loss_class: 0.8821 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:20:25] 	Iter 174200 Done. | loss1: 2.1174 | loss_class: 2.1167 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:21:15] 	Iter 174300 Done. | loss1: 1.8028 | loss_class: 1.8021 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:22:06] 	Iter 174400 Done. | loss1: 1.5645 | loss_class: 1.5637 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|10:22:55] 	Iter 174500 Done. | loss1: 0.8369 | loss_class: 0.8363 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|10:23:45] 	Iter 174600 Done. | loss1: 1.2598 | loss_class: 1.2591 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:24:36] 	Iter 174700 Done. | loss1: 0.7217 | loss_class: 0.7212 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|10:25:26] 	Iter 174800 Done. | loss1: 0.5914 | loss_class: 0.5906 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|10:26:16] 	Iter 174900 Done. | loss1: 0.9787 | loss_class: 0.9779 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|10:27:06] 	Iter 175000 Done. | loss1: 0.9374 | loss_class: 0.9367 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:27:56] 	Iter 175100 Done. | loss1: 0.3179 | loss_class: 0.3172 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|10:28:46] 	Iter 175200 Done. | loss1: 0.1427 | loss_class: 0.1420 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:29:36] 	Iter 175300 Done. | loss1: 1.9829 | loss_class: 1.9823 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:30:26] 	Iter 175400 Done. | loss1: 1.1738 | loss_class: 1.1732 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:31:17] 	Iter 175500 Done. | loss1: 1.4969 | loss_class: 1.4960 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|10:32:07] 	Iter 175600 Done. | loss1: 1.3758 | loss_class: 1.3753 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:32:58] 	Iter 175700 Done. | loss1: 1.1230 | loss_class: 1.1225 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:33:48] 	Iter 175800 Done. | loss1: 0.3945 | loss_class: 0.3940 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|10:34:39] 	Iter 175900 Done. | loss1: 0.9630 | loss_class: 0.9622 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|10:35:29] 	Iter 176000 Done. | loss1: 0.4337 | loss_class: 0.4331 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:36:20] 	Iter 176100 Done. | loss1: 2.7710 | loss_class: 2.7705 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|10:37:12] 	Iter 176200 Done. | loss1: 1.8827 | loss_class: 1.8819 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:38:03] 	Iter 176300 Done. | loss1: 0.8361 | loss_class: 0.8354 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:38:53] 	Iter 176400 Done. | loss1: 0.4938 | loss_class: 0.4931 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:39:43] 	Iter 176500 Done. | loss1: 0.3833 | loss_class: 0.3827 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:40:33] 	Iter 176600 Done. | loss1: 0.8419 | loss_class: 0.8413 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:41:23] 	Iter 176700 Done. | loss1: 1.7226 | loss_class: 1.7221 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:42:13] 	Iter 176800 Done. | loss1: 0.3150 | loss_class: 0.3142 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:43:04] 	Iter 176900 Done. | loss1: 1.0533 | loss_class: 1.0527 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:43:55] 	Iter 177000 Done. | loss1: 1.0352 | loss_class: 1.0345 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:44:47] 	Iter 177100 Done. | loss1: 1.0067 | loss_class: 1.0062 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:45:40] 	Iter 177200 Done. | loss1: 1.9173 | loss_class: 1.9167 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:46:31] 	Iter 177300 Done. | loss1: 0.4430 | loss_class: 0.4425 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|10:47:23] 	Iter 177400 Done. | loss1: 0.3705 | loss_class: 0.3698 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:48:16] 	Iter 177500 Done. | loss1: 1.3621 | loss_class: 1.3613 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|10:49:08] 	Iter 177600 Done. | loss1: 2.4802 | loss_class: 2.4795 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:50:01] 	Iter 177700 Done. | loss1: 1.1548 | loss_class: 1.1541 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|10:50:53] 	Iter 177800 Done. | loss1: 1.6690 | loss_class: 1.6685 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|10:51:45] 	Iter 177900 Done. | loss1: 2.6747 | loss_class: 2.6741 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|10:52:37] 	Iter 178000 Done. | loss1: 1.4828 | loss_class: 1.4824 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|10:53:28] 	Iter 178100 Done. | loss1: 1.3958 | loss_class: 1.3948 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|10:54:20] 	Iter 178200 Done. | loss1: 1.7109 | loss_class: 1.7104 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|10:55:12] 	Iter 178300 Done. | loss1: 2.0687 | loss_class: 2.0681 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:56:03] 	Iter 178400 Done. | loss1: 0.7435 | loss_class: 0.7429 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:56:55] 	Iter 178500 Done. | loss1: 0.2175 | loss_class: 0.2171 | loss_recon: 0.0003 | lr: 0.100000
[06.23.21|10:57:46] 	Iter 178600 Done. | loss1: 1.3008 | loss_class: 1.3002 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|10:58:38] 	Iter 178700 Done. | loss1: 0.5148 | loss_class: 0.5144 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|10:59:30] 	Iter 178800 Done. | loss1: 1.8520 | loss_class: 1.8513 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:00:21] 	Iter 178900 Done. | loss1: 2.3615 | loss_class: 2.3606 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|11:01:13] 	Iter 179000 Done. | loss1: 1.0401 | loss_class: 1.0394 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:02:05] 	Iter 179100 Done. | loss1: 0.8233 | loss_class: 0.8226 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:02:57] 	Iter 179200 Done. | loss1: 1.7090 | loss_class: 1.7085 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:03:49] 	Iter 179300 Done. | loss1: 1.8201 | loss_class: 1.8195 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:04:40] 	Iter 179400 Done. | loss1: 0.0883 | loss_class: 0.0876 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:05:32] 	Iter 179500 Done. | loss1: 2.2495 | loss_class: 2.2491 | loss_recon: 0.0003 | lr: 0.100000
[06.23.21|11:06:23] 	Iter 179600 Done. | loss1: 0.9131 | loss_class: 0.9125 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:07:15] 	Iter 179700 Done. | loss1: 0.2585 | loss_class: 0.2578 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:08:07] 	Iter 179800 Done. | loss1: 1.3144 | loss_class: 1.3136 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|11:08:59] 	Iter 179900 Done. | loss1: 1.2867 | loss_class: 1.2863 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:09:50] 	Iter 180000 Done. | loss1: 2.1076 | loss_class: 2.1070 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:10:42] 	Iter 180100 Done. | loss1: 0.0895 | loss_class: 0.0889 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:11:34] 	Iter 180200 Done. | loss1: 0.7017 | loss_class: 0.7011 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:12:26] 	Iter 180300 Done. | loss1: 1.3109 | loss_class: 1.3103 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:13:15] 	mean_loss1: 1.217418708088595
[06.23.21|11:13:15] 	mean_loss_class: 1.2167695837559183
[06.23.21|11:13:15] 	mean_loss_recon: 0.000649124600433335
[06.23.21|11:13:15] Time consumption:
[06.23.21|11:13:15] Done.
[06.23.21|11:13:15] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch27_model1.pt.
[06.23.21|11:13:15] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch27_model2.pt.
[06.23.21|11:13:15] Training epoch: 28
[06.23.21|11:13:18] 	Iter 180400 Done. | loss1: 1.9765 | loss_class: 1.9760 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:14:10] 	Iter 180500 Done. | loss1: 1.7658 | loss_class: 1.7653 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:15:01] 	Iter 180600 Done. | loss1: 0.2204 | loss_class: 0.2199 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:15:53] 	Iter 180700 Done. | loss1: 1.0415 | loss_class: 1.0408 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:16:45] 	Iter 180800 Done. | loss1: 1.5722 | loss_class: 1.5715 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:17:36] 	Iter 180900 Done. | loss1: 1.5672 | loss_class: 1.5666 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:18:28] 	Iter 181000 Done. | loss1: 1.1941 | loss_class: 1.1934 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:19:20] 	Iter 181100 Done. | loss1: 3.5319 | loss_class: 3.5313 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:20:11] 	Iter 181200 Done. | loss1: 1.8954 | loss_class: 1.8948 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:21:03] 	Iter 181300 Done. | loss1: 1.8558 | loss_class: 1.8553 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|11:21:54] 	Iter 181400 Done. | loss1: 0.7682 | loss_class: 0.7674 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|11:22:46] 	Iter 181500 Done. | loss1: 2.5488 | loss_class: 2.5482 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:23:38] 	Iter 181600 Done. | loss1: 1.0973 | loss_class: 1.0966 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:24:29] 	Iter 181700 Done. | loss1: 0.8810 | loss_class: 0.8803 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:25:21] 	Iter 181800 Done. | loss1: 1.5977 | loss_class: 1.5972 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:26:12] 	Iter 181900 Done. | loss1: 0.6608 | loss_class: 0.6601 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:27:04] 	Iter 182000 Done. | loss1: 0.4510 | loss_class: 0.4504 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:27:55] 	Iter 182100 Done. | loss1: 1.2248 | loss_class: 1.2241 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:28:46] 	Iter 182200 Done. | loss1: 1.9117 | loss_class: 1.9112 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:29:38] 	Iter 182300 Done. | loss1: 0.9522 | loss_class: 0.9515 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|11:30:29] 	Iter 182400 Done. | loss1: 1.9958 | loss_class: 1.9952 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:31:21] 	Iter 182500 Done. | loss1: 0.7261 | loss_class: 0.7256 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:32:12] 	Iter 182600 Done. | loss1: 0.5648 | loss_class: 0.5641 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:33:04] 	Iter 182700 Done. | loss1: 0.7405 | loss_class: 0.7397 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|11:33:55] 	Iter 182800 Done. | loss1: 0.1121 | loss_class: 0.1115 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:34:47] 	Iter 182900 Done. | loss1: 1.9455 | loss_class: 1.9450 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:35:39] 	Iter 183000 Done. | loss1: 0.2861 | loss_class: 0.2853 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:36:30] 	Iter 183100 Done. | loss1: 0.7724 | loss_class: 0.7717 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:37:22] 	Iter 183200 Done. | loss1: 2.4218 | loss_class: 2.4212 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:38:13] 	Iter 183300 Done. | loss1: 1.3177 | loss_class: 1.3171 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:39:05] 	Iter 183400 Done. | loss1: 1.8373 | loss_class: 1.8365 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|11:39:56] 	Iter 183500 Done. | loss1: 2.2501 | loss_class: 2.2494 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:40:48] 	Iter 183600 Done. | loss1: 1.5866 | loss_class: 1.5860 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:41:39] 	Iter 183700 Done. | loss1: 1.9204 | loss_class: 1.9197 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:42:31] 	Iter 183800 Done. | loss1: 1.2346 | loss_class: 1.2339 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:43:23] 	Iter 183900 Done. | loss1: 1.3058 | loss_class: 1.3054 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:44:14] 	Iter 184000 Done. | loss1: 0.5365 | loss_class: 0.5360 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:45:06] 	Iter 184100 Done. | loss1: 1.7781 | loss_class: 1.7774 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:45:57] 	Iter 184200 Done. | loss1: 1.3080 | loss_class: 1.3075 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:46:49] 	Iter 184300 Done. | loss1: 0.3071 | loss_class: 0.3063 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|11:47:40] 	Iter 184400 Done. | loss1: 1.0822 | loss_class: 1.0817 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:48:32] 	Iter 184500 Done. | loss1: 1.1269 | loss_class: 1.1264 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:49:23] 	Iter 184600 Done. | loss1: 0.0577 | loss_class: 0.0571 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:50:15] 	Iter 184700 Done. | loss1: 1.5656 | loss_class: 1.5649 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:51:06] 	Iter 184800 Done. | loss1: 2.6534 | loss_class: 2.6528 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|11:51:58] 	Iter 184900 Done. | loss1: 1.9418 | loss_class: 1.9414 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:52:50] 	Iter 185000 Done. | loss1: 2.7877 | loss_class: 2.7872 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:53:41] 	Iter 185100 Done. | loss1: 1.6306 | loss_class: 1.6302 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:54:32] 	Iter 185200 Done. | loss1: 0.9882 | loss_class: 0.9876 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:55:23] 	Iter 185300 Done. | loss1: 2.9084 | loss_class: 2.9078 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:56:14] 	Iter 185400 Done. | loss1: 1.0537 | loss_class: 1.0532 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:57:06] 	Iter 185500 Done. | loss1: 0.8470 | loss_class: 0.8465 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:57:56] 	Iter 185600 Done. | loss1: 0.7408 | loss_class: 0.7403 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|11:58:48] 	Iter 185700 Done. | loss1: 0.4885 | loss_class: 0.4879 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|11:59:39] 	Iter 185800 Done. | loss1: 0.9854 | loss_class: 0.9849 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:00:30] 	Iter 185900 Done. | loss1: 0.7594 | loss_class: 0.7589 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|12:01:21] 	Iter 186000 Done. | loss1: 0.9308 | loss_class: 0.9304 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|12:02:12] 	Iter 186100 Done. | loss1: 0.4253 | loss_class: 0.4247 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:03:03] 	Iter 186200 Done. | loss1: 1.2558 | loss_class: 1.2549 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|12:03:55] 	Iter 186300 Done. | loss1: 0.5073 | loss_class: 0.5068 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:04:47] 	Iter 186400 Done. | loss1: 2.8288 | loss_class: 2.8280 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:05:39] 	Iter 186500 Done. | loss1: 2.7942 | loss_class: 2.7935 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:06:30] 	Iter 186600 Done. | loss1: 0.7628 | loss_class: 0.7620 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|12:07:22] 	Iter 186700 Done. | loss1: 0.5468 | loss_class: 0.5461 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:08:13] 	Iter 186800 Done. | loss1: 0.9122 | loss_class: 0.9117 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|12:09:05] 	Iter 186900 Done. | loss1: 0.0475 | loss_class: 0.0467 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|12:09:57] 	Iter 187000 Done. | loss1: 0.4938 | loss_class: 0.4932 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:10:48] 	Iter 187100 Done. | loss1: 1.0253 | loss_class: 1.0248 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|12:11:40] 	Iter 187200 Done. | loss1: 0.5059 | loss_class: 0.5054 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:12:32] 	Iter 187300 Done. | loss1: 2.2920 | loss_class: 2.2914 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:13:24] 	Iter 187400 Done. | loss1: 0.3659 | loss_class: 0.3654 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|12:14:15] 	Iter 187500 Done. | loss1: 2.2315 | loss_class: 2.2309 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:15:07] 	Iter 187600 Done. | loss1: 1.5394 | loss_class: 1.5387 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:15:58] 	Iter 187700 Done. | loss1: 0.6736 | loss_class: 0.6729 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:16:50] 	Iter 187800 Done. | loss1: 2.6834 | loss_class: 2.6826 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|12:17:42] 	Iter 187900 Done. | loss1: 1.2932 | loss_class: 1.2927 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|12:18:34] 	Iter 188000 Done. | loss1: 3.1007 | loss_class: 3.1001 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:19:26] 	Iter 188100 Done. | loss1: 0.4491 | loss_class: 0.4485 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:20:17] 	Iter 188200 Done. | loss1: 0.6959 | loss_class: 0.6952 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:21:09] 	Iter 188300 Done. | loss1: 0.5388 | loss_class: 0.5379 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|12:22:00] 	Iter 188400 Done. | loss1: 0.1185 | loss_class: 0.1179 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:22:52] 	Iter 188500 Done. | loss1: 3.6150 | loss_class: 3.6144 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:23:44] 	Iter 188600 Done. | loss1: 2.5167 | loss_class: 2.5161 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:24:36] 	Iter 188700 Done. | loss1: 1.7888 | loss_class: 1.7883 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|12:25:28] 	Iter 188800 Done. | loss1: 0.4070 | loss_class: 0.4063 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|12:26:19] 	Iter 188900 Done. | loss1: 1.1253 | loss_class: 1.1247 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:27:11] 	Iter 189000 Done. | loss1: 0.2958 | loss_class: 0.2952 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:28:03] 	Iter 189100 Done. | loss1: 0.7652 | loss_class: 0.7645 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:28:55] 	Iter 189200 Done. | loss1: 1.6459 | loss_class: 1.6452 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:29:47] 	Iter 189300 Done. | loss1: 1.9628 | loss_class: 1.9621 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:30:39] 	Iter 189400 Done. | loss1: 1.4649 | loss_class: 1.4641 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|12:31:31] 	Iter 189500 Done. | loss1: 0.3662 | loss_class: 0.3655 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:32:23] 	Iter 189600 Done. | loss1: 0.8404 | loss_class: 0.8398 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:33:14] 	Iter 189700 Done. | loss1: 2.4491 | loss_class: 2.4485 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:34:06] 	Iter 189800 Done. | loss1: 0.8481 | loss_class: 0.8474 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|12:34:58] 	Iter 189900 Done. | loss1: 0.4559 | loss_class: 0.4553 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|12:35:50] 	Iter 190000 Done. | loss1: 1.4345 | loss_class: 1.4339 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:36:42] 	Iter 190100 Done. | loss1: 0.5295 | loss_class: 0.5286 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|12:37:34] 	Iter 190200 Done. | loss1: 1.0602 | loss_class: 1.0595 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:38:26] 	Iter 190300 Done. | loss1: 1.4921 | loss_class: 1.4915 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:39:18] 	Iter 190400 Done. | loss1: 0.9033 | loss_class: 0.9026 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:39:26] 	mean_loss1: 1.2063856076385406
[06.23.21|12:39:26] 	mean_loss_class: 1.2057373385917862
[06.23.21|12:39:26] 	mean_loss_recon: 0.0006482688228522968
[06.23.21|12:39:26] Time consumption:
[06.23.21|12:39:26] Done.
[06.23.21|12:39:27] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch28_model1.pt.
[06.23.21|12:39:27] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch28_model2.pt.
[06.23.21|12:39:27] Training epoch: 29
[06.23.21|12:40:10] 	Iter 190500 Done. | loss1: 1.3491 | loss_class: 1.3483 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|12:41:01] 	Iter 190600 Done. | loss1: 0.4361 | loss_class: 0.4354 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:41:52] 	Iter 190700 Done. | loss1: 1.5874 | loss_class: 1.5866 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|12:42:43] 	Iter 190800 Done. | loss1: 0.3708 | loss_class: 0.3702 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:43:35] 	Iter 190900 Done. | loss1: 1.1714 | loss_class: 1.1707 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:44:25] 	Iter 191000 Done. | loss1: 2.2237 | loss_class: 2.2232 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|12:45:17] 	Iter 191100 Done. | loss1: 1.0154 | loss_class: 1.0147 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:46:09] 	Iter 191200 Done. | loss1: 1.1404 | loss_class: 1.1399 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|12:47:00] 	Iter 191300 Done. | loss1: 1.5790 | loss_class: 1.5783 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:47:51] 	Iter 191400 Done. | loss1: 2.6242 | loss_class: 2.6234 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|12:48:43] 	Iter 191500 Done. | loss1: 1.6315 | loss_class: 1.6306 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|12:49:34] 	Iter 191600 Done. | loss1: 1.7696 | loss_class: 1.7692 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|12:50:27] 	Iter 191700 Done. | loss1: 1.8511 | loss_class: 1.8506 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:51:18] 	Iter 191800 Done. | loss1: 1.5842 | loss_class: 1.5835 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:52:09] 	Iter 191900 Done. | loss1: 0.7443 | loss_class: 0.7436 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:53:01] 	Iter 192000 Done. | loss1: 0.7763 | loss_class: 0.7757 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|12:53:52] 	Iter 192100 Done. | loss1: 0.6890 | loss_class: 0.6883 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:54:43] 	Iter 192200 Done. | loss1: 0.5993 | loss_class: 0.5986 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:55:35] 	Iter 192300 Done. | loss1: 1.8661 | loss_class: 1.8657 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|12:56:26] 	Iter 192400 Done. | loss1: 1.2680 | loss_class: 1.2674 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|12:57:17] 	Iter 192500 Done. | loss1: 1.4647 | loss_class: 1.4640 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|12:58:09] 	Iter 192600 Done. | loss1: 0.0629 | loss_class: 0.0619 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|12:59:00] 	Iter 192700 Done. | loss1: 2.1359 | loss_class: 2.1354 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|12:59:53] 	Iter 192800 Done. | loss1: 1.4642 | loss_class: 1.4637 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|13:00:44] 	Iter 192900 Done. | loss1: 0.8871 | loss_class: 0.8866 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:01:37] 	Iter 193000 Done. | loss1: 0.4527 | loss_class: 0.4520 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:02:28] 	Iter 193100 Done. | loss1: 2.8340 | loss_class: 2.8334 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:03:19] 	Iter 193200 Done. | loss1: 0.7183 | loss_class: 0.7176 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|13:04:11] 	Iter 193300 Done. | loss1: 2.1649 | loss_class: 2.1643 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:05:02] 	Iter 193400 Done. | loss1: 1.7079 | loss_class: 1.7073 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:05:54] 	Iter 193500 Done. | loss1: 1.2798 | loss_class: 1.2790 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|13:06:46] 	Iter 193600 Done. | loss1: 0.4447 | loss_class: 0.4441 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:07:37] 	Iter 193700 Done. | loss1: 0.5289 | loss_class: 0.5285 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:08:27] 	Iter 193800 Done. | loss1: 0.6049 | loss_class: 0.6040 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|13:09:18] 	Iter 193900 Done. | loss1: 0.4703 | loss_class: 0.4697 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:10:09] 	Iter 194000 Done. | loss1: 0.3004 | loss_class: 0.2998 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:11:00] 	Iter 194100 Done. | loss1: 2.3107 | loss_class: 2.3098 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|13:11:52] 	Iter 194200 Done. | loss1: 2.4804 | loss_class: 2.4800 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|13:12:44] 	Iter 194300 Done. | loss1: 1.1808 | loss_class: 1.1801 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:13:36] 	Iter 194400 Done. | loss1: 0.8031 | loss_class: 0.8022 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|13:14:28] 	Iter 194500 Done. | loss1: 2.2418 | loss_class: 2.2412 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:15:20] 	Iter 194600 Done. | loss1: 0.4377 | loss_class: 0.4370 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:16:11] 	Iter 194700 Done. | loss1: 2.7609 | loss_class: 2.7602 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:17:01] 	Iter 194800 Done. | loss1: 1.6310 | loss_class: 1.6300 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|13:17:51] 	Iter 194900 Done. | loss1: 0.9460 | loss_class: 0.9454 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:18:44] 	Iter 195000 Done. | loss1: 0.7950 | loss_class: 0.7943 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:19:35] 	Iter 195100 Done. | loss1: 0.8570 | loss_class: 0.8564 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:20:27] 	Iter 195200 Done. | loss1: 2.7819 | loss_class: 2.7815 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|13:21:19] 	Iter 195300 Done. | loss1: 1.0382 | loss_class: 1.0375 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:22:12] 	Iter 195400 Done. | loss1: 0.4177 | loss_class: 0.4172 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:23:04] 	Iter 195500 Done. | loss1: 1.1586 | loss_class: 1.1581 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:23:56] 	Iter 195600 Done. | loss1: 0.9406 | loss_class: 0.9400 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:24:47] 	Iter 195700 Done. | loss1: 0.5842 | loss_class: 0.5832 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|13:25:40] 	Iter 195800 Done. | loss1: 1.3209 | loss_class: 1.3203 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:26:31] 	Iter 195900 Done. | loss1: 0.3321 | loss_class: 0.3313 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|13:27:22] 	Iter 196000 Done. | loss1: 2.1774 | loss_class: 2.1769 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:28:13] 	Iter 196100 Done. | loss1: 0.6082 | loss_class: 0.6072 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|13:29:04] 	Iter 196200 Done. | loss1: 0.6734 | loss_class: 0.6728 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:29:54] 	Iter 196300 Done. | loss1: 0.0969 | loss_class: 0.0962 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:30:44] 	Iter 196400 Done. | loss1: 0.1757 | loss_class: 0.1751 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:31:35] 	Iter 196500 Done. | loss1: 1.7418 | loss_class: 1.7413 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:32:25] 	Iter 196600 Done. | loss1: 1.1988 | loss_class: 1.1983 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:33:15] 	Iter 196700 Done. | loss1: 1.7643 | loss_class: 1.7637 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:34:06] 	Iter 196800 Done. | loss1: 0.1600 | loss_class: 0.1593 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:34:56] 	Iter 196900 Done. | loss1: 1.1585 | loss_class: 1.1580 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:35:46] 	Iter 197000 Done. | loss1: 0.4192 | loss_class: 0.4187 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:36:37] 	Iter 197100 Done. | loss1: 1.6936 | loss_class: 1.6930 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:37:27] 	Iter 197200 Done. | loss1: 1.8538 | loss_class: 1.8533 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:38:17] 	Iter 197300 Done. | loss1: 0.7969 | loss_class: 0.7963 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:39:08] 	Iter 197400 Done. | loss1: 0.9746 | loss_class: 0.9740 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:39:58] 	Iter 197500 Done. | loss1: 0.5836 | loss_class: 0.5829 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:40:48] 	Iter 197600 Done. | loss1: 0.1900 | loss_class: 0.1893 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:41:38] 	Iter 197700 Done. | loss1: 0.7441 | loss_class: 0.7435 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:42:29] 	Iter 197800 Done. | loss1: 0.8174 | loss_class: 0.8165 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|13:43:19] 	Iter 197900 Done. | loss1: 2.1673 | loss_class: 2.1665 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:44:09] 	Iter 198000 Done. | loss1: 1.5335 | loss_class: 1.5330 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:45:00] 	Iter 198100 Done. | loss1: 1.3620 | loss_class: 1.3616 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|13:45:50] 	Iter 198200 Done. | loss1: 1.8740 | loss_class: 1.8736 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|13:46:41] 	Iter 198300 Done. | loss1: 0.7297 | loss_class: 0.7291 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:47:31] 	Iter 198400 Done. | loss1: 1.1648 | loss_class: 1.1641 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:48:21] 	Iter 198500 Done. | loss1: 2.0452 | loss_class: 2.0446 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:49:11] 	Iter 198600 Done. | loss1: 1.1977 | loss_class: 1.1971 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:50:02] 	Iter 198700 Done. | loss1: 0.7956 | loss_class: 0.7950 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:50:52] 	Iter 198800 Done. | loss1: 1.4813 | loss_class: 1.4806 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:51:42] 	Iter 198900 Done. | loss1: 1.0401 | loss_class: 1.0396 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|13:52:33] 	Iter 199000 Done. | loss1: 1.2568 | loss_class: 1.2563 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:53:24] 	Iter 199100 Done. | loss1: 0.3502 | loss_class: 0.3495 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:54:15] 	Iter 199200 Done. | loss1: 1.3017 | loss_class: 1.3010 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:55:06] 	Iter 199300 Done. | loss1: 0.2021 | loss_class: 0.2014 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:55:57] 	Iter 199400 Done. | loss1: 1.6000 | loss_class: 1.5993 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:56:47] 	Iter 199500 Done. | loss1: 0.6799 | loss_class: 0.6794 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:57:38] 	Iter 199600 Done. | loss1: 0.7785 | loss_class: 0.7778 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|13:58:28] 	Iter 199700 Done. | loss1: 0.4212 | loss_class: 0.4207 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|13:59:19] 	Iter 199800 Done. | loss1: 0.6212 | loss_class: 0.6208 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:00:09] 	Iter 199900 Done. | loss1: 0.6052 | loss_class: 0.6046 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:01:00] 	Iter 200000 Done. | loss1: 0.2853 | loss_class: 0.2846 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:01:50] 	Iter 200100 Done. | loss1: 0.3809 | loss_class: 0.3801 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:02:41] 	Iter 200200 Done. | loss1: 2.5137 | loss_class: 2.5132 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:03:32] 	Iter 200300 Done. | loss1: 0.9201 | loss_class: 0.9194 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:04:22] 	Iter 200400 Done. | loss1: 1.2113 | loss_class: 1.2106 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:04:42] 	mean_loss1: 1.2036467539726412
[06.23.21|14:04:42] 	mean_loss_class: 1.202999315630218
[06.23.21|14:04:42] 	mean_loss_recon: 0.0006474384603134631
[06.23.21|14:04:42] Time consumption:
[06.23.21|14:04:42] Done.
[06.23.21|14:04:42] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch29_model1.pt.
[06.23.21|14:04:42] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch29_model2.pt.
[06.23.21|14:04:42] Eval epoch: 29
[06.23.21|14:11:11] 	mean_loss1: 1.307129144899605
[06.23.21|14:11:11] 	mean_loss_class: 1.3067566270167514
[06.23.21|14:11:11] 	mean_loss_recon: 0.03725177795416856
[06.23.21|14:11:11] 

[06.23.21|14:11:11] 	Top1: 63.90%
[06.23.21|14:11:11] 

[06.23.21|14:11:11] 	Top5: 90.22%
[06.23.21|14:11:11] Done.
[06.23.21|14:11:11] Training epoch: 30
[06.23.21|14:11:42] 	Iter 200500 Done. | loss1: 1.7020 | loss_class: 1.7015 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:12:33] 	Iter 200600 Done. | loss1: 0.7733 | loss_class: 0.7728 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:13:23] 	Iter 200700 Done. | loss1: 0.2468 | loss_class: 0.2462 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:14:13] 	Iter 200800 Done. | loss1: 0.8391 | loss_class: 0.8385 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:15:04] 	Iter 200900 Done. | loss1: 0.6377 | loss_class: 0.6370 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:15:54] 	Iter 201000 Done. | loss1: 0.2499 | loss_class: 0.2493 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:16:44] 	Iter 201100 Done. | loss1: 1.5705 | loss_class: 1.5699 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:17:35] 	Iter 201200 Done. | loss1: 1.1466 | loss_class: 1.1461 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:18:25] 	Iter 201300 Done. | loss1: 0.9324 | loss_class: 0.9318 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:19:16] 	Iter 201400 Done. | loss1: 0.1186 | loss_class: 0.1178 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|14:20:06] 	Iter 201500 Done. | loss1: 0.6396 | loss_class: 0.6389 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|14:20:56] 	Iter 201600 Done. | loss1: 0.8319 | loss_class: 0.8312 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:21:47] 	Iter 201700 Done. | loss1: 0.8736 | loss_class: 0.8730 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:22:38] 	Iter 201800 Done. | loss1: 1.1723 | loss_class: 1.1716 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:23:29] 	Iter 201900 Done. | loss1: 1.5116 | loss_class: 1.5110 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:24:19] 	Iter 202000 Done. | loss1: 1.9417 | loss_class: 1.9411 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:25:09] 	Iter 202100 Done. | loss1: 0.1077 | loss_class: 0.1069 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|14:25:59] 	Iter 202200 Done. | loss1: 1.6387 | loss_class: 1.6379 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|14:26:49] 	Iter 202300 Done. | loss1: 1.0405 | loss_class: 1.0399 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:27:39] 	Iter 202400 Done. | loss1: 0.7375 | loss_class: 0.7369 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:28:29] 	Iter 202500 Done. | loss1: 0.5576 | loss_class: 0.5569 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:29:21] 	Iter 202600 Done. | loss1: 0.3709 | loss_class: 0.3703 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:30:13] 	Iter 202700 Done. | loss1: 0.3168 | loss_class: 0.3161 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:31:04] 	Iter 202800 Done. | loss1: 1.1431 | loss_class: 1.1425 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:31:55] 	Iter 202900 Done. | loss1: 0.2889 | loss_class: 0.2884 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:32:46] 	Iter 203000 Done. | loss1: 1.1406 | loss_class: 1.1400 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:33:36] 	Iter 203100 Done. | loss1: 0.6715 | loss_class: 0.6708 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:34:29] 	Iter 203200 Done. | loss1: 1.1440 | loss_class: 1.1435 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:35:20] 	Iter 203300 Done. | loss1: 1.8412 | loss_class: 1.8407 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:36:10] 	Iter 203400 Done. | loss1: 1.1496 | loss_class: 1.1490 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:36:59] 	Iter 203500 Done. | loss1: 0.8946 | loss_class: 0.8940 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:37:49] 	Iter 203600 Done. | loss1: 1.3621 | loss_class: 1.3613 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|14:38:39] 	Iter 203700 Done. | loss1: 1.8500 | loss_class: 1.8495 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:39:34] 	Iter 203800 Done. | loss1: 1.2579 | loss_class: 1.2573 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:40:30] 	Iter 203900 Done. | loss1: 0.8969 | loss_class: 0.8963 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:41:23] 	Iter 204000 Done. | loss1: 1.5882 | loss_class: 1.5873 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|14:42:13] 	Iter 204100 Done. | loss1: 1.6410 | loss_class: 1.6402 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|14:43:03] 	Iter 204200 Done. | loss1: 1.3941 | loss_class: 1.3934 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:43:53] 	Iter 204300 Done. | loss1: 0.3021 | loss_class: 0.3016 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:44:43] 	Iter 204400 Done. | loss1: 0.4592 | loss_class: 0.4584 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|14:45:33] 	Iter 204500 Done. | loss1: 1.2840 | loss_class: 1.2835 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:46:23] 	Iter 204600 Done. | loss1: 2.3395 | loss_class: 2.3389 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:47:13] 	Iter 204700 Done. | loss1: 1.3381 | loss_class: 1.3376 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:48:03] 	Iter 204800 Done. | loss1: 1.0671 | loss_class: 1.0664 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:48:53] 	Iter 204900 Done. | loss1: 0.6930 | loss_class: 0.6923 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:49:43] 	Iter 205000 Done. | loss1: 1.7227 | loss_class: 1.7219 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|14:50:33] 	Iter 205100 Done. | loss1: 1.3697 | loss_class: 1.3692 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|14:51:23] 	Iter 205200 Done. | loss1: 0.3330 | loss_class: 0.3324 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:52:13] 	Iter 205300 Done. | loss1: 1.1993 | loss_class: 1.1988 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:53:06] 	Iter 205400 Done. | loss1: 2.1255 | loss_class: 2.1246 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|14:53:57] 	Iter 205500 Done. | loss1: 1.5873 | loss_class: 1.5866 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:54:48] 	Iter 205600 Done. | loss1: 2.1332 | loss_class: 2.1325 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|14:55:38] 	Iter 205700 Done. | loss1: 2.6831 | loss_class: 2.6825 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:56:29] 	Iter 205800 Done. | loss1: 2.0039 | loss_class: 2.0033 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:57:19] 	Iter 205900 Done. | loss1: 2.0632 | loss_class: 2.0626 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:58:09] 	Iter 206000 Done. | loss1: 1.4376 | loss_class: 1.4370 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:59:00] 	Iter 206100 Done. | loss1: 2.0377 | loss_class: 2.0370 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|14:59:50] 	Iter 206200 Done. | loss1: 0.4517 | loss_class: 0.4511 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:00:40] 	Iter 206300 Done. | loss1: 1.9949 | loss_class: 1.9942 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:01:30] 	Iter 206400 Done. | loss1: 0.4115 | loss_class: 0.4108 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:02:23] 	Iter 206500 Done. | loss1: 1.2187 | loss_class: 1.2180 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:03:18] 	Iter 206600 Done. | loss1: 0.7834 | loss_class: 0.7828 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:04:08] 	Iter 206700 Done. | loss1: 1.5834 | loss_class: 1.5828 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:04:59] 	Iter 206800 Done. | loss1: 0.8488 | loss_class: 0.8483 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|15:05:49] 	Iter 206900 Done. | loss1: 2.2517 | loss_class: 2.2510 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:06:39] 	Iter 207000 Done. | loss1: 1.7256 | loss_class: 1.7250 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:07:30] 	Iter 207100 Done. | loss1: 0.4964 | loss_class: 0.4959 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|15:08:22] 	Iter 207200 Done. | loss1: 1.0911 | loss_class: 1.0904 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:09:13] 	Iter 207300 Done. | loss1: 1.9914 | loss_class: 1.9909 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|15:10:05] 	Iter 207400 Done. | loss1: 0.7732 | loss_class: 0.7726 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:10:57] 	Iter 207500 Done. | loss1: 0.5170 | loss_class: 0.5166 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|15:11:48] 	Iter 207600 Done. | loss1: 0.3423 | loss_class: 0.3417 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|15:12:39] 	Iter 207700 Done. | loss1: 0.8267 | loss_class: 0.8261 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:13:29] 	Iter 207800 Done. | loss1: 0.8620 | loss_class: 0.8614 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:14:20] 	Iter 207900 Done. | loss1: 1.4886 | loss_class: 1.4881 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|15:15:10] 	Iter 208000 Done. | loss1: 1.5676 | loss_class: 1.5667 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|15:16:00] 	Iter 208100 Done. | loss1: 0.7438 | loss_class: 0.7433 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|15:16:50] 	Iter 208200 Done. | loss1: 1.3020 | loss_class: 1.3012 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:17:41] 	Iter 208300 Done. | loss1: 0.7987 | loss_class: 0.7981 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:18:32] 	Iter 208400 Done. | loss1: 0.2729 | loss_class: 0.2725 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|15:19:22] 	Iter 208500 Done. | loss1: 0.8446 | loss_class: 0.8439 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|15:20:12] 	Iter 208600 Done. | loss1: 1.0246 | loss_class: 1.0238 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|15:21:02] 	Iter 208700 Done. | loss1: 0.1761 | loss_class: 0.1756 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:21:52] 	Iter 208800 Done. | loss1: 2.2454 | loss_class: 2.2446 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|15:22:43] 	Iter 208900 Done. | loss1: 1.0038 | loss_class: 1.0033 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:23:33] 	Iter 209000 Done. | loss1: 1.0835 | loss_class: 1.0830 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|15:24:23] 	Iter 209100 Done. | loss1: 1.4776 | loss_class: 1.4771 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|15:25:14] 	Iter 209200 Done. | loss1: 0.9902 | loss_class: 0.9895 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:26:04] 	Iter 209300 Done. | loss1: 1.7324 | loss_class: 1.7319 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:26:54] 	Iter 209400 Done. | loss1: 1.0010 | loss_class: 1.0004 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:27:44] 	Iter 209500 Done. | loss1: 0.2859 | loss_class: 0.2851 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|15:28:36] 	Iter 209600 Done. | loss1: 1.8340 | loss_class: 1.8332 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|15:29:27] 	Iter 209700 Done. | loss1: 1.7791 | loss_class: 1.7784 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:30:18] 	Iter 209800 Done. | loss1: 1.5645 | loss_class: 1.5638 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|15:31:08] 	Iter 209900 Done. | loss1: 0.0454 | loss_class: 0.0449 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|15:31:58] 	Iter 210000 Done. | loss1: 0.3125 | loss_class: 0.3118 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:32:49] 	Iter 210100 Done. | loss1: 0.9900 | loss_class: 0.9893 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:33:39] 	Iter 210200 Done. | loss1: 1.3922 | loss_class: 1.3917 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|15:34:30] 	Iter 210300 Done. | loss1: 0.7118 | loss_class: 0.7112 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:35:20] 	Iter 210400 Done. | loss1: 0.9316 | loss_class: 0.9307 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|15:35:51] 	mean_loss1: 1.1838652753835048
[06.23.21|15:35:51] 	mean_loss_class: 1.183216404921491
[06.23.21|15:35:51] 	mean_loss_recon: 0.0006488703290843227
[06.23.21|15:35:51] Time consumption:
[06.23.21|15:35:51] Done.
[06.23.21|15:35:51] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch30_model1.pt.
[06.23.21|15:35:51] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch30_model2.pt.
[06.23.21|15:35:51] Training epoch: 31
[06.23.21|15:36:12] 	Iter 210500 Done. | loss1: 0.3415 | loss_class: 0.3408 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:37:04] 	Iter 210600 Done. | loss1: 3.6557 | loss_class: 3.6548 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|15:37:56] 	Iter 210700 Done. | loss1: 1.1920 | loss_class: 1.1912 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|15:38:46] 	Iter 210800 Done. | loss1: 1.7798 | loss_class: 1.7791 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:39:38] 	Iter 210900 Done. | loss1: 1.7561 | loss_class: 1.7556 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:40:31] 	Iter 211000 Done. | loss1: 2.1765 | loss_class: 2.1759 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:41:23] 	Iter 211100 Done. | loss1: 1.3378 | loss_class: 1.3373 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:42:14] 	Iter 211200 Done. | loss1: 1.4022 | loss_class: 1.4016 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|15:43:05] 	Iter 211300 Done. | loss1: 2.1509 | loss_class: 2.1505 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|15:44:00] 	Iter 211400 Done. | loss1: 1.4193 | loss_class: 1.4186 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:44:52] 	Iter 211500 Done. | loss1: 0.1359 | loss_class: 0.1350 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|15:45:42] 	Iter 211600 Done. | loss1: 1.3114 | loss_class: 1.3106 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|15:46:33] 	Iter 211700 Done. | loss1: 1.1505 | loss_class: 1.1500 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:47:24] 	Iter 211800 Done. | loss1: 0.6304 | loss_class: 0.6295 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|15:48:17] 	Iter 211900 Done. | loss1: 0.9918 | loss_class: 0.9911 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:49:09] 	Iter 212000 Done. | loss1: 1.6323 | loss_class: 1.6316 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|15:50:00] 	Iter 212100 Done. | loss1: 1.2627 | loss_class: 1.2620 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:50:52] 	Iter 212200 Done. | loss1: 0.4987 | loss_class: 0.4979 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|15:51:44] 	Iter 212300 Done. | loss1: 1.0110 | loss_class: 1.0103 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:52:35] 	Iter 212400 Done. | loss1: 0.8429 | loss_class: 0.8424 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:53:27] 	Iter 212500 Done. | loss1: 0.8963 | loss_class: 0.8953 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|15:54:18] 	Iter 212600 Done. | loss1: 0.3993 | loss_class: 0.3987 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:55:10] 	Iter 212700 Done. | loss1: 0.6893 | loss_class: 0.6885 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|15:56:02] 	Iter 212800 Done. | loss1: 0.8380 | loss_class: 0.8374 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|15:56:54] 	Iter 212900 Done. | loss1: 0.6208 | loss_class: 0.6202 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|15:57:46] 	Iter 213000 Done. | loss1: 0.4676 | loss_class: 0.4667 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|15:58:38] 	Iter 213100 Done. | loss1: 1.3681 | loss_class: 1.3673 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|15:59:30] 	Iter 213200 Done. | loss1: 0.7277 | loss_class: 0.7272 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:00:22] 	Iter 213300 Done. | loss1: 0.6874 | loss_class: 0.6869 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:01:13] 	Iter 213400 Done. | loss1: 1.0963 | loss_class: 1.0956 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:02:05] 	Iter 213500 Done. | loss1: 1.0348 | loss_class: 1.0341 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:02:57] 	Iter 213600 Done. | loss1: 1.8346 | loss_class: 1.8335 | loss_recon: 0.0012 | lr: 0.100000
[06.23.21|16:03:48] 	Iter 213700 Done. | loss1: 0.7395 | loss_class: 0.7379 | loss_recon: 0.0016 | lr: 0.100000
[06.23.21|16:04:40] 	Iter 213800 Done. | loss1: 0.3593 | loss_class: 0.3586 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:05:31] 	Iter 213900 Done. | loss1: 0.5374 | loss_class: 0.5367 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:06:23] 	Iter 214000 Done. | loss1: 1.8066 | loss_class: 1.8062 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|16:07:14] 	Iter 214100 Done. | loss1: 1.0853 | loss_class: 1.0846 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:08:06] 	Iter 214200 Done. | loss1: 1.6288 | loss_class: 1.6282 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:08:58] 	Iter 214300 Done. | loss1: 3.3936 | loss_class: 3.3927 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|16:09:49] 	Iter 214400 Done. | loss1: 0.7622 | loss_class: 0.7615 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:10:41] 	Iter 214500 Done. | loss1: 0.4401 | loss_class: 0.4396 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:11:32] 	Iter 214600 Done. | loss1: 0.3438 | loss_class: 0.3433 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:12:24] 	Iter 214700 Done. | loss1: 3.7429 | loss_class: 3.7422 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:13:15] 	Iter 214800 Done. | loss1: 1.3023 | loss_class: 1.3017 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:14:07] 	Iter 214900 Done. | loss1: 1.0727 | loss_class: 1.0718 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|16:14:58] 	Iter 215000 Done. | loss1: 1.8361 | loss_class: 1.8353 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|16:15:49] 	Iter 215100 Done. | loss1: 1.1682 | loss_class: 1.1675 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|16:16:41] 	Iter 215200 Done. | loss1: 1.3464 | loss_class: 1.3459 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:17:33] 	Iter 215300 Done. | loss1: 0.9622 | loss_class: 0.9617 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:18:24] 	Iter 215400 Done. | loss1: 0.8957 | loss_class: 0.8951 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:19:16] 	Iter 215500 Done. | loss1: 0.6031 | loss_class: 0.6026 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:20:07] 	Iter 215600 Done. | loss1: 1.5900 | loss_class: 1.5894 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:20:59] 	Iter 215700 Done. | loss1: 0.1461 | loss_class: 0.1454 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:21:50] 	Iter 215800 Done. | loss1: 1.2773 | loss_class: 1.2767 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:22:42] 	Iter 215900 Done. | loss1: 0.8440 | loss_class: 0.8434 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:23:34] 	Iter 216000 Done. | loss1: 0.5083 | loss_class: 0.5077 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:24:25] 	Iter 216100 Done. | loss1: 2.1593 | loss_class: 2.1587 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:25:17] 	Iter 216200 Done. | loss1: 1.6465 | loss_class: 1.6457 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:26:08] 	Iter 216300 Done. | loss1: 0.7153 | loss_class: 0.7144 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|16:26:59] 	Iter 216400 Done. | loss1: 1.0989 | loss_class: 1.0982 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:27:51] 	Iter 216500 Done. | loss1: 0.3625 | loss_class: 0.3618 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:28:42] 	Iter 216600 Done. | loss1: 1.5617 | loss_class: 1.5611 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:29:33] 	Iter 216700 Done. | loss1: 1.4827 | loss_class: 1.4819 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|16:30:24] 	Iter 216800 Done. | loss1: 1.6273 | loss_class: 1.6267 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:31:15] 	Iter 216900 Done. | loss1: 0.8224 | loss_class: 0.8216 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:32:07] 	Iter 217000 Done. | loss1: 1.2243 | loss_class: 1.2236 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:32:58] 	Iter 217100 Done. | loss1: 1.4506 | loss_class: 1.4500 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:33:50] 	Iter 217200 Done. | loss1: 0.9933 | loss_class: 0.9928 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:34:41] 	Iter 217300 Done. | loss1: 0.3783 | loss_class: 0.3776 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:35:32] 	Iter 217400 Done. | loss1: 1.2815 | loss_class: 1.2810 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:36:23] 	Iter 217500 Done. | loss1: 0.1607 | loss_class: 0.1596 | loss_recon: 0.0011 | lr: 0.100000
[06.23.21|16:37:14] 	Iter 217600 Done. | loss1: 1.5500 | loss_class: 1.5494 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:38:06] 	Iter 217700 Done. | loss1: 0.8400 | loss_class: 0.8394 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:38:57] 	Iter 217800 Done. | loss1: 0.1806 | loss_class: 0.1800 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:39:49] 	Iter 217900 Done. | loss1: 1.7256 | loss_class: 1.7249 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:40:40] 	Iter 218000 Done. | loss1: 1.8606 | loss_class: 1.8600 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:41:31] 	Iter 218100 Done. | loss1: 0.9062 | loss_class: 0.9056 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:42:22] 	Iter 218200 Done. | loss1: 1.1450 | loss_class: 1.1443 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:43:13] 	Iter 218300 Done. | loss1: 2.9297 | loss_class: 2.9291 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:44:05] 	Iter 218400 Done. | loss1: 0.7957 | loss_class: 0.7950 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:44:56] 	Iter 218500 Done. | loss1: 1.0689 | loss_class: 1.0682 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:45:46] 	Iter 218600 Done. | loss1: 0.3439 | loss_class: 0.3433 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:46:37] 	Iter 218700 Done. | loss1: 0.7903 | loss_class: 0.7895 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|16:47:29] 	Iter 218800 Done. | loss1: 1.8652 | loss_class: 1.8647 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:48:20] 	Iter 218900 Done. | loss1: 1.8448 | loss_class: 1.8443 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:49:11] 	Iter 219000 Done. | loss1: 1.3739 | loss_class: 1.3733 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:50:02] 	Iter 219100 Done. | loss1: 0.1894 | loss_class: 0.1888 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:50:54] 	Iter 219200 Done. | loss1: 0.4069 | loss_class: 0.4063 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:51:45] 	Iter 219300 Done. | loss1: 1.1643 | loss_class: 1.1638 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:52:36] 	Iter 219400 Done. | loss1: 1.2098 | loss_class: 1.2091 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|16:53:28] 	Iter 219500 Done. | loss1: 1.0374 | loss_class: 1.0369 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:54:19] 	Iter 219600 Done. | loss1: 0.9667 | loss_class: 0.9663 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|16:55:10] 	Iter 219700 Done. | loss1: 1.7632 | loss_class: 1.7623 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|16:56:01] 	Iter 219800 Done. | loss1: 1.9170 | loss_class: 1.9164 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|16:56:53] 	Iter 219900 Done. | loss1: 0.1710 | loss_class: 0.1700 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|16:57:44] 	Iter 220000 Done. | loss1: 1.6990 | loss_class: 1.6982 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|16:58:36] 	Iter 220100 Done. | loss1: 1.4807 | loss_class: 1.4801 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|16:59:27] 	Iter 220200 Done. | loss1: 0.3031 | loss_class: 0.3022 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|17:00:18] 	Iter 220300 Done. | loss1: 0.1709 | loss_class: 0.1700 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|17:01:10] 	Iter 220400 Done. | loss1: 0.4638 | loss_class: 0.4632 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|17:01:52] 	mean_loss1: 1.190942364777088
[06.23.21|17:01:52] 	mean_loss_class: 1.1902962064559528
[06.23.21|17:01:52] 	mean_loss_recon: 0.0006461579029674516
[06.23.21|17:01:52] Time consumption:
[06.23.21|17:01:52] Done.
[06.23.21|17:01:53] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch31_model1.pt.
[06.23.21|17:01:53] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch31_model2.pt.
[06.23.21|17:01:53] Training epoch: 32
[06.23.21|17:02:02] 	Iter 220500 Done. | loss1: 1.4855 | loss_class: 1.4847 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:02:53] 	Iter 220600 Done. | loss1: 1.0925 | loss_class: 1.0918 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:03:44] 	Iter 220700 Done. | loss1: 0.8243 | loss_class: 0.8236 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:04:36] 	Iter 220800 Done. | loss1: 1.1811 | loss_class: 1.1804 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:05:28] 	Iter 220900 Done. | loss1: 1.4973 | loss_class: 1.4965 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|17:06:19] 	Iter 221000 Done. | loss1: 0.9743 | loss_class: 0.9736 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:07:10] 	Iter 221100 Done. | loss1: 0.5390 | loss_class: 0.5383 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:08:01] 	Iter 221200 Done. | loss1: 0.5101 | loss_class: 0.5094 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:08:52] 	Iter 221300 Done. | loss1: 0.7795 | loss_class: 0.7787 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|17:09:44] 	Iter 221400 Done. | loss1: 1.8993 | loss_class: 1.8987 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:10:35] 	Iter 221500 Done. | loss1: 1.4524 | loss_class: 1.4518 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:11:26] 	Iter 221600 Done. | loss1: 2.4343 | loss_class: 2.4338 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:12:17] 	Iter 221700 Done. | loss1: 0.4362 | loss_class: 0.4357 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|17:13:08] 	Iter 221800 Done. | loss1: 0.5662 | loss_class: 0.5656 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:13:59] 	Iter 221900 Done. | loss1: 0.7889 | loss_class: 0.7881 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|17:14:51] 	Iter 222000 Done. | loss1: 1.3815 | loss_class: 1.3810 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|17:15:42] 	Iter 222100 Done. | loss1: 1.2535 | loss_class: 1.2529 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|17:16:33] 	Iter 222200 Done. | loss1: 1.9969 | loss_class: 1.9963 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:17:25] 	Iter 222300 Done. | loss1: 2.6086 | loss_class: 2.6081 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|17:18:16] 	Iter 222400 Done. | loss1: 0.8451 | loss_class: 0.8445 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:19:07] 	Iter 222500 Done. | loss1: 1.2244 | loss_class: 1.2237 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:19:59] 	Iter 222600 Done. | loss1: 2.0029 | loss_class: 2.0024 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|17:20:50] 	Iter 222700 Done. | loss1: 1.1926 | loss_class: 1.1919 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:21:42] 	Iter 222800 Done. | loss1: 1.3891 | loss_class: 1.3885 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:22:33] 	Iter 222900 Done. | loss1: 0.9812 | loss_class: 0.9807 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|17:23:25] 	Iter 223000 Done. | loss1: 0.1572 | loss_class: 0.1566 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|17:24:16] 	Iter 223100 Done. | loss1: 0.6738 | loss_class: 0.6732 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:25:07] 	Iter 223200 Done. | loss1: 0.7334 | loss_class: 0.7328 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:25:58] 	Iter 223300 Done. | loss1: 0.6243 | loss_class: 0.6236 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:26:49] 	Iter 223400 Done. | loss1: 2.4254 | loss_class: 2.4247 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:27:41] 	Iter 223500 Done. | loss1: 1.6627 | loss_class: 1.6620 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:28:32] 	Iter 223600 Done. | loss1: 1.7315 | loss_class: 1.7309 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:29:24] 	Iter 223700 Done. | loss1: 0.7134 | loss_class: 0.7130 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|17:30:16] 	Iter 223800 Done. | loss1: 0.1997 | loss_class: 0.1991 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:31:07] 	Iter 223900 Done. | loss1: 0.9060 | loss_class: 0.9052 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|17:31:59] 	Iter 224000 Done. | loss1: 0.0711 | loss_class: 0.0705 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:32:51] 	Iter 224100 Done. | loss1: 1.2833 | loss_class: 1.2827 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:33:43] 	Iter 224200 Done. | loss1: 0.9501 | loss_class: 0.9494 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:34:34] 	Iter 224300 Done. | loss1: 0.7803 | loss_class: 0.7794 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|17:35:26] 	Iter 224400 Done. | loss1: 0.4508 | loss_class: 0.4497 | loss_recon: 0.0011 | lr: 0.100000
[06.23.21|17:36:17] 	Iter 224500 Done. | loss1: 0.7713 | loss_class: 0.7705 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|17:37:09] 	Iter 224600 Done. | loss1: 1.1097 | loss_class: 1.1091 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|17:38:00] 	Iter 224700 Done. | loss1: 1.6596 | loss_class: 1.6590 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:38:51] 	Iter 224800 Done. | loss1: 0.2598 | loss_class: 0.2592 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:39:43] 	Iter 224900 Done. | loss1: 2.1745 | loss_class: 2.1738 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:40:34] 	Iter 225000 Done. | loss1: 0.3835 | loss_class: 0.3829 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:41:26] 	Iter 225100 Done. | loss1: 1.0521 | loss_class: 1.0513 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|17:42:17] 	Iter 225200 Done. | loss1: 1.2116 | loss_class: 1.2110 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:43:09] 	Iter 225300 Done. | loss1: 1.2640 | loss_class: 1.2631 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|17:44:00] 	Iter 225400 Done. | loss1: 1.6072 | loss_class: 1.6066 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:44:51] 	Iter 225500 Done. | loss1: 0.7290 | loss_class: 0.7283 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:45:43] 	Iter 225600 Done. | loss1: 0.7987 | loss_class: 0.7979 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|17:46:35] 	Iter 225700 Done. | loss1: 0.5732 | loss_class: 0.5728 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|17:47:26] 	Iter 225800 Done. | loss1: 0.7437 | loss_class: 0.7431 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:48:17] 	Iter 225900 Done. | loss1: 0.2207 | loss_class: 0.2201 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:49:09] 	Iter 226000 Done. | loss1: 0.0459 | loss_class: 0.0453 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:50:00] 	Iter 226100 Done. | loss1: 0.7551 | loss_class: 0.7543 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|17:50:52] 	Iter 226200 Done. | loss1: 2.3784 | loss_class: 2.3777 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:51:43] 	Iter 226300 Done. | loss1: 1.9606 | loss_class: 1.9599 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:52:34] 	Iter 226400 Done. | loss1: 0.9998 | loss_class: 0.9990 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|17:53:26] 	Iter 226500 Done. | loss1: 0.8951 | loss_class: 0.8943 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:54:17] 	Iter 226600 Done. | loss1: 0.8639 | loss_class: 0.8632 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|17:55:08] 	Iter 226700 Done. | loss1: 0.5704 | loss_class: 0.5696 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|17:56:00] 	Iter 226800 Done. | loss1: 0.6563 | loss_class: 0.6558 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:56:51] 	Iter 226900 Done. | loss1: 2.7707 | loss_class: 2.7700 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:57:43] 	Iter 227000 Done. | loss1: 0.8394 | loss_class: 0.8387 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|17:58:34] 	Iter 227100 Done. | loss1: 0.4721 | loss_class: 0.4714 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|17:59:26] 	Iter 227200 Done. | loss1: 0.9245 | loss_class: 0.9239 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:00:17] 	Iter 227300 Done. | loss1: 1.5583 | loss_class: 1.5578 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:01:09] 	Iter 227400 Done. | loss1: 2.5595 | loss_class: 2.5589 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:02:00] 	Iter 227500 Done. | loss1: 0.7470 | loss_class: 0.7465 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|18:02:52] 	Iter 227600 Done. | loss1: 0.3917 | loss_class: 0.3911 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:03:44] 	Iter 227700 Done. | loss1: 2.4768 | loss_class: 2.4762 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:04:35] 	Iter 227800 Done. | loss1: 0.1725 | loss_class: 0.1714 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|18:05:26] 	Iter 227900 Done. | loss1: 1.8229 | loss_class: 1.8222 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:06:18] 	Iter 228000 Done. | loss1: 1.1827 | loss_class: 1.1821 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|18:07:09] 	Iter 228100 Done. | loss1: 2.0098 | loss_class: 2.0091 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:08:00] 	Iter 228200 Done. | loss1: 1.5508 | loss_class: 1.5503 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|18:08:52] 	Iter 228300 Done. | loss1: 0.3441 | loss_class: 0.3437 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|18:09:43] 	Iter 228400 Done. | loss1: 0.5152 | loss_class: 0.5146 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:10:35] 	Iter 228500 Done. | loss1: 0.7101 | loss_class: 0.7096 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|18:11:26] 	Iter 228600 Done. | loss1: 3.4103 | loss_class: 3.4098 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:12:18] 	Iter 228700 Done. | loss1: 1.6942 | loss_class: 1.6938 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|18:13:09] 	Iter 228800 Done. | loss1: 0.6682 | loss_class: 0.6675 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:14:01] 	Iter 228900 Done. | loss1: 1.4704 | loss_class: 1.4699 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|18:14:52] 	Iter 229000 Done. | loss1: 0.8347 | loss_class: 0.8341 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:15:43] 	Iter 229100 Done. | loss1: 2.5632 | loss_class: 2.5626 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:16:35] 	Iter 229200 Done. | loss1: 0.9596 | loss_class: 0.9589 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:17:27] 	Iter 229300 Done. | loss1: 1.3935 | loss_class: 1.3929 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:18:18] 	Iter 229400 Done. | loss1: 1.2865 | loss_class: 1.2859 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:19:09] 	Iter 229500 Done. | loss1: 1.8697 | loss_class: 1.8693 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|18:20:01] 	Iter 229600 Done. | loss1: 0.6318 | loss_class: 0.6310 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:20:52] 	Iter 229700 Done. | loss1: 1.5602 | loss_class: 1.5596 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:21:44] 	Iter 229800 Done. | loss1: 1.2039 | loss_class: 1.2031 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|18:22:35] 	Iter 229900 Done. | loss1: 0.4907 | loss_class: 0.4900 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:23:27] 	Iter 230000 Done. | loss1: 0.0588 | loss_class: 0.0581 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:24:19] 	Iter 230100 Done. | loss1: 0.9913 | loss_class: 0.9907 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:25:10] 	Iter 230200 Done. | loss1: 2.2462 | loss_class: 2.2454 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|18:26:01] 	Iter 230300 Done. | loss1: 1.4240 | loss_class: 1.4233 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:26:52] 	Iter 230400 Done. | loss1: 0.9450 | loss_class: 0.9443 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:27:45] 	Iter 230500 Done. | loss1: 1.6619 | loss_class: 1.6612 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:27:47] 	mean_loss1: 1.1769523952730414
[06.23.21|18:27:47] 	mean_loss_class: 1.176303066862749
[06.23.21|18:27:47] 	mean_loss_recon: 0.000649328273997035
[06.23.21|18:27:47] Time consumption:
[06.23.21|18:27:47] Done.
[06.23.21|18:27:47] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch32_model1.pt.
[06.23.21|18:27:48] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch32_model2.pt.
[06.23.21|18:27:48] Training epoch: 33
[06.23.21|18:28:36] 	Iter 230600 Done. | loss1: 0.7036 | loss_class: 0.7028 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|18:29:27] 	Iter 230700 Done. | loss1: 2.0165 | loss_class: 2.0160 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|18:30:18] 	Iter 230800 Done. | loss1: 0.6752 | loss_class: 0.6746 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:31:09] 	Iter 230900 Done. | loss1: 1.1933 | loss_class: 1.1926 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:32:01] 	Iter 231000 Done. | loss1: 0.1226 | loss_class: 0.1218 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|18:32:52] 	Iter 231100 Done. | loss1: 0.9882 | loss_class: 0.9876 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:33:43] 	Iter 231200 Done. | loss1: 1.0179 | loss_class: 1.0172 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:34:34] 	Iter 231300 Done. | loss1: 2.0494 | loss_class: 2.0489 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|18:35:25] 	Iter 231400 Done. | loss1: 1.4782 | loss_class: 1.4775 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:36:16] 	Iter 231500 Done. | loss1: 0.4672 | loss_class: 0.4665 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:37:08] 	Iter 231600 Done. | loss1: 1.7737 | loss_class: 1.7730 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:37:59] 	Iter 231700 Done. | loss1: 0.7254 | loss_class: 0.7248 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:38:50] 	Iter 231800 Done. | loss1: 0.8710 | loss_class: 0.8705 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|18:39:42] 	Iter 231900 Done. | loss1: 0.4939 | loss_class: 0.4934 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|18:40:33] 	Iter 232000 Done. | loss1: 0.5341 | loss_class: 0.5334 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:41:25] 	Iter 232100 Done. | loss1: 0.8094 | loss_class: 0.8090 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|18:42:16] 	Iter 232200 Done. | loss1: 0.7327 | loss_class: 0.7321 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:43:07] 	Iter 232300 Done. | loss1: 2.4017 | loss_class: 2.4008 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|18:43:58] 	Iter 232400 Done. | loss1: 0.1284 | loss_class: 0.1277 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:44:49] 	Iter 232500 Done. | loss1: 2.2130 | loss_class: 2.2124 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:45:41] 	Iter 232600 Done. | loss1: 0.4707 | loss_class: 0.4698 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|18:46:34] 	Iter 232700 Done. | loss1: 0.9752 | loss_class: 0.9744 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|18:47:26] 	Iter 232800 Done. | loss1: 1.5844 | loss_class: 1.5839 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:48:17] 	Iter 232900 Done. | loss1: 0.3291 | loss_class: 0.3284 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:49:08] 	Iter 233000 Done. | loss1: 1.2372 | loss_class: 1.2365 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:49:59] 	Iter 233100 Done. | loss1: 1.1376 | loss_class: 1.1369 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:50:50] 	Iter 233200 Done. | loss1: 1.9358 | loss_class: 1.9350 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|18:51:41] 	Iter 233300 Done. | loss1: 1.1234 | loss_class: 1.1227 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:52:33] 	Iter 233400 Done. | loss1: 1.3112 | loss_class: 1.3100 | loss_recon: 0.0012 | lr: 0.100000
[06.23.21|18:53:24] 	Iter 233500 Done. | loss1: 1.0425 | loss_class: 1.0418 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:54:15] 	Iter 233600 Done. | loss1: 1.3627 | loss_class: 1.3621 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:55:07] 	Iter 233700 Done. | loss1: 0.8114 | loss_class: 0.8105 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|18:55:58] 	Iter 233800 Done. | loss1: 1.6941 | loss_class: 1.6935 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:56:49] 	Iter 233900 Done. | loss1: 0.5562 | loss_class: 0.5552 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|18:57:41] 	Iter 234000 Done. | loss1: 1.8211 | loss_class: 1.8205 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|18:58:32] 	Iter 234100 Done. | loss1: 1.0138 | loss_class: 1.0131 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|18:59:24] 	Iter 234200 Done. | loss1: 1.6825 | loss_class: 1.6818 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:00:15] 	Iter 234300 Done. | loss1: 0.9717 | loss_class: 0.9710 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:01:06] 	Iter 234400 Done. | loss1: 4.7973 | loss_class: 4.7968 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|19:01:58] 	Iter 234500 Done. | loss1: 1.6872 | loss_class: 1.6866 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:02:50] 	Iter 234600 Done. | loss1: 2.0805 | loss_class: 2.0798 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:03:41] 	Iter 234700 Done. | loss1: 2.5897 | loss_class: 2.5890 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:04:32] 	Iter 234800 Done. | loss1: 2.0984 | loss_class: 2.0978 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:05:24] 	Iter 234900 Done. | loss1: 0.5431 | loss_class: 0.5424 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:06:14] 	Iter 235000 Done. | loss1: 0.2153 | loss_class: 0.2147 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:07:05] 	Iter 235100 Done. | loss1: 0.9623 | loss_class: 0.9616 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:07:57] 	Iter 235200 Done. | loss1: 0.3805 | loss_class: 0.3797 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:08:50] 	Iter 235300 Done. | loss1: 1.1520 | loss_class: 1.1513 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:09:43] 	Iter 235400 Done. | loss1: 1.2730 | loss_class: 1.2723 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:10:36] 	Iter 235500 Done. | loss1: 1.6522 | loss_class: 1.6517 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|19:11:29] 	Iter 235600 Done. | loss1: 0.9824 | loss_class: 0.9817 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:12:20] 	Iter 235700 Done. | loss1: 0.5415 | loss_class: 0.5409 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:13:14] 	Iter 235800 Done. | loss1: 1.3117 | loss_class: 1.3109 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:14:07] 	Iter 235900 Done. | loss1: 1.3834 | loss_class: 1.3829 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|19:14:59] 	Iter 236000 Done. | loss1: 0.7716 | loss_class: 0.7710 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:15:51] 	Iter 236100 Done. | loss1: 0.5325 | loss_class: 0.5319 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:16:44] 	Iter 236200 Done. | loss1: 0.8111 | loss_class: 0.8103 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:17:36] 	Iter 236300 Done. | loss1: 2.8736 | loss_class: 2.8729 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:18:28] 	Iter 236400 Done. | loss1: 0.6974 | loss_class: 0.6967 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:19:20] 	Iter 236500 Done. | loss1: 0.8888 | loss_class: 0.8883 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|19:20:13] 	Iter 236600 Done. | loss1: 2.9871 | loss_class: 2.9865 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:21:05] 	Iter 236700 Done. | loss1: 2.2637 | loss_class: 2.2627 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|19:21:58] 	Iter 236800 Done. | loss1: 1.0582 | loss_class: 1.0577 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|19:22:49] 	Iter 236900 Done. | loss1: 0.9705 | loss_class: 0.9699 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:23:41] 	Iter 237000 Done. | loss1: 3.0760 | loss_class: 3.0753 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:24:33] 	Iter 237100 Done. | loss1: 0.5098 | loss_class: 0.5094 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|19:25:25] 	Iter 237200 Done. | loss1: 1.1713 | loss_class: 1.1707 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:26:17] 	Iter 237300 Done. | loss1: 0.3140 | loss_class: 0.3133 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:27:09] 	Iter 237400 Done. | loss1: 0.8090 | loss_class: 0.8082 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:28:02] 	Iter 237500 Done. | loss1: 1.4982 | loss_class: 1.4976 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:28:54] 	Iter 237600 Done. | loss1: 0.6910 | loss_class: 0.6904 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:29:48] 	Iter 237700 Done. | loss1: 1.1893 | loss_class: 1.1887 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:30:40] 	Iter 237800 Done. | loss1: 1.0338 | loss_class: 1.0331 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:31:33] 	Iter 237900 Done. | loss1: 1.6562 | loss_class: 1.6555 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:32:25] 	Iter 238000 Done. | loss1: 0.6103 | loss_class: 0.6098 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|19:33:17] 	Iter 238100 Done. | loss1: 2.5919 | loss_class: 2.5911 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:34:09] 	Iter 238200 Done. | loss1: 1.5044 | loss_class: 1.5038 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:35:01] 	Iter 238300 Done. | loss1: 3.0347 | loss_class: 3.0342 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|19:35:53] 	Iter 238400 Done. | loss1: 0.2459 | loss_class: 0.2454 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|19:36:45] 	Iter 238500 Done. | loss1: 2.7266 | loss_class: 2.7261 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|19:37:37] 	Iter 238600 Done. | loss1: 0.9932 | loss_class: 0.9925 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:38:29] 	Iter 238700 Done. | loss1: 0.3960 | loss_class: 0.3954 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:39:21] 	Iter 238800 Done. | loss1: 1.5603 | loss_class: 1.5596 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:40:14] 	Iter 238900 Done. | loss1: 2.2351 | loss_class: 2.2343 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:41:06] 	Iter 239000 Done. | loss1: 1.0142 | loss_class: 1.0136 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:41:58] 	Iter 239100 Done. | loss1: 1.7159 | loss_class: 1.7153 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:42:49] 	Iter 239200 Done. | loss1: 1.2736 | loss_class: 1.2728 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:43:42] 	Iter 239300 Done. | loss1: 1.2956 | loss_class: 1.2950 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:44:33] 	Iter 239400 Done. | loss1: 1.3911 | loss_class: 1.3904 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:45:24] 	Iter 239500 Done. | loss1: 0.1943 | loss_class: 0.1936 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:46:16] 	Iter 239600 Done. | loss1: 0.9721 | loss_class: 0.9715 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:47:07] 	Iter 239700 Done. | loss1: 0.6878 | loss_class: 0.6869 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|19:47:58] 	Iter 239800 Done. | loss1: 1.7955 | loss_class: 1.7948 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:48:49] 	Iter 239900 Done. | loss1: 1.0717 | loss_class: 1.0712 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|19:49:41] 	Iter 240000 Done. | loss1: 1.3062 | loss_class: 1.3056 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:50:33] 	Iter 240100 Done. | loss1: 0.8395 | loss_class: 0.8388 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:51:24] 	Iter 240200 Done. | loss1: 1.8345 | loss_class: 1.8336 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|19:52:16] 	Iter 240300 Done. | loss1: 1.9069 | loss_class: 1.9062 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:53:07] 	Iter 240400 Done. | loss1: 1.7708 | loss_class: 1.7701 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|19:53:59] 	Iter 240500 Done. | loss1: 1.0213 | loss_class: 1.0207 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:54:13] 	mean_loss1: 1.175897919364081
[06.23.21|19:54:13] 	mean_loss_class: 1.17525131466353
[06.23.21|19:54:13] 	mean_loss_recon: 0.0006466046566929057
[06.23.21|19:54:13] Time consumption:
[06.23.21|19:54:13] Done.
[06.23.21|19:54:14] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch33_model1.pt.
[06.23.21|19:54:14] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch33_model2.pt.
[06.23.21|19:54:14] Training epoch: 34
[06.23.21|19:54:51] 	Iter 240600 Done. | loss1: 1.5511 | loss_class: 1.5505 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:55:42] 	Iter 240700 Done. | loss1: 0.1049 | loss_class: 0.1041 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|19:56:33] 	Iter 240800 Done. | loss1: 2.4444 | loss_class: 2.4438 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|19:57:23] 	Iter 240900 Done. | loss1: 1.9379 | loss_class: 1.9374 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|19:58:14] 	Iter 241000 Done. | loss1: 0.1999 | loss_class: 0.1994 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|19:59:04] 	Iter 241100 Done. | loss1: 2.8035 | loss_class: 2.8030 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|19:59:55] 	Iter 241200 Done. | loss1: 0.1585 | loss_class: 0.1579 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:00:45] 	Iter 241300 Done. | loss1: 1.6772 | loss_class: 1.6761 | loss_recon: 0.0011 | lr: 0.100000
[06.23.21|20:01:36] 	Iter 241400 Done. | loss1: 0.5834 | loss_class: 0.5829 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:02:26] 	Iter 241500 Done. | loss1: 0.8004 | loss_class: 0.7996 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|20:03:17] 	Iter 241600 Done. | loss1: 2.4611 | loss_class: 2.4606 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:04:08] 	Iter 241700 Done. | loss1: 1.1207 | loss_class: 1.1200 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|20:05:00] 	Iter 241800 Done. | loss1: 1.2278 | loss_class: 1.2271 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:05:51] 	Iter 241900 Done. | loss1: 1.4910 | loss_class: 1.4903 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:06:43] 	Iter 242000 Done. | loss1: 1.4240 | loss_class: 1.4234 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:07:34] 	Iter 242100 Done. | loss1: 1.2169 | loss_class: 1.2163 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:08:26] 	Iter 242200 Done. | loss1: 0.8946 | loss_class: 0.8941 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:09:17] 	Iter 242300 Done. | loss1: 0.3637 | loss_class: 0.3631 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:10:08] 	Iter 242400 Done. | loss1: 0.7221 | loss_class: 0.7216 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:10:59] 	Iter 242500 Done. | loss1: 1.0755 | loss_class: 1.0751 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|20:11:51] 	Iter 242600 Done. | loss1: 1.7417 | loss_class: 1.7413 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|20:12:42] 	Iter 242700 Done. | loss1: 0.4913 | loss_class: 0.4907 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:13:33] 	Iter 242800 Done. | loss1: 2.5780 | loss_class: 2.5775 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:14:23] 	Iter 242900 Done. | loss1: 0.2105 | loss_class: 0.2100 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:15:14] 	Iter 243000 Done. | loss1: 1.1061 | loss_class: 1.1055 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:16:05] 	Iter 243100 Done. | loss1: 1.5341 | loss_class: 1.5334 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:16:55] 	Iter 243200 Done. | loss1: 0.6621 | loss_class: 0.6616 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:17:46] 	Iter 243300 Done. | loss1: 0.3829 | loss_class: 0.3821 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:18:36] 	Iter 243400 Done. | loss1: 0.3268 | loss_class: 0.3262 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:19:26] 	Iter 243500 Done. | loss1: 0.5896 | loss_class: 0.5889 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|20:20:16] 	Iter 243600 Done. | loss1: 1.0222 | loss_class: 1.0217 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:21:07] 	Iter 243700 Done. | loss1: 1.9087 | loss_class: 1.9083 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:21:59] 	Iter 243800 Done. | loss1: 2.1798 | loss_class: 2.1792 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:22:49] 	Iter 243900 Done. | loss1: 0.8731 | loss_class: 0.8726 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|20:23:40] 	Iter 244000 Done. | loss1: 0.6678 | loss_class: 0.6672 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:24:30] 	Iter 244100 Done. | loss1: 2.2004 | loss_class: 2.1996 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:25:21] 	Iter 244200 Done. | loss1: 0.3152 | loss_class: 0.3145 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|20:26:14] 	Iter 244300 Done. | loss1: 0.1324 | loss_class: 0.1317 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|20:27:05] 	Iter 244400 Done. | loss1: 0.1040 | loss_class: 0.1033 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:27:55] 	Iter 244500 Done. | loss1: 0.7314 | loss_class: 0.7306 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:28:46] 	Iter 244600 Done. | loss1: 1.0817 | loss_class: 1.0811 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:29:36] 	Iter 244700 Done. | loss1: 0.3301 | loss_class: 0.3295 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:30:29] 	Iter 244800 Done. | loss1: 1.3011 | loss_class: 1.3006 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:31:19] 	Iter 244900 Done. | loss1: 0.2292 | loss_class: 0.2286 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:32:10] 	Iter 245000 Done. | loss1: 0.6024 | loss_class: 0.6020 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:33:00] 	Iter 245100 Done. | loss1: 1.0807 | loss_class: 1.0801 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:33:53] 	Iter 245200 Done. | loss1: 1.3482 | loss_class: 1.3474 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|20:34:43] 	Iter 245300 Done. | loss1: 1.5289 | loss_class: 1.5284 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:35:33] 	Iter 245400 Done. | loss1: 0.9805 | loss_class: 0.9797 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:36:23] 	Iter 245500 Done. | loss1: 0.2241 | loss_class: 0.2234 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:37:13] 	Iter 245600 Done. | loss1: 1.4479 | loss_class: 1.4474 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:38:03] 	Iter 245700 Done. | loss1: 1.2354 | loss_class: 1.2349 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:38:53] 	Iter 245800 Done. | loss1: 0.9478 | loss_class: 0.9471 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:39:43] 	Iter 245900 Done. | loss1: 2.4478 | loss_class: 2.4471 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:40:33] 	Iter 246000 Done. | loss1: 0.2154 | loss_class: 0.2148 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:41:24] 	Iter 246100 Done. | loss1: 1.9221 | loss_class: 1.9216 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:42:14] 	Iter 246200 Done. | loss1: 0.8824 | loss_class: 0.8817 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:43:04] 	Iter 246300 Done. | loss1: 0.7429 | loss_class: 0.7421 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|20:43:55] 	Iter 246400 Done. | loss1: 1.0961 | loss_class: 1.0955 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:44:46] 	Iter 246500 Done. | loss1: 1.5801 | loss_class: 1.5792 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|20:45:36] 	Iter 246600 Done. | loss1: 1.2329 | loss_class: 1.2321 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|20:46:26] 	Iter 246700 Done. | loss1: 0.4353 | loss_class: 0.4346 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|20:47:17] 	Iter 246800 Done. | loss1: 0.4221 | loss_class: 0.4214 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:48:08] 	Iter 246900 Done. | loss1: 2.0759 | loss_class: 2.0753 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:48:59] 	Iter 247000 Done. | loss1: 1.4826 | loss_class: 1.4819 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:49:49] 	Iter 247100 Done. | loss1: 0.9970 | loss_class: 0.9965 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:50:40] 	Iter 247200 Done. | loss1: 0.8346 | loss_class: 0.8340 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:51:31] 	Iter 247300 Done. | loss1: 1.2254 | loss_class: 1.2247 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:52:21] 	Iter 247400 Done. | loss1: 0.5159 | loss_class: 0.5153 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:53:12] 	Iter 247500 Done. | loss1: 3.2202 | loss_class: 3.2196 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:54:04] 	Iter 247600 Done. | loss1: 0.4596 | loss_class: 0.4589 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:54:56] 	Iter 247700 Done. | loss1: 1.8963 | loss_class: 1.8956 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:55:47] 	Iter 247800 Done. | loss1: 0.3659 | loss_class: 0.3651 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|20:56:37] 	Iter 247900 Done. | loss1: 0.4816 | loss_class: 0.4810 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|20:57:28] 	Iter 248000 Done. | loss1: 1.7773 | loss_class: 1.7766 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|20:58:20] 	Iter 248100 Done. | loss1: 1.5199 | loss_class: 1.5194 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|20:59:11] 	Iter 248200 Done. | loss1: 1.2703 | loss_class: 1.2697 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:00:02] 	Iter 248300 Done. | loss1: 1.4523 | loss_class: 1.4516 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:00:54] 	Iter 248400 Done. | loss1: 0.1826 | loss_class: 0.1821 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:01:46] 	Iter 248500 Done. | loss1: 1.4312 | loss_class: 1.4306 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:02:37] 	Iter 248600 Done. | loss1: 2.1846 | loss_class: 2.1839 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:03:28] 	Iter 248700 Done. | loss1: 0.3420 | loss_class: 0.3411 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|21:04:20] 	Iter 248800 Done. | loss1: 1.2992 | loss_class: 1.2983 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|21:05:11] 	Iter 248900 Done. | loss1: 0.5701 | loss_class: 0.5694 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:06:03] 	Iter 249000 Done. | loss1: 0.2542 | loss_class: 0.2536 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:06:55] 	Iter 249100 Done. | loss1: 0.2311 | loss_class: 0.2303 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|21:07:47] 	Iter 249200 Done. | loss1: 0.4673 | loss_class: 0.4667 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:08:39] 	Iter 249300 Done. | loss1: 2.1428 | loss_class: 2.1422 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:09:31] 	Iter 249400 Done. | loss1: 2.9035 | loss_class: 2.9028 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:10:22] 	Iter 249500 Done. | loss1: 2.3281 | loss_class: 2.3275 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:11:14] 	Iter 249600 Done. | loss1: 0.3888 | loss_class: 0.3881 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:12:06] 	Iter 249700 Done. | loss1: 1.4049 | loss_class: 1.4043 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:12:58] 	Iter 249800 Done. | loss1: 1.2816 | loss_class: 1.2808 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|21:13:50] 	Iter 249900 Done. | loss1: 0.2202 | loss_class: 0.2197 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:14:42] 	Iter 250000 Done. | loss1: 0.5207 | loss_class: 0.5200 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:15:34] 	Iter 250100 Done. | loss1: 1.2915 | loss_class: 1.2908 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:16:25] 	Iter 250200 Done. | loss1: 0.0642 | loss_class: 0.0636 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:17:17] 	Iter 250300 Done. | loss1: 2.8259 | loss_class: 2.8251 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|21:18:08] 	Iter 250400 Done. | loss1: 1.4496 | loss_class: 1.4491 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:19:00] 	Iter 250500 Done. | loss1: 0.8410 | loss_class: 0.8403 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:19:25] 	mean_loss1: 1.169794406849282
[06.23.21|21:19:25] 	mean_loss_class: 1.1691464212217735
[06.23.21|21:19:25] 	mean_loss_recon: 0.000647985393579546
[06.23.21|21:19:25] Time consumption:
[06.23.21|21:19:25] Done.
[06.23.21|21:19:26] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch34_model1.pt.
[06.23.21|21:19:26] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch34_model2.pt.
[06.23.21|21:19:26] Eval epoch: 34
[06.23.21|21:26:32] 	mean_loss1: 1.2570079622804657
[06.23.21|21:26:32] 	mean_loss_class: 1.2566350757382636
[06.23.21|21:26:32] 	mean_loss_recon: 0.03728860218886488
[06.23.21|21:26:32] 

[06.23.21|21:26:32] 	Top1: 64.62%
[06.23.21|21:26:32] 

[06.23.21|21:26:32] 	Top5: 90.91%
[06.23.21|21:26:32] Done.
[06.23.21|21:26:32] Training epoch: 35
[06.23.21|21:27:00] 	Iter 250600 Done. | loss1: 0.1964 | loss_class: 0.1957 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:27:51] 	Iter 250700 Done. | loss1: 1.1996 | loss_class: 1.1988 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|21:28:43] 	Iter 250800 Done. | loss1: 1.1345 | loss_class: 1.1342 | loss_recon: 0.0003 | lr: 0.100000
[06.23.21|21:29:34] 	Iter 250900 Done. | loss1: 2.0689 | loss_class: 2.0684 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:30:26] 	Iter 251000 Done. | loss1: 1.2029 | loss_class: 1.2022 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:31:18] 	Iter 251100 Done. | loss1: 1.2672 | loss_class: 1.2666 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:32:10] 	Iter 251200 Done. | loss1: 1.7152 | loss_class: 1.7143 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|21:33:01] 	Iter 251300 Done. | loss1: 1.2578 | loss_class: 1.2572 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:33:53] 	Iter 251400 Done. | loss1: 1.2691 | loss_class: 1.2684 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:34:45] 	Iter 251500 Done. | loss1: 1.4901 | loss_class: 1.4894 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:35:37] 	Iter 251600 Done. | loss1: 1.0210 | loss_class: 1.0204 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:36:29] 	Iter 251700 Done. | loss1: 0.6025 | loss_class: 0.6019 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:37:21] 	Iter 251800 Done. | loss1: 1.5860 | loss_class: 1.5853 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:38:13] 	Iter 251900 Done. | loss1: 1.9950 | loss_class: 1.9945 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:39:04] 	Iter 252000 Done. | loss1: 0.3279 | loss_class: 0.3273 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:39:55] 	Iter 252100 Done. | loss1: 1.1889 | loss_class: 1.1881 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|21:40:48] 	Iter 252200 Done. | loss1: 0.3159 | loss_class: 0.3153 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:41:41] 	Iter 252300 Done. | loss1: 2.2076 | loss_class: 2.2070 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:42:32] 	Iter 252400 Done. | loss1: 2.8647 | loss_class: 2.8641 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:43:25] 	Iter 252500 Done. | loss1: 0.9903 | loss_class: 0.9897 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:44:16] 	Iter 252600 Done. | loss1: 0.4857 | loss_class: 0.4849 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:45:08] 	Iter 252700 Done. | loss1: 1.5558 | loss_class: 1.5553 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:45:59] 	Iter 252800 Done. | loss1: 0.9811 | loss_class: 0.9806 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:46:50] 	Iter 252900 Done. | loss1: 0.3325 | loss_class: 0.3319 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:47:42] 	Iter 253000 Done. | loss1: 1.7323 | loss_class: 1.7319 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|21:48:33] 	Iter 253100 Done. | loss1: 2.9955 | loss_class: 2.9947 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:49:25] 	Iter 253200 Done. | loss1: 1.0832 | loss_class: 1.0827 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:50:17] 	Iter 253300 Done. | loss1: 0.7308 | loss_class: 0.7303 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:51:09] 	Iter 253400 Done. | loss1: 1.2505 | loss_class: 1.2499 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:52:00] 	Iter 253500 Done. | loss1: 0.7418 | loss_class: 0.7411 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:52:52] 	Iter 253600 Done. | loss1: 0.3988 | loss_class: 0.3981 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:53:44] 	Iter 253700 Done. | loss1: 1.4125 | loss_class: 1.4120 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:54:37] 	Iter 253800 Done. | loss1: 0.4778 | loss_class: 0.4772 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:55:28] 	Iter 253900 Done. | loss1: 1.6054 | loss_class: 1.6048 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:56:21] 	Iter 254000 Done. | loss1: 1.7457 | loss_class: 1.7451 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:57:13] 	Iter 254100 Done. | loss1: 1.1199 | loss_class: 1.1192 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|21:58:05] 	Iter 254200 Done. | loss1: 2.1138 | loss_class: 2.1131 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|21:58:56] 	Iter 254300 Done. | loss1: 0.6589 | loss_class: 0.6584 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|21:59:48] 	Iter 254400 Done. | loss1: 0.4652 | loss_class: 0.4647 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:00:41] 	Iter 254500 Done. | loss1: 1.4145 | loss_class: 1.4136 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|22:01:34] 	Iter 254600 Done. | loss1: 0.3213 | loss_class: 0.3208 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:02:25] 	Iter 254700 Done. | loss1: 0.6618 | loss_class: 0.6612 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:03:16] 	Iter 254800 Done. | loss1: 1.1119 | loss_class: 1.1110 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|22:04:08] 	Iter 254900 Done. | loss1: 0.3562 | loss_class: 0.3555 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:04:59] 	Iter 255000 Done. | loss1: 0.8001 | loss_class: 0.7994 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:05:50] 	Iter 255100 Done. | loss1: 0.9605 | loss_class: 0.9598 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:06:41] 	Iter 255200 Done. | loss1: 0.0598 | loss_class: 0.0590 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|22:07:32] 	Iter 255300 Done. | loss1: 2.7542 | loss_class: 2.7535 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:08:24] 	Iter 255400 Done. | loss1: 0.4161 | loss_class: 0.4154 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:09:15] 	Iter 255500 Done. | loss1: 0.7029 | loss_class: 0.7021 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:10:07] 	Iter 255600 Done. | loss1: 0.7812 | loss_class: 0.7806 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:10:58] 	Iter 255700 Done. | loss1: 0.3713 | loss_class: 0.3708 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:11:49] 	Iter 255800 Done. | loss1: 1.5307 | loss_class: 1.5297 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|22:12:40] 	Iter 255900 Done. | loss1: 1.5743 | loss_class: 1.5736 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|22:13:31] 	Iter 256000 Done. | loss1: 2.9227 | loss_class: 2.9219 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|22:14:22] 	Iter 256100 Done. | loss1: 0.8231 | loss_class: 0.8225 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:15:13] 	Iter 256200 Done. | loss1: 0.6585 | loss_class: 0.6578 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:16:04] 	Iter 256300 Done. | loss1: 0.7510 | loss_class: 0.7503 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:16:55] 	Iter 256400 Done. | loss1: 1.4828 | loss_class: 1.4821 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:17:45] 	Iter 256500 Done. | loss1: 0.7734 | loss_class: 0.7727 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:18:36] 	Iter 256600 Done. | loss1: 0.8070 | loss_class: 0.8064 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:19:26] 	Iter 256700 Done. | loss1: 0.9751 | loss_class: 0.9743 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|22:20:17] 	Iter 256800 Done. | loss1: 1.0213 | loss_class: 1.0208 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:21:08] 	Iter 256900 Done. | loss1: 1.7290 | loss_class: 1.7283 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:21:59] 	Iter 257000 Done. | loss1: 1.0259 | loss_class: 1.0253 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:22:50] 	Iter 257100 Done. | loss1: 1.8526 | loss_class: 1.8520 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:23:40] 	Iter 257200 Done. | loss1: 0.7248 | loss_class: 0.7241 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:24:31] 	Iter 257300 Done. | loss1: 1.6026 | loss_class: 1.6019 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:25:22] 	Iter 257400 Done. | loss1: 0.1126 | loss_class: 0.1118 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|22:26:13] 	Iter 257500 Done. | loss1: 1.0740 | loss_class: 1.0733 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:27:05] 	Iter 257600 Done. | loss1: 0.2743 | loss_class: 0.2736 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:27:56] 	Iter 257700 Done. | loss1: 1.6993 | loss_class: 1.6988 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:28:48] 	Iter 257800 Done. | loss1: 1.3204 | loss_class: 1.3198 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:29:38] 	Iter 257900 Done. | loss1: 0.8109 | loss_class: 0.8104 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:30:29] 	Iter 258000 Done. | loss1: 1.7432 | loss_class: 1.7427 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:31:20] 	Iter 258100 Done. | loss1: 1.2442 | loss_class: 1.2438 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:32:10] 	Iter 258200 Done. | loss1: 1.5721 | loss_class: 1.5714 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:33:00] 	Iter 258300 Done. | loss1: 3.7293 | loss_class: 3.7285 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|22:33:51] 	Iter 258400 Done. | loss1: 2.1800 | loss_class: 2.1792 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|22:34:42] 	Iter 258500 Done. | loss1: 2.3767 | loss_class: 2.3762 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:35:33] 	Iter 258600 Done. | loss1: 1.2791 | loss_class: 1.2784 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:36:23] 	Iter 258700 Done. | loss1: 0.3375 | loss_class: 0.3369 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:37:14] 	Iter 258800 Done. | loss1: 2.5301 | loss_class: 2.5295 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:38:05] 	Iter 258900 Done. | loss1: 0.4434 | loss_class: 0.4428 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:38:56] 	Iter 259000 Done. | loss1: 0.3981 | loss_class: 0.3972 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|22:39:46] 	Iter 259100 Done. | loss1: 2.6325 | loss_class: 2.6321 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|22:40:37] 	Iter 259200 Done. | loss1: 2.6711 | loss_class: 2.6705 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:41:27] 	Iter 259300 Done. | loss1: 0.9930 | loss_class: 0.9923 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:42:18] 	Iter 259400 Done. | loss1: 0.1179 | loss_class: 0.1172 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:43:09] 	Iter 259500 Done. | loss1: 0.7706 | loss_class: 0.7700 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:43:59] 	Iter 259600 Done. | loss1: 2.4688 | loss_class: 2.4683 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:44:51] 	Iter 259700 Done. | loss1: 1.3451 | loss_class: 1.3445 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:45:42] 	Iter 259800 Done. | loss1: 2.1646 | loss_class: 2.1640 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:46:34] 	Iter 259900 Done. | loss1: 0.2562 | loss_class: 0.2555 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:47:26] 	Iter 260000 Done. | loss1: 1.0228 | loss_class: 1.0222 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:48:17] 	Iter 260100 Done. | loss1: 0.2096 | loss_class: 0.2091 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:49:09] 	Iter 260200 Done. | loss1: 1.1722 | loss_class: 1.1717 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:50:01] 	Iter 260300 Done. | loss1: 1.7883 | loss_class: 1.7878 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:50:52] 	Iter 260400 Done. | loss1: 1.3319 | loss_class: 1.3313 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:51:43] 	Iter 260500 Done. | loss1: 1.1566 | loss_class: 1.1560 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:52:19] 	mean_loss1: 1.1665435936121666
[06.23.21|22:52:19] 	mean_loss_class: 1.1658963553473276
[06.23.21|22:52:19] 	mean_loss_recon: 0.000647238652974388
[06.23.21|22:52:19] Time consumption:
[06.23.21|22:52:19] Done.
[06.23.21|22:52:20] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch35_model1.pt.
[06.23.21|22:52:20] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch35_model2.pt.
[06.23.21|22:52:20] Training epoch: 36
[06.23.21|22:52:35] 	Iter 260600 Done. | loss1: 0.3801 | loss_class: 0.3796 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:53:26] 	Iter 260700 Done. | loss1: 0.3680 | loss_class: 0.3676 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:54:16] 	Iter 260800 Done. | loss1: 1.2404 | loss_class: 1.2395 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|22:55:06] 	Iter 260900 Done. | loss1: 2.1663 | loss_class: 2.1655 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|22:55:56] 	Iter 261000 Done. | loss1: 1.5155 | loss_class: 1.5148 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:56:46] 	Iter 261100 Done. | loss1: 1.3690 | loss_class: 1.3684 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|22:57:36] 	Iter 261200 Done. | loss1: 2.1581 | loss_class: 2.1574 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|22:58:26] 	Iter 261300 Done. | loss1: 0.9658 | loss_class: 0.9653 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|22:59:16] 	Iter 261400 Done. | loss1: 0.2395 | loss_class: 0.2390 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:00:06] 	Iter 261500 Done. | loss1: 0.6569 | loss_class: 0.6562 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:00:57] 	Iter 261600 Done. | loss1: 1.4313 | loss_class: 1.4307 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:01:47] 	Iter 261700 Done. | loss1: 1.7198 | loss_class: 1.7191 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:02:38] 	Iter 261800 Done. | loss1: 0.3153 | loss_class: 0.3146 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:03:28] 	Iter 261900 Done. | loss1: 2.5724 | loss_class: 2.5716 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:04:19] 	Iter 262000 Done. | loss1: 1.0599 | loss_class: 1.0591 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|23:05:10] 	Iter 262100 Done. | loss1: 1.4851 | loss_class: 1.4841 | loss_recon: 0.0010 | lr: 0.100000
[06.23.21|23:06:00] 	Iter 262200 Done. | loss1: 1.4326 | loss_class: 1.4318 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|23:06:51] 	Iter 262300 Done. | loss1: 0.9627 | loss_class: 0.9618 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|23:07:42] 	Iter 262400 Done. | loss1: 1.2494 | loss_class: 1.2487 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|23:08:34] 	Iter 262500 Done. | loss1: 1.1057 | loss_class: 1.1050 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:09:25] 	Iter 262600 Done. | loss1: 2.1235 | loss_class: 2.1229 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:10:15] 	Iter 262700 Done. | loss1: 0.0670 | loss_class: 0.0665 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:11:06] 	Iter 262800 Done. | loss1: 1.3347 | loss_class: 1.3339 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|23:11:57] 	Iter 262900 Done. | loss1: 1.2871 | loss_class: 1.2865 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:12:47] 	Iter 263000 Done. | loss1: 0.0941 | loss_class: 0.0933 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|23:13:37] 	Iter 263100 Done. | loss1: 0.7977 | loss_class: 0.7970 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:14:27] 	Iter 263200 Done. | loss1: 0.9126 | loss_class: 0.9121 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|23:15:18] 	Iter 263300 Done. | loss1: 0.8563 | loss_class: 0.8555 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|23:16:10] 	Iter 263400 Done. | loss1: 1.5115 | loss_class: 1.5108 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:17:00] 	Iter 263500 Done. | loss1: 0.8967 | loss_class: 0.8958 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|23:17:50] 	Iter 263600 Done. | loss1: 1.6968 | loss_class: 1.6962 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:18:40] 	Iter 263700 Done. | loss1: 1.5226 | loss_class: 1.5217 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|23:19:30] 	Iter 263800 Done. | loss1: 1.9308 | loss_class: 1.9303 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:20:20] 	Iter 263900 Done. | loss1: 0.8408 | loss_class: 0.8403 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:21:10] 	Iter 264000 Done. | loss1: 0.1661 | loss_class: 0.1656 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:22:00] 	Iter 264100 Done. | loss1: 2.5064 | loss_class: 2.5058 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:22:50] 	Iter 264200 Done. | loss1: 0.2814 | loss_class: 0.2808 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:23:40] 	Iter 264300 Done. | loss1: 0.4198 | loss_class: 0.4192 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:24:30] 	Iter 264400 Done. | loss1: 0.3673 | loss_class: 0.3667 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:25:20] 	Iter 264500 Done. | loss1: 1.7194 | loss_class: 1.7188 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:26:10] 	Iter 264600 Done. | loss1: 1.0959 | loss_class: 1.0954 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:27:00] 	Iter 264700 Done. | loss1: 1.3053 | loss_class: 1.3046 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:27:49] 	Iter 264800 Done. | loss1: 1.4696 | loss_class: 1.4690 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:28:40] 	Iter 264900 Done. | loss1: 0.3862 | loss_class: 0.3855 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:29:30] 	Iter 265000 Done. | loss1: 2.3245 | loss_class: 2.3239 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:30:20] 	Iter 265100 Done. | loss1: 0.6660 | loss_class: 0.6654 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:31:10] 	Iter 265200 Done. | loss1: 1.3829 | loss_class: 1.3824 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:32:00] 	Iter 265300 Done. | loss1: 1.5675 | loss_class: 1.5670 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:32:51] 	Iter 265400 Done. | loss1: 4.3691 | loss_class: 4.3683 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|23:33:41] 	Iter 265500 Done. | loss1: 1.2928 | loss_class: 1.2922 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:34:32] 	Iter 265600 Done. | loss1: 2.8741 | loss_class: 2.8734 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:35:22] 	Iter 265700 Done. | loss1: 1.0812 | loss_class: 1.0805 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:36:13] 	Iter 265800 Done. | loss1: 0.9103 | loss_class: 0.9096 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:37:03] 	Iter 265900 Done. | loss1: 2.1813 | loss_class: 2.1807 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:37:54] 	Iter 266000 Done. | loss1: 0.8208 | loss_class: 0.8199 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|23:38:45] 	Iter 266100 Done. | loss1: 1.3547 | loss_class: 1.3542 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:39:35] 	Iter 266200 Done. | loss1: 0.3288 | loss_class: 0.3283 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:40:26] 	Iter 266300 Done. | loss1: 1.0184 | loss_class: 1.0177 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:41:17] 	Iter 266400 Done. | loss1: 1.1326 | loss_class: 1.1321 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:42:07] 	Iter 266500 Done. | loss1: 1.4344 | loss_class: 1.4338 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:42:57] 	Iter 266600 Done. | loss1: 1.6186 | loss_class: 1.6181 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:43:49] 	Iter 266700 Done. | loss1: 0.9380 | loss_class: 0.9373 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:44:39] 	Iter 266800 Done. | loss1: 0.5690 | loss_class: 0.5685 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:45:29] 	Iter 266900 Done. | loss1: 2.4540 | loss_class: 2.4535 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:46:20] 	Iter 267000 Done. | loss1: 0.5528 | loss_class: 0.5519 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|23:47:12] 	Iter 267100 Done. | loss1: 0.9893 | loss_class: 0.9888 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:48:05] 	Iter 267200 Done. | loss1: 0.2601 | loss_class: 0.2594 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:48:55] 	Iter 267300 Done. | loss1: 0.5007 | loss_class: 0.5000 | loss_recon: 0.0008 | lr: 0.100000
[06.23.21|23:49:45] 	Iter 267400 Done. | loss1: 0.5300 | loss_class: 0.5293 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:50:36] 	Iter 267500 Done. | loss1: 1.6356 | loss_class: 1.6349 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:51:26] 	Iter 267600 Done. | loss1: 1.9520 | loss_class: 1.9515 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:52:15] 	Iter 267700 Done. | loss1: 0.4640 | loss_class: 0.4636 | loss_recon: 0.0004 | lr: 0.100000
[06.23.21|23:53:06] 	Iter 267800 Done. | loss1: 2.4852 | loss_class: 2.4847 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:53:57] 	Iter 267900 Done. | loss1: 1.7148 | loss_class: 1.7143 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:54:47] 	Iter 268000 Done. | loss1: 2.4114 | loss_class: 2.4106 | loss_recon: 0.0007 | lr: 0.100000
[06.23.21|23:55:37] 	Iter 268100 Done. | loss1: 1.3200 | loss_class: 1.3192 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|23:56:28] 	Iter 268200 Done. | loss1: 0.5813 | loss_class: 0.5808 | loss_recon: 0.0005 | lr: 0.100000
[06.23.21|23:57:18] 	Iter 268300 Done. | loss1: 0.9426 | loss_class: 0.9417 | loss_recon: 0.0009 | lr: 0.100000
[06.23.21|23:58:08] 	Iter 268400 Done. | loss1: 2.3754 | loss_class: 2.3748 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:58:59] 	Iter 268500 Done. | loss1: 2.6609 | loss_class: 2.6602 | loss_recon: 0.0006 | lr: 0.100000
[06.23.21|23:59:49] 	Iter 268600 Done. | loss1: 0.8373 | loss_class: 0.8367 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:00:40] 	Iter 268700 Done. | loss1: 1.2988 | loss_class: 1.2980 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|00:01:29] 	Iter 268800 Done. | loss1: 1.1014 | loss_class: 1.1008 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:02:19] 	Iter 268900 Done. | loss1: 1.1119 | loss_class: 1.1112 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|00:03:09] 	Iter 269000 Done. | loss1: 0.7532 | loss_class: 0.7527 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:03:59] 	Iter 269100 Done. | loss1: 2.5456 | loss_class: 2.5449 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:04:49] 	Iter 269200 Done. | loss1: 0.7573 | loss_class: 0.7566 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:05:40] 	Iter 269300 Done. | loss1: 1.8063 | loss_class: 1.8055 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|00:06:30] 	Iter 269400 Done. | loss1: 1.4730 | loss_class: 1.4723 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:07:21] 	Iter 269500 Done. | loss1: 2.1847 | loss_class: 2.1842 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:08:11] 	Iter 269600 Done. | loss1: 1.2118 | loss_class: 1.2112 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:09:01] 	Iter 269700 Done. | loss1: 0.7697 | loss_class: 0.7692 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:09:54] 	Iter 269800 Done. | loss1: 1.4347 | loss_class: 1.4337 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|00:10:48] 	Iter 269900 Done. | loss1: 0.6930 | loss_class: 0.6922 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:11:41] 	Iter 270000 Done. | loss1: 1.7259 | loss_class: 1.7253 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:12:34] 	Iter 270100 Done. | loss1: 1.1987 | loss_class: 1.1981 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:13:27] 	Iter 270200 Done. | loss1: 1.4536 | loss_class: 1.4531 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:14:17] 	Iter 270300 Done. | loss1: 1.5008 | loss_class: 1.5003 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:15:08] 	Iter 270400 Done. | loss1: 0.9785 | loss_class: 0.9779 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:15:58] 	Iter 270500 Done. | loss1: 0.8629 | loss_class: 0.8623 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:16:46] 	mean_loss1: 1.1647647228879983
[06.24.21|00:16:46] 	mean_loss_class: 1.1641163727726898
[06.24.21|00:16:46] 	mean_loss_recon: 0.0006483500236130521
[06.24.21|00:16:46] Time consumption:
[06.24.21|00:16:46] Done.
[06.24.21|00:16:46] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch36_model1.pt.
[06.24.21|00:16:46] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch36_model2.pt.
[06.24.21|00:16:46] Training epoch: 37
[06.24.21|00:16:50] 	Iter 270600 Done. | loss1: 1.6838 | loss_class: 1.6832 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:17:40] 	Iter 270700 Done. | loss1: 0.3363 | loss_class: 0.3357 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:18:33] 	Iter 270800 Done. | loss1: 0.2951 | loss_class: 0.2946 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:19:25] 	Iter 270900 Done. | loss1: 0.6076 | loss_class: 0.6070 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:20:17] 	Iter 271000 Done. | loss1: 1.0919 | loss_class: 1.0912 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:21:08] 	Iter 271100 Done. | loss1: 0.8197 | loss_class: 0.8193 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|00:22:00] 	Iter 271200 Done. | loss1: 1.3525 | loss_class: 1.3519 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:22:53] 	Iter 271300 Done. | loss1: 1.2103 | loss_class: 1.2097 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:23:44] 	Iter 271400 Done. | loss1: 1.5439 | loss_class: 1.5434 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:24:34] 	Iter 271500 Done. | loss1: 1.8064 | loss_class: 1.8057 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|00:25:24] 	Iter 271600 Done. | loss1: 1.7874 | loss_class: 1.7867 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:26:15] 	Iter 271700 Done. | loss1: 0.6389 | loss_class: 0.6383 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:27:06] 	Iter 271800 Done. | loss1: 3.0323 | loss_class: 3.0318 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|00:27:57] 	Iter 271900 Done. | loss1: 1.3902 | loss_class: 1.3896 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:28:47] 	Iter 272000 Done. | loss1: 1.4327 | loss_class: 1.4320 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|00:29:38] 	Iter 272100 Done. | loss1: 0.7416 | loss_class: 0.7409 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|00:30:28] 	Iter 272200 Done. | loss1: 1.9261 | loss_class: 1.9255 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:31:18] 	Iter 272300 Done. | loss1: 0.6074 | loss_class: 0.6066 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:32:10] 	Iter 272400 Done. | loss1: 0.4308 | loss_class: 0.4301 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:33:00] 	Iter 272500 Done. | loss1: 0.6567 | loss_class: 0.6561 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:33:52] 	Iter 272600 Done. | loss1: 1.6024 | loss_class: 1.6017 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:34:48] 	Iter 272700 Done. | loss1: 1.2863 | loss_class: 1.2857 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:35:42] 	Iter 272800 Done. | loss1: 0.6025 | loss_class: 0.6018 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:36:32] 	Iter 272900 Done. | loss1: 1.3326 | loss_class: 1.3321 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:37:23] 	Iter 273000 Done. | loss1: 1.2825 | loss_class: 1.2820 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:38:13] 	Iter 273100 Done. | loss1: 1.2511 | loss_class: 1.2504 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:39:03] 	Iter 273200 Done. | loss1: 0.3513 | loss_class: 0.3504 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|00:39:53] 	Iter 273300 Done. | loss1: 0.4129 | loss_class: 0.4122 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:40:43] 	Iter 273400 Done. | loss1: 1.5021 | loss_class: 1.5012 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|00:41:33] 	Iter 273500 Done. | loss1: 1.4335 | loss_class: 1.4329 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:42:23] 	Iter 273600 Done. | loss1: 1.6952 | loss_class: 1.6948 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:43:15] 	Iter 273700 Done. | loss1: 0.6557 | loss_class: 0.6551 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:44:07] 	Iter 273800 Done. | loss1: 0.4092 | loss_class: 0.4083 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|00:44:58] 	Iter 273900 Done. | loss1: 2.0790 | loss_class: 2.0785 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:45:49] 	Iter 274000 Done. | loss1: 0.9211 | loss_class: 0.9206 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:46:39] 	Iter 274100 Done. | loss1: 1.1278 | loss_class: 1.1270 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|00:47:30] 	Iter 274200 Done. | loss1: 0.5026 | loss_class: 0.5018 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:48:20] 	Iter 274300 Done. | loss1: 0.5477 | loss_class: 0.5472 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:49:11] 	Iter 274400 Done. | loss1: 1.4801 | loss_class: 1.4795 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:50:03] 	Iter 274500 Done. | loss1: 2.6076 | loss_class: 2.6072 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|00:50:55] 	Iter 274600 Done. | loss1: 0.7896 | loss_class: 0.7889 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:51:46] 	Iter 274700 Done. | loss1: 1.1085 | loss_class: 1.1080 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:52:36] 	Iter 274800 Done. | loss1: 1.1579 | loss_class: 1.1574 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:53:26] 	Iter 274900 Done. | loss1: 1.7471 | loss_class: 1.7463 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:54:16] 	Iter 275000 Done. | loss1: 1.0034 | loss_class: 1.0028 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:55:07] 	Iter 275100 Done. | loss1: 0.1034 | loss_class: 0.1027 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:55:57] 	Iter 275200 Done. | loss1: 0.9127 | loss_class: 0.9120 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|00:56:51] 	Iter 275300 Done. | loss1: 1.4238 | loss_class: 1.4233 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|00:57:43] 	Iter 275400 Done. | loss1: 0.8748 | loss_class: 0.8741 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:58:33] 	Iter 275500 Done. | loss1: 0.9833 | loss_class: 0.9827 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|00:59:23] 	Iter 275600 Done. | loss1: 1.2342 | loss_class: 1.2337 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:00:13] 	Iter 275700 Done. | loss1: 1.7004 | loss_class: 1.6998 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:01:03] 	Iter 275800 Done. | loss1: 0.7606 | loss_class: 0.7600 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:01:56] 	Iter 275900 Done. | loss1: 0.6894 | loss_class: 0.6888 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:02:46] 	Iter 276000 Done. | loss1: 1.3819 | loss_class: 1.3813 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:03:36] 	Iter 276100 Done. | loss1: 1.5318 | loss_class: 1.5311 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:04:26] 	Iter 276200 Done. | loss1: 0.5978 | loss_class: 0.5973 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:05:17] 	Iter 276300 Done. | loss1: 0.8433 | loss_class: 0.8426 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|01:06:07] 	Iter 276400 Done. | loss1: 0.7856 | loss_class: 0.7851 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:06:57] 	Iter 276500 Done. | loss1: 0.6648 | loss_class: 0.6641 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:07:48] 	Iter 276600 Done. | loss1: 1.7484 | loss_class: 1.7477 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:08:38] 	Iter 276700 Done. | loss1: 0.8422 | loss_class: 0.8417 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:09:29] 	Iter 276800 Done. | loss1: 0.4342 | loss_class: 0.4337 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:10:22] 	Iter 276900 Done. | loss1: 1.0993 | loss_class: 1.0989 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|01:11:12] 	Iter 277000 Done. | loss1: 1.4971 | loss_class: 1.4964 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:12:03] 	Iter 277100 Done. | loss1: 0.2637 | loss_class: 0.2633 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|01:12:52] 	Iter 277200 Done. | loss1: 0.8190 | loss_class: 0.8184 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:13:43] 	Iter 277300 Done. | loss1: 0.9472 | loss_class: 0.9466 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:14:33] 	Iter 277400 Done. | loss1: 2.2025 | loss_class: 2.2021 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:15:22] 	Iter 277500 Done. | loss1: 1.8979 | loss_class: 1.8973 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:16:12] 	Iter 277600 Done. | loss1: 0.6229 | loss_class: 0.6223 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:17:02] 	Iter 277700 Done. | loss1: 0.6869 | loss_class: 0.6863 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:17:52] 	Iter 277800 Done. | loss1: 1.9684 | loss_class: 1.9678 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:18:42] 	Iter 277900 Done. | loss1: 0.2366 | loss_class: 0.2358 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|01:19:33] 	Iter 278000 Done. | loss1: 2.6547 | loss_class: 2.6542 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:20:23] 	Iter 278100 Done. | loss1: 1.0177 | loss_class: 1.0170 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:21:13] 	Iter 278200 Done. | loss1: 1.3011 | loss_class: 1.3006 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:22:03] 	Iter 278300 Done. | loss1: 0.3139 | loss_class: 0.3132 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:22:53] 	Iter 278400 Done. | loss1: 2.4697 | loss_class: 2.4691 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:23:43] 	Iter 278500 Done. | loss1: 0.2883 | loss_class: 0.2876 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:24:37] 	Iter 278600 Done. | loss1: 1.9855 | loss_class: 1.9848 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:25:27] 	Iter 278700 Done. | loss1: 1.8474 | loss_class: 1.8468 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:26:17] 	Iter 278800 Done. | loss1: 0.4148 | loss_class: 0.4141 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:27:08] 	Iter 278900 Done. | loss1: 0.9206 | loss_class: 0.9200 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:27:59] 	Iter 279000 Done. | loss1: 0.5155 | loss_class: 0.5149 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:28:48] 	Iter 279100 Done. | loss1: 1.0259 | loss_class: 1.0255 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:29:39] 	Iter 279200 Done. | loss1: 2.3735 | loss_class: 2.3729 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:30:30] 	Iter 279300 Done. | loss1: 1.6025 | loss_class: 1.6019 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:31:24] 	Iter 279400 Done. | loss1: 1.0372 | loss_class: 1.0364 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:32:14] 	Iter 279500 Done. | loss1: 1.8090 | loss_class: 1.8084 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:33:04] 	Iter 279600 Done. | loss1: 1.6170 | loss_class: 1.6162 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:33:53] 	Iter 279700 Done. | loss1: 1.1573 | loss_class: 1.1568 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:34:43] 	Iter 279800 Done. | loss1: 0.0782 | loss_class: 0.0775 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:35:33] 	Iter 279900 Done. | loss1: 0.8571 | loss_class: 0.8566 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:36:24] 	Iter 280000 Done. | loss1: 1.1520 | loss_class: 1.1513 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:37:14] 	Iter 280100 Done. | loss1: 0.6930 | loss_class: 0.6922 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|01:38:04] 	Iter 280200 Done. | loss1: 0.7282 | loss_class: 0.7275 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:38:54] 	Iter 280300 Done. | loss1: 1.5278 | loss_class: 1.5270 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:39:44] 	Iter 280400 Done. | loss1: 0.1129 | loss_class: 0.1121 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|01:40:34] 	Iter 280500 Done. | loss1: 0.6606 | loss_class: 0.6598 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|01:41:24] 	Iter 280600 Done. | loss1: 1.0602 | loss_class: 1.0596 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:41:32] 	mean_loss1: 1.1597557982938735
[06.24.21|01:41:32] 	mean_loss_class: 1.1591084752984737
[06.24.21|01:41:32] 	mean_loss_recon: 0.0006473226350033516
[06.24.21|01:41:32] Time consumption:
[06.24.21|01:41:32] Done.
[06.24.21|01:41:32] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch37_model1.pt.
[06.24.21|01:41:32] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch37_model2.pt.
[06.24.21|01:41:32] Training epoch: 38
[06.24.21|01:42:16] 	Iter 280700 Done. | loss1: 0.8230 | loss_class: 0.8224 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:43:07] 	Iter 280800 Done. | loss1: 0.5762 | loss_class: 0.5757 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:43:58] 	Iter 280900 Done. | loss1: 1.9278 | loss_class: 1.9273 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:44:49] 	Iter 281000 Done. | loss1: 2.4446 | loss_class: 2.4440 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:45:39] 	Iter 281100 Done. | loss1: 3.2455 | loss_class: 3.2449 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:46:30] 	Iter 281200 Done. | loss1: 1.2521 | loss_class: 1.2516 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:47:23] 	Iter 281300 Done. | loss1: 1.0341 | loss_class: 1.0337 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:48:15] 	Iter 281400 Done. | loss1: 1.0592 | loss_class: 1.0582 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|01:49:05] 	Iter 281500 Done. | loss1: 0.7404 | loss_class: 0.7400 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:49:55] 	Iter 281600 Done. | loss1: 1.7308 | loss_class: 1.7302 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:50:45] 	Iter 281700 Done. | loss1: 0.6672 | loss_class: 0.6665 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:51:35] 	Iter 281800 Done. | loss1: 0.4298 | loss_class: 0.4293 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:52:26] 	Iter 281900 Done. | loss1: 0.4525 | loss_class: 0.4520 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|01:53:16] 	Iter 282000 Done. | loss1: 0.9800 | loss_class: 0.9793 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:54:06] 	Iter 282100 Done. | loss1: 0.4245 | loss_class: 0.4236 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|01:54:57] 	Iter 282200 Done. | loss1: 0.4328 | loss_class: 0.4321 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:55:47] 	Iter 282300 Done. | loss1: 0.6189 | loss_class: 0.6182 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:56:37] 	Iter 282400 Done. | loss1: 0.9845 | loss_class: 0.9839 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|01:57:27] 	Iter 282500 Done. | loss1: 1.8655 | loss_class: 1.8652 | loss_recon: 0.0003 | lr: 0.100000
[06.24.21|01:58:17] 	Iter 282600 Done. | loss1: 0.2259 | loss_class: 0.2252 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:59:08] 	Iter 282700 Done. | loss1: 0.9964 | loss_class: 0.9956 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|01:59:59] 	Iter 282800 Done. | loss1: 0.3848 | loss_class: 0.3839 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|02:00:49] 	Iter 282900 Done. | loss1: 0.4861 | loss_class: 0.4856 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:01:40] 	Iter 283000 Done. | loss1: 0.5954 | loss_class: 0.5950 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:02:31] 	Iter 283100 Done. | loss1: 0.7220 | loss_class: 0.7215 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:03:21] 	Iter 283200 Done. | loss1: 0.2565 | loss_class: 0.2559 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:04:11] 	Iter 283300 Done. | loss1: 0.9652 | loss_class: 0.9646 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:05:01] 	Iter 283400 Done. | loss1: 1.5887 | loss_class: 1.5878 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|02:05:52] 	Iter 283500 Done. | loss1: 1.1030 | loss_class: 1.1024 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:06:42] 	Iter 283600 Done. | loss1: 0.9248 | loss_class: 0.9244 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|02:07:32] 	Iter 283700 Done. | loss1: 2.9509 | loss_class: 2.9503 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:08:22] 	Iter 283800 Done. | loss1: 1.2831 | loss_class: 1.2825 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:09:12] 	Iter 283900 Done. | loss1: 1.4423 | loss_class: 1.4417 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:10:03] 	Iter 284000 Done. | loss1: 0.7607 | loss_class: 0.7600 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:10:53] 	Iter 284100 Done. | loss1: 1.0219 | loss_class: 1.0214 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:11:44] 	Iter 284200 Done. | loss1: 0.7875 | loss_class: 0.7868 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:12:35] 	Iter 284300 Done. | loss1: 3.3508 | loss_class: 3.3502 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:13:25] 	Iter 284400 Done. | loss1: 3.7760 | loss_class: 3.7754 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:14:15] 	Iter 284500 Done. | loss1: 0.9044 | loss_class: 0.9039 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:15:05] 	Iter 284600 Done. | loss1: 0.5293 | loss_class: 0.5288 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:15:55] 	Iter 284700 Done. | loss1: 0.3659 | loss_class: 0.3654 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:16:47] 	Iter 284800 Done. | loss1: 1.2144 | loss_class: 1.2138 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:17:37] 	Iter 284900 Done. | loss1: 1.9625 | loss_class: 1.9620 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:18:28] 	Iter 285000 Done. | loss1: 0.3195 | loss_class: 0.3190 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:19:18] 	Iter 285100 Done. | loss1: 2.8360 | loss_class: 2.8353 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:20:09] 	Iter 285200 Done. | loss1: 1.4525 | loss_class: 1.4517 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|02:21:00] 	Iter 285300 Done. | loss1: 1.0911 | loss_class: 1.0906 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|02:21:50] 	Iter 285400 Done. | loss1: 0.5334 | loss_class: 0.5328 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:22:41] 	Iter 285500 Done. | loss1: 1.2606 | loss_class: 1.2600 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:23:31] 	Iter 285600 Done. | loss1: 1.7305 | loss_class: 1.7298 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:24:22] 	Iter 285700 Done. | loss1: 1.7863 | loss_class: 1.7856 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:25:13] 	Iter 285800 Done. | loss1: 1.8058 | loss_class: 1.8054 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:26:03] 	Iter 285900 Done. | loss1: 0.7062 | loss_class: 0.7054 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|02:26:53] 	Iter 286000 Done. | loss1: 0.3738 | loss_class: 0.3730 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:27:44] 	Iter 286100 Done. | loss1: 0.4973 | loss_class: 0.4967 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:28:34] 	Iter 286200 Done. | loss1: 1.3899 | loss_class: 1.3892 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:29:24] 	Iter 286300 Done. | loss1: 0.9441 | loss_class: 0.9434 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:30:14] 	Iter 286400 Done. | loss1: 0.7233 | loss_class: 0.7228 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:31:04] 	Iter 286500 Done. | loss1: 0.8526 | loss_class: 0.8521 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:31:54] 	Iter 286600 Done. | loss1: 0.8782 | loss_class: 0.8776 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:32:44] 	Iter 286700 Done. | loss1: 1.5203 | loss_class: 1.5197 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:33:34] 	Iter 286800 Done. | loss1: 2.1248 | loss_class: 2.1241 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:34:24] 	Iter 286900 Done. | loss1: 0.9483 | loss_class: 0.9474 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|02:35:13] 	Iter 287000 Done. | loss1: 2.7993 | loss_class: 2.7987 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:36:03] 	Iter 287100 Done. | loss1: 0.2275 | loss_class: 0.2268 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|02:36:53] 	Iter 287200 Done. | loss1: 0.5218 | loss_class: 0.5211 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:37:43] 	Iter 287300 Done. | loss1: 1.0710 | loss_class: 1.0704 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:38:33] 	Iter 287400 Done. | loss1: 0.7812 | loss_class: 0.7806 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:39:23] 	Iter 287500 Done. | loss1: 1.6560 | loss_class: 1.6553 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:40:12] 	Iter 287600 Done. | loss1: 2.8611 | loss_class: 2.8605 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:41:03] 	Iter 287700 Done. | loss1: 0.8352 | loss_class: 0.8345 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:41:58] 	Iter 287800 Done. | loss1: 1.0679 | loss_class: 1.0673 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:42:48] 	Iter 287900 Done. | loss1: 0.6154 | loss_class: 0.6148 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:43:41] 	Iter 288000 Done. | loss1: 1.4381 | loss_class: 1.4376 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:44:32] 	Iter 288100 Done. | loss1: 1.1515 | loss_class: 1.1509 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:45:23] 	Iter 288200 Done. | loss1: 1.1329 | loss_class: 1.1324 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:46:13] 	Iter 288300 Done. | loss1: 1.5786 | loss_class: 1.5780 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:47:04] 	Iter 288400 Done. | loss1: 3.2019 | loss_class: 3.2011 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|02:47:54] 	Iter 288500 Done. | loss1: 1.3233 | loss_class: 1.3228 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:48:45] 	Iter 288600 Done. | loss1: 1.3176 | loss_class: 1.3169 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:49:36] 	Iter 288700 Done. | loss1: 1.6537 | loss_class: 1.6532 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:50:26] 	Iter 288800 Done. | loss1: 1.1052 | loss_class: 1.1045 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:51:18] 	Iter 288900 Done. | loss1: 2.5346 | loss_class: 2.5340 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:52:10] 	Iter 289000 Done. | loss1: 1.4322 | loss_class: 1.4316 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:53:02] 	Iter 289100 Done. | loss1: 0.6919 | loss_class: 0.6915 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|02:53:54] 	Iter 289200 Done. | loss1: 1.5252 | loss_class: 1.5247 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:54:46] 	Iter 289300 Done. | loss1: 0.3864 | loss_class: 0.3858 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:55:38] 	Iter 289400 Done. | loss1: 1.3884 | loss_class: 1.3878 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:56:29] 	Iter 289500 Done. | loss1: 1.0348 | loss_class: 1.0341 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|02:57:21] 	Iter 289600 Done. | loss1: 0.1918 | loss_class: 0.1911 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|02:58:12] 	Iter 289700 Done. | loss1: 0.9493 | loss_class: 0.9488 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:59:04] 	Iter 289800 Done. | loss1: 1.3406 | loss_class: 1.3401 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|02:59:55] 	Iter 289900 Done. | loss1: 0.8670 | loss_class: 0.8663 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:00:47] 	Iter 290000 Done. | loss1: 1.5220 | loss_class: 1.5214 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:01:38] 	Iter 290100 Done. | loss1: 3.1735 | loss_class: 3.1728 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:02:30] 	Iter 290200 Done. | loss1: 1.0744 | loss_class: 1.0738 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:03:23] 	Iter 290300 Done. | loss1: 0.5530 | loss_class: 0.5524 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:04:14] 	Iter 290400 Done. | loss1: 1.8613 | loss_class: 1.8605 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|03:05:06] 	Iter 290500 Done. | loss1: 0.5057 | loss_class: 0.5050 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:05:58] 	Iter 290600 Done. | loss1: 0.3857 | loss_class: 0.3851 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:06:17] 	mean_loss1: 1.154557900235669
[06.24.21|03:06:17] 	mean_loss_class: 1.1539104207229007
[06.24.21|03:06:17] 	mean_loss_recon: 0.0006474793097738424
[06.24.21|03:06:17] Time consumption:
[06.24.21|03:06:17] Done.
[06.24.21|03:06:17] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch38_model1.pt.
[06.24.21|03:06:17] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch38_model2.pt.
[06.24.21|03:06:17] Training epoch: 39
[06.24.21|03:06:51] 	Iter 290700 Done. | loss1: 0.2147 | loss_class: 0.2141 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:07:44] 	Iter 290800 Done. | loss1: 0.2019 | loss_class: 0.2013 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:08:35] 	Iter 290900 Done. | loss1: 1.6720 | loss_class: 1.6714 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:09:26] 	Iter 291000 Done. | loss1: 0.1588 | loss_class: 0.1581 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:10:16] 	Iter 291100 Done. | loss1: 0.7347 | loss_class: 0.7342 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:11:08] 	Iter 291200 Done. | loss1: 1.6566 | loss_class: 1.6559 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:12:01] 	Iter 291300 Done. | loss1: 1.8107 | loss_class: 1.8102 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:12:51] 	Iter 291400 Done. | loss1: 1.1050 | loss_class: 1.1044 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:13:43] 	Iter 291500 Done. | loss1: 1.4169 | loss_class: 1.4162 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:14:35] 	Iter 291600 Done. | loss1: 1.0928 | loss_class: 1.0921 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:15:25] 	Iter 291700 Done. | loss1: 2.2573 | loss_class: 2.2565 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|03:16:16] 	Iter 291800 Done. | loss1: 0.8687 | loss_class: 0.8678 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|03:17:07] 	Iter 291900 Done. | loss1: 0.9783 | loss_class: 0.9775 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|03:17:58] 	Iter 292000 Done. | loss1: 2.0372 | loss_class: 2.0367 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:18:48] 	Iter 292100 Done. | loss1: 1.0530 | loss_class: 1.0522 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:19:39] 	Iter 292200 Done. | loss1: 0.7171 | loss_class: 0.7168 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|03:20:29] 	Iter 292300 Done. | loss1: 0.9195 | loss_class: 0.9189 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:21:20] 	Iter 292400 Done. | loss1: 1.5663 | loss_class: 1.5659 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|03:22:10] 	Iter 292500 Done. | loss1: 1.4565 | loss_class: 1.4561 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|03:23:01] 	Iter 292600 Done. | loss1: 0.5454 | loss_class: 0.5448 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:23:51] 	Iter 292700 Done. | loss1: 1.5363 | loss_class: 1.5354 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|03:24:41] 	Iter 292800 Done. | loss1: 0.6182 | loss_class: 0.6176 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:25:32] 	Iter 292900 Done. | loss1: 1.0505 | loss_class: 1.0499 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:26:22] 	Iter 293000 Done. | loss1: 1.3130 | loss_class: 1.3125 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:27:12] 	Iter 293100 Done. | loss1: 0.2722 | loss_class: 0.2714 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:28:03] 	Iter 293200 Done. | loss1: 1.0382 | loss_class: 1.0374 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|03:28:54] 	Iter 293300 Done. | loss1: 1.5808 | loss_class: 1.5801 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:29:44] 	Iter 293400 Done. | loss1: 0.3077 | loss_class: 0.3072 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:30:35] 	Iter 293500 Done. | loss1: 1.3682 | loss_class: 1.3675 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:31:25] 	Iter 293600 Done. | loss1: 0.9281 | loss_class: 0.9274 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:32:15] 	Iter 293700 Done. | loss1: 0.1872 | loss_class: 0.1864 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:33:05] 	Iter 293800 Done. | loss1: 0.7318 | loss_class: 0.7312 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:33:55] 	Iter 293900 Done. | loss1: 1.0006 | loss_class: 1.0001 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:34:46] 	Iter 294000 Done. | loss1: 2.2908 | loss_class: 2.2902 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:35:36] 	Iter 294100 Done. | loss1: 2.5385 | loss_class: 2.5380 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:36:26] 	Iter 294200 Done. | loss1: 2.4808 | loss_class: 2.4802 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:37:19] 	Iter 294300 Done. | loss1: 0.6139 | loss_class: 0.6134 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:38:10] 	Iter 294400 Done. | loss1: 1.1623 | loss_class: 1.1617 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:39:00] 	Iter 294500 Done. | loss1: 0.3426 | loss_class: 0.3420 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:39:51] 	Iter 294600 Done. | loss1: 1.3969 | loss_class: 1.3964 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:40:41] 	Iter 294700 Done. | loss1: 0.8452 | loss_class: 0.8446 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:41:32] 	Iter 294800 Done. | loss1: 0.3798 | loss_class: 0.3792 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:42:22] 	Iter 294900 Done. | loss1: 0.6784 | loss_class: 0.6780 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:43:13] 	Iter 295000 Done. | loss1: 3.5442 | loss_class: 3.5435 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:44:03] 	Iter 295100 Done. | loss1: 1.2994 | loss_class: 1.2987 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:44:53] 	Iter 295200 Done. | loss1: 1.6920 | loss_class: 1.6915 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:45:42] 	Iter 295300 Done. | loss1: 1.1456 | loss_class: 1.1450 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:46:32] 	Iter 295400 Done. | loss1: 1.5013 | loss_class: 1.5007 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:47:23] 	Iter 295500 Done. | loss1: 2.3815 | loss_class: 2.3811 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|03:48:13] 	Iter 295600 Done. | loss1: 2.1016 | loss_class: 2.1009 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:49:03] 	Iter 295700 Done. | loss1: 0.7403 | loss_class: 0.7396 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:49:53] 	Iter 295800 Done. | loss1: 0.9535 | loss_class: 0.9529 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:50:42] 	Iter 295900 Done. | loss1: 0.5085 | loss_class: 0.5078 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:51:32] 	Iter 296000 Done. | loss1: 0.8357 | loss_class: 0.8349 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|03:52:22] 	Iter 296100 Done. | loss1: 1.5594 | loss_class: 1.5587 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:53:12] 	Iter 296200 Done. | loss1: 1.3990 | loss_class: 1.3983 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:54:02] 	Iter 296300 Done. | loss1: 1.0663 | loss_class: 1.0657 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:54:52] 	Iter 296400 Done. | loss1: 0.1461 | loss_class: 0.1453 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|03:55:42] 	Iter 296500 Done. | loss1: 0.2651 | loss_class: 0.2645 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:56:32] 	Iter 296600 Done. | loss1: 1.2041 | loss_class: 1.2033 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|03:57:22] 	Iter 296700 Done. | loss1: 2.3123 | loss_class: 2.3118 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|03:58:12] 	Iter 296800 Done. | loss1: 0.6380 | loss_class: 0.6373 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|03:59:02] 	Iter 296900 Done. | loss1: 1.7699 | loss_class: 1.7693 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|03:59:52] 	Iter 297000 Done. | loss1: 0.5588 | loss_class: 0.5583 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|04:00:43] 	Iter 297100 Done. | loss1: 0.5588 | loss_class: 0.5581 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:01:34] 	Iter 297200 Done. | loss1: 2.2085 | loss_class: 2.2079 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:02:24] 	Iter 297300 Done. | loss1: 0.9566 | loss_class: 0.9561 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|04:03:14] 	Iter 297400 Done. | loss1: 0.8445 | loss_class: 0.8440 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|04:04:04] 	Iter 297500 Done. | loss1: 2.3640 | loss_class: 2.3634 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:04:55] 	Iter 297600 Done. | loss1: 2.9240 | loss_class: 2.9233 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:05:45] 	Iter 297700 Done. | loss1: 1.8937 | loss_class: 1.8932 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:06:36] 	Iter 297800 Done. | loss1: 1.1782 | loss_class: 1.1777 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|04:07:26] 	Iter 297900 Done. | loss1: 0.9473 | loss_class: 0.9466 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:08:17] 	Iter 298000 Done. | loss1: 1.8849 | loss_class: 1.8841 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|04:09:07] 	Iter 298100 Done. | loss1: 0.8088 | loss_class: 0.8083 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:09:58] 	Iter 298200 Done. | loss1: 2.0504 | loss_class: 2.0498 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:10:48] 	Iter 298300 Done. | loss1: 0.9286 | loss_class: 0.9279 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:11:38] 	Iter 298400 Done. | loss1: 0.4304 | loss_class: 0.4296 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|04:12:29] 	Iter 298500 Done. | loss1: 2.5698 | loss_class: 2.5693 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:13:20] 	Iter 298600 Done. | loss1: 1.8419 | loss_class: 1.8414 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|04:14:11] 	Iter 298700 Done. | loss1: 0.9026 | loss_class: 0.9018 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|04:15:01] 	Iter 298800 Done. | loss1: 1.8064 | loss_class: 1.8049 | loss_recon: 0.0015 | lr: 0.100000
[06.24.21|04:15:52] 	Iter 298900 Done. | loss1: 0.7201 | loss_class: 0.7195 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:16:42] 	Iter 299000 Done. | loss1: 1.3653 | loss_class: 1.3648 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|04:17:32] 	Iter 299100 Done. | loss1: 0.9793 | loss_class: 0.9786 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:18:23] 	Iter 299200 Done. | loss1: 1.2008 | loss_class: 1.2001 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:19:13] 	Iter 299300 Done. | loss1: 2.4583 | loss_class: 2.4577 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|04:20:04] 	Iter 299400 Done. | loss1: 1.9291 | loss_class: 1.9285 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:20:54] 	Iter 299500 Done. | loss1: 0.3644 | loss_class: 0.3638 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:21:44] 	Iter 299600 Done. | loss1: 0.7909 | loss_class: 0.7903 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:22:34] 	Iter 299700 Done. | loss1: 1.3690 | loss_class: 1.3684 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:23:25] 	Iter 299800 Done. | loss1: 1.1589 | loss_class: 1.1584 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|04:24:15] 	Iter 299900 Done. | loss1: 1.1532 | loss_class: 1.1525 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:25:05] 	Iter 300000 Done. | loss1: 1.4341 | loss_class: 1.4335 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|04:25:56] 	Iter 300100 Done. | loss1: 1.0933 | loss_class: 1.0928 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|04:26:46] 	Iter 300200 Done. | loss1: 0.9344 | loss_class: 0.9336 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|04:27:37] 	Iter 300300 Done. | loss1: 0.6012 | loss_class: 0.6004 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|04:28:27] 	Iter 300400 Done. | loss1: 1.6103 | loss_class: 1.6095 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|04:29:18] 	Iter 300500 Done. | loss1: 0.6941 | loss_class: 0.6934 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:30:09] 	Iter 300600 Done. | loss1: 1.5684 | loss_class: 1.5676 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:30:42] 	mean_loss1: 1.148236310038081
[06.24.21|04:30:42] 	mean_loss_class: 1.1475889867139957
[06.24.21|04:30:42] 	mean_loss_recon: 0.0006473232673027991
[06.24.21|04:30:42] Time consumption:
[06.24.21|04:30:42] Done.
[06.24.21|04:30:42] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch39_model1.pt.
[06.24.21|04:30:42] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch39_model2.pt.
[06.24.21|04:30:42] Eval epoch: 39
[06.24.21|04:37:13] 	mean_loss1: 1.2153872038389362
[06.24.21|04:37:13] 	mean_loss_class: 1.2150354819126832
[06.24.21|04:37:13] 	mean_loss_recon: 0.035172222532303066
[06.24.21|04:37:13] 

[06.24.21|04:37:13] 	Top1: 65.68%
[06.24.21|04:37:13] 

[06.24.21|04:37:13] 	Top5: 91.41%
[06.24.21|04:37:13] Done.
[06.24.21|04:37:13] Training epoch: 40
[06.24.21|04:37:34] 	Iter 300700 Done. | loss1: 2.5567 | loss_class: 2.5562 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:38:25] 	Iter 300800 Done. | loss1: 2.3074 | loss_class: 2.3068 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:39:16] 	Iter 300900 Done. | loss1: 1.9361 | loss_class: 1.9355 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:40:06] 	Iter 301000 Done. | loss1: 1.3137 | loss_class: 1.3131 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:40:56] 	Iter 301100 Done. | loss1: 1.0966 | loss_class: 1.0960 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:41:46] 	Iter 301200 Done. | loss1: 1.8731 | loss_class: 1.8724 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:42:37] 	Iter 301300 Done. | loss1: 0.2334 | loss_class: 0.2326 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|04:43:30] 	Iter 301400 Done. | loss1: 2.4189 | loss_class: 2.4182 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:44:22] 	Iter 301500 Done. | loss1: 0.2453 | loss_class: 0.2446 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:45:12] 	Iter 301600 Done. | loss1: 2.6451 | loss_class: 2.6444 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:46:03] 	Iter 301700 Done. | loss1: 0.8404 | loss_class: 0.8397 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:46:53] 	Iter 301800 Done. | loss1: 2.9284 | loss_class: 2.9280 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|04:47:43] 	Iter 301900 Done. | loss1: 0.8451 | loss_class: 0.8443 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|04:48:33] 	Iter 302000 Done. | loss1: 2.3178 | loss_class: 2.3172 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:49:23] 	Iter 302100 Done. | loss1: 1.6831 | loss_class: 1.6826 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|04:50:13] 	Iter 302200 Done. | loss1: 0.3038 | loss_class: 0.3032 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:51:02] 	Iter 302300 Done. | loss1: 1.2601 | loss_class: 1.2594 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:51:52] 	Iter 302400 Done. | loss1: 0.0369 | loss_class: 0.0363 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:52:42] 	Iter 302500 Done. | loss1: 0.7128 | loss_class: 0.7121 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:53:32] 	Iter 302600 Done. | loss1: 0.6485 | loss_class: 0.6477 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|04:54:22] 	Iter 302700 Done. | loss1: 1.7355 | loss_class: 1.7348 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:55:12] 	Iter 302800 Done. | loss1: 1.0764 | loss_class: 1.0758 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|04:56:02] 	Iter 302900 Done. | loss1: 0.4980 | loss_class: 0.4973 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:56:52] 	Iter 303000 Done. | loss1: 0.5412 | loss_class: 0.5404 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|04:57:42] 	Iter 303100 Done. | loss1: 3.2957 | loss_class: 3.2951 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|04:58:32] 	Iter 303200 Done. | loss1: 1.0957 | loss_class: 1.0952 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|04:59:22] 	Iter 303300 Done. | loss1: 1.1348 | loss_class: 1.1341 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:00:11] 	Iter 303400 Done. | loss1: 0.3613 | loss_class: 0.3605 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:01:03] 	Iter 303500 Done. | loss1: 0.2232 | loss_class: 0.2226 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:01:53] 	Iter 303600 Done. | loss1: 1.7735 | loss_class: 1.7729 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:02:43] 	Iter 303700 Done. | loss1: 0.3298 | loss_class: 0.3291 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:03:33] 	Iter 303800 Done. | loss1: 1.2284 | loss_class: 1.2278 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:04:24] 	Iter 303900 Done. | loss1: 0.4126 | loss_class: 0.4121 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:05:14] 	Iter 304000 Done. | loss1: 0.9594 | loss_class: 0.9588 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:06:04] 	Iter 304100 Done. | loss1: 0.8918 | loss_class: 0.8912 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:06:54] 	Iter 304200 Done. | loss1: 0.0722 | loss_class: 0.0714 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|05:07:44] 	Iter 304300 Done. | loss1: 2.7626 | loss_class: 2.7620 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:08:34] 	Iter 304400 Done. | loss1: 0.5996 | loss_class: 0.5989 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:09:24] 	Iter 304500 Done. | loss1: 0.5513 | loss_class: 0.5506 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:10:15] 	Iter 304600 Done. | loss1: 0.7574 | loss_class: 0.7569 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:11:06] 	Iter 304700 Done. | loss1: 0.6701 | loss_class: 0.6693 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|05:11:56] 	Iter 304800 Done. | loss1: 0.6274 | loss_class: 0.6266 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:12:47] 	Iter 304900 Done. | loss1: 0.7527 | loss_class: 0.7521 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:13:39] 	Iter 305000 Done. | loss1: 0.7647 | loss_class: 0.7640 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:14:29] 	Iter 305100 Done. | loss1: 0.9375 | loss_class: 0.9370 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:15:20] 	Iter 305200 Done. | loss1: 1.0362 | loss_class: 1.0357 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:16:10] 	Iter 305300 Done. | loss1: 0.1907 | loss_class: 0.1900 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:17:00] 	Iter 305400 Done. | loss1: 1.6297 | loss_class: 1.6292 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:17:50] 	Iter 305500 Done. | loss1: 1.3300 | loss_class: 1.3293 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:18:40] 	Iter 305600 Done. | loss1: 0.5013 | loss_class: 0.5008 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:19:31] 	Iter 305700 Done. | loss1: 1.5458 | loss_class: 1.5453 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:20:21] 	Iter 305800 Done. | loss1: 2.9405 | loss_class: 2.9397 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|05:21:11] 	Iter 305900 Done. | loss1: 1.1309 | loss_class: 1.1301 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|05:22:01] 	Iter 306000 Done. | loss1: 1.2565 | loss_class: 1.2556 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|05:22:52] 	Iter 306100 Done. | loss1: 0.8796 | loss_class: 0.8789 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:23:43] 	Iter 306200 Done. | loss1: 1.8240 | loss_class: 1.8235 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:24:33] 	Iter 306300 Done. | loss1: 0.2276 | loss_class: 0.2270 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:25:27] 	Iter 306400 Done. | loss1: 0.6315 | loss_class: 0.6308 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:26:18] 	Iter 306500 Done. | loss1: 0.8037 | loss_class: 0.8030 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:27:08] 	Iter 306600 Done. | loss1: 1.4400 | loss_class: 1.4392 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|05:28:02] 	Iter 306700 Done. | loss1: 1.2319 | loss_class: 1.2312 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|05:28:53] 	Iter 306800 Done. | loss1: 0.5555 | loss_class: 0.5548 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:29:44] 	Iter 306900 Done. | loss1: 0.1417 | loss_class: 0.1409 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:30:35] 	Iter 307000 Done. | loss1: 3.1364 | loss_class: 3.1355 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|05:31:25] 	Iter 307100 Done. | loss1: 0.8735 | loss_class: 0.8730 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:32:16] 	Iter 307200 Done. | loss1: 0.1932 | loss_class: 0.1926 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:33:06] 	Iter 307300 Done. | loss1: 0.2175 | loss_class: 0.2167 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|05:33:56] 	Iter 307400 Done. | loss1: 0.8747 | loss_class: 0.8740 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:34:46] 	Iter 307500 Done. | loss1: 1.5068 | loss_class: 1.5062 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:35:37] 	Iter 307600 Done. | loss1: 0.4453 | loss_class: 0.4445 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:36:28] 	Iter 307700 Done. | loss1: 1.2965 | loss_class: 1.2959 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:37:18] 	Iter 307800 Done. | loss1: 0.8544 | loss_class: 0.8539 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:38:09] 	Iter 307900 Done. | loss1: 0.6474 | loss_class: 0.6467 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:38:59] 	Iter 308000 Done. | loss1: 0.8025 | loss_class: 0.8016 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|05:39:49] 	Iter 308100 Done. | loss1: 0.3348 | loss_class: 0.3341 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:40:39] 	Iter 308200 Done. | loss1: 1.6682 | loss_class: 1.6675 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:41:30] 	Iter 308300 Done. | loss1: 0.6140 | loss_class: 0.6134 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:42:20] 	Iter 308400 Done. | loss1: 0.6627 | loss_class: 0.6619 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|05:43:11] 	Iter 308500 Done. | loss1: 1.7514 | loss_class: 1.7508 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:44:01] 	Iter 308600 Done. | loss1: 0.5468 | loss_class: 0.5462 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:44:52] 	Iter 308700 Done. | loss1: 1.3015 | loss_class: 1.3010 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:45:42] 	Iter 308800 Done. | loss1: 0.6556 | loss_class: 0.6550 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:46:32] 	Iter 308900 Done. | loss1: 0.4498 | loss_class: 0.4491 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:47:23] 	Iter 309000 Done. | loss1: 1.4480 | loss_class: 1.4475 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:48:13] 	Iter 309100 Done. | loss1: 1.2727 | loss_class: 1.2720 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:49:04] 	Iter 309200 Done. | loss1: 1.0909 | loss_class: 1.0904 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:49:55] 	Iter 309300 Done. | loss1: 0.3587 | loss_class: 0.3581 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:50:45] 	Iter 309400 Done. | loss1: 1.1953 | loss_class: 1.1946 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:51:35] 	Iter 309500 Done. | loss1: 1.3945 | loss_class: 1.3939 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:52:25] 	Iter 309600 Done. | loss1: 0.8708 | loss_class: 0.8700 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|05:53:16] 	Iter 309700 Done. | loss1: 0.0289 | loss_class: 0.0282 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|05:54:06] 	Iter 309800 Done. | loss1: 2.1138 | loss_class: 2.1132 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:54:56] 	Iter 309900 Done. | loss1: 2.9747 | loss_class: 2.9742 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:55:46] 	Iter 310000 Done. | loss1: 0.7680 | loss_class: 0.7673 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:56:36] 	Iter 310100 Done. | loss1: 0.5088 | loss_class: 0.5083 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:57:26] 	Iter 310200 Done. | loss1: 1.6255 | loss_class: 1.6249 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|05:58:16] 	Iter 310300 Done. | loss1: 1.4854 | loss_class: 1.4848 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|05:59:07] 	Iter 310400 Done. | loss1: 0.3314 | loss_class: 0.3307 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|05:59:57] 	Iter 310500 Done. | loss1: 0.4726 | loss_class: 0.4720 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:00:48] 	Iter 310600 Done. | loss1: 0.4450 | loss_class: 0.4445 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:01:29] 	mean_loss1: 1.152010858849931
[06.24.21|06:01:29] 	mean_loss_class: 1.1513638552246686
[06.24.21|06:01:29] 	mean_loss_recon: 0.0006470029343442214
[06.24.21|06:01:29] Time consumption:
[06.24.21|06:01:29] Done.
[06.24.21|06:01:29] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch40_model1.pt.
[06.24.21|06:01:29] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch40_model2.pt.
[06.24.21|06:01:29] Training epoch: 41
[06.24.21|06:01:39] 	Iter 310700 Done. | loss1: 0.5566 | loss_class: 0.5561 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:02:31] 	Iter 310800 Done. | loss1: 0.3828 | loss_class: 0.3822 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:03:22] 	Iter 310900 Done. | loss1: 0.7223 | loss_class: 0.7214 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|06:04:13] 	Iter 311000 Done. | loss1: 0.1880 | loss_class: 0.1874 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:05:03] 	Iter 311100 Done. | loss1: 2.0206 | loss_class: 2.0196 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|06:05:54] 	Iter 311200 Done. | loss1: 1.0834 | loss_class: 1.0825 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|06:06:44] 	Iter 311300 Done. | loss1: 4.8798 | loss_class: 4.8793 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:07:34] 	Iter 311400 Done. | loss1: 1.1445 | loss_class: 1.1438 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|06:08:23] 	Iter 311500 Done. | loss1: 1.3510 | loss_class: 1.3504 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|06:09:13] 	Iter 311600 Done. | loss1: 0.5442 | loss_class: 0.5434 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|06:10:04] 	Iter 311700 Done. | loss1: 1.2627 | loss_class: 1.2618 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|06:10:56] 	Iter 311800 Done. | loss1: 1.4712 | loss_class: 1.4707 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|06:11:48] 	Iter 311900 Done. | loss1: 2.2763 | loss_class: 2.2757 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:12:39] 	Iter 312000 Done. | loss1: 0.3547 | loss_class: 0.3541 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:13:28] 	Iter 312100 Done. | loss1: 2.5239 | loss_class: 2.5228 | loss_recon: 0.0011 | lr: 0.100000
[06.24.21|06:14:18] 	Iter 312200 Done. | loss1: 1.2591 | loss_class: 1.2585 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:15:08] 	Iter 312300 Done. | loss1: 0.4351 | loss_class: 0.4341 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|06:15:58] 	Iter 312400 Done. | loss1: 0.5889 | loss_class: 0.5882 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|06:16:48] 	Iter 312500 Done. | loss1: 1.6963 | loss_class: 1.6958 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:17:42] 	Iter 312600 Done. | loss1: 0.3594 | loss_class: 0.3588 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:18:34] 	Iter 312700 Done. | loss1: 0.7630 | loss_class: 0.7624 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:19:25] 	Iter 312800 Done. | loss1: 1.1389 | loss_class: 1.1384 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:20:17] 	Iter 312900 Done. | loss1: 2.6935 | loss_class: 2.6929 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:21:07] 	Iter 313000 Done. | loss1: 0.9193 | loss_class: 0.9188 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:21:58] 	Iter 313100 Done. | loss1: 0.4441 | loss_class: 0.4436 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:22:48] 	Iter 313200 Done. | loss1: 1.4100 | loss_class: 1.4094 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:23:41] 	Iter 313300 Done. | loss1: 1.0827 | loss_class: 1.0821 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:24:36] 	Iter 313400 Done. | loss1: 1.4756 | loss_class: 1.4751 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:25:27] 	Iter 313500 Done. | loss1: 0.6394 | loss_class: 0.6387 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|06:26:18] 	Iter 313600 Done. | loss1: 0.5872 | loss_class: 0.5867 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:27:09] 	Iter 313700 Done. | loss1: 2.5779 | loss_class: 2.5773 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:27:59] 	Iter 313800 Done. | loss1: 1.0161 | loss_class: 1.0156 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:28:49] 	Iter 313900 Done. | loss1: 0.1867 | loss_class: 0.1860 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:29:39] 	Iter 314000 Done. | loss1: 1.0511 | loss_class: 1.0506 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:30:28] 	Iter 314100 Done. | loss1: 0.2904 | loss_class: 0.2899 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:31:18] 	Iter 314200 Done. | loss1: 0.4428 | loss_class: 0.4421 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|06:32:08] 	Iter 314300 Done. | loss1: 0.0216 | loss_class: 0.0211 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:32:58] 	Iter 314400 Done. | loss1: 0.4041 | loss_class: 0.4032 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|06:33:47] 	Iter 314500 Done. | loss1: 0.8719 | loss_class: 0.8713 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:34:38] 	Iter 314600 Done. | loss1: 1.1674 | loss_class: 1.1668 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:35:32] 	Iter 314700 Done. | loss1: 0.7150 | loss_class: 0.7143 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|06:36:22] 	Iter 314800 Done. | loss1: 1.4778 | loss_class: 1.4774 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|06:37:13] 	Iter 314900 Done. | loss1: 2.0978 | loss_class: 2.0971 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|06:38:03] 	Iter 315000 Done. | loss1: 1.9985 | loss_class: 1.9980 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:38:53] 	Iter 315100 Done. | loss1: 1.3405 | loss_class: 1.3399 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:39:44] 	Iter 315200 Done. | loss1: 0.4155 | loss_class: 0.4150 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:40:34] 	Iter 315300 Done. | loss1: 0.9100 | loss_class: 0.9092 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|06:41:26] 	Iter 315400 Done. | loss1: 0.0452 | loss_class: 0.0444 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|06:42:19] 	Iter 315500 Done. | loss1: 0.5146 | loss_class: 0.5141 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|06:43:09] 	Iter 315600 Done. | loss1: 1.0870 | loss_class: 1.0863 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|06:44:00] 	Iter 315700 Done. | loss1: 1.0525 | loss_class: 1.0517 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|06:44:55] 	Iter 315800 Done. | loss1: 0.6646 | loss_class: 0.6638 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|06:45:50] 	Iter 315900 Done. | loss1: 0.6067 | loss_class: 0.6060 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|06:46:41] 	Iter 316000 Done. | loss1: 0.2614 | loss_class: 0.2608 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:47:31] 	Iter 316100 Done. | loss1: 1.2471 | loss_class: 1.2463 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|06:48:21] 	Iter 316200 Done. | loss1: 0.2072 | loss_class: 0.2064 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|06:49:11] 	Iter 316300 Done. | loss1: 0.4402 | loss_class: 0.4394 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|06:50:01] 	Iter 316400 Done. | loss1: 1.1842 | loss_class: 1.1836 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:50:51] 	Iter 316500 Done. | loss1: 1.6267 | loss_class: 1.6261 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:51:41] 	Iter 316600 Done. | loss1: 0.5529 | loss_class: 0.5521 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|06:52:30] 	Iter 316700 Done. | loss1: 1.5975 | loss_class: 1.5967 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|06:53:20] 	Iter 316800 Done. | loss1: 0.4276 | loss_class: 0.4252 | loss_recon: 0.0024 | lr: 0.100000
[06.24.21|06:54:10] 	Iter 316900 Done. | loss1: 0.2585 | loss_class: 0.2578 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|06:55:00] 	Iter 317000 Done. | loss1: 0.4807 | loss_class: 0.4800 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|06:55:50] 	Iter 317100 Done. | loss1: 1.2079 | loss_class: 1.2073 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:56:39] 	Iter 317200 Done. | loss1: 2.1641 | loss_class: 2.1635 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:57:29] 	Iter 317300 Done. | loss1: 2.6682 | loss_class: 2.6676 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:58:19] 	Iter 317400 Done. | loss1: 0.7420 | loss_class: 0.7414 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|06:59:10] 	Iter 317500 Done. | loss1: 0.3237 | loss_class: 0.3231 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:00:00] 	Iter 317600 Done. | loss1: 2.0037 | loss_class: 2.0030 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:00:50] 	Iter 317700 Done. | loss1: 0.5327 | loss_class: 0.5323 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:01:40] 	Iter 317800 Done. | loss1: 1.0591 | loss_class: 1.0585 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:02:31] 	Iter 317900 Done. | loss1: 0.2899 | loss_class: 0.2894 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:03:22] 	Iter 318000 Done. | loss1: 1.5917 | loss_class: 1.5912 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:04:12] 	Iter 318100 Done. | loss1: 0.9168 | loss_class: 0.9163 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:05:02] 	Iter 318200 Done. | loss1: 0.4492 | loss_class: 0.4485 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|07:05:52] 	Iter 318300 Done. | loss1: 0.8275 | loss_class: 0.8269 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:06:42] 	Iter 318400 Done. | loss1: 0.7604 | loss_class: 0.7598 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:07:33] 	Iter 318500 Done. | loss1: 1.1845 | loss_class: 1.1840 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:08:23] 	Iter 318600 Done. | loss1: 1.1726 | loss_class: 1.1718 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|07:09:13] 	Iter 318700 Done. | loss1: 1.0315 | loss_class: 1.0309 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:10:03] 	Iter 318800 Done. | loss1: 1.4400 | loss_class: 1.4394 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:10:53] 	Iter 318900 Done. | loss1: 0.8958 | loss_class: 0.8945 | loss_recon: 0.0012 | lr: 0.100000
[06.24.21|07:11:44] 	Iter 319000 Done. | loss1: 0.8060 | loss_class: 0.8055 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:12:34] 	Iter 319100 Done. | loss1: 1.2110 | loss_class: 1.2104 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:13:25] 	Iter 319200 Done. | loss1: 1.7937 | loss_class: 1.7930 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:14:15] 	Iter 319300 Done. | loss1: 1.5238 | loss_class: 1.5231 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:15:05] 	Iter 319400 Done. | loss1: 0.7821 | loss_class: 0.7816 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:15:55] 	Iter 319500 Done. | loss1: 3.0713 | loss_class: 3.0708 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:16:45] 	Iter 319600 Done. | loss1: 1.8010 | loss_class: 1.8004 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:17:36] 	Iter 319700 Done. | loss1: 0.6516 | loss_class: 0.6509 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|07:18:26] 	Iter 319800 Done. | loss1: 1.0773 | loss_class: 1.0767 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:19:16] 	Iter 319900 Done. | loss1: 3.3515 | loss_class: 3.3509 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:20:06] 	Iter 320000 Done. | loss1: 0.0761 | loss_class: 0.0753 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|07:20:57] 	Iter 320100 Done. | loss1: 0.9871 | loss_class: 0.9862 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|07:21:47] 	Iter 320200 Done. | loss1: 2.4347 | loss_class: 2.4340 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|07:22:38] 	Iter 320300 Done. | loss1: 1.6642 | loss_class: 1.6636 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:23:29] 	Iter 320400 Done. | loss1: 0.3872 | loss_class: 0.3866 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:24:19] 	Iter 320500 Done. | loss1: 0.1072 | loss_class: 0.1067 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|07:25:09] 	Iter 320600 Done. | loss1: 2.9147 | loss_class: 2.9141 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:26:01] 	Iter 320700 Done. | loss1: 2.3142 | loss_class: 2.3137 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:26:02] 	mean_loss1: 1.1532904589772663
[06.24.21|07:26:02] 	mean_loss_class: 1.1526426881143976
[06.24.21|07:26:02] 	mean_loss_recon: 0.0006477708898959011
[06.24.21|07:26:02] Time consumption:
[06.24.21|07:26:02] Done.
[06.24.21|07:26:02] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch41_model1.pt.
[06.24.21|07:26:02] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch41_model2.pt.
[06.24.21|07:26:02] Training epoch: 42
[06.24.21|07:26:51] 	Iter 320800 Done. | loss1: 0.5939 | loss_class: 0.5933 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:27:41] 	Iter 320900 Done. | loss1: 2.1256 | loss_class: 2.1246 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|07:28:31] 	Iter 321000 Done. | loss1: 0.8927 | loss_class: 0.8921 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:29:21] 	Iter 321100 Done. | loss1: 1.4219 | loss_class: 1.4213 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:30:11] 	Iter 321200 Done. | loss1: 2.3055 | loss_class: 2.3051 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:31:01] 	Iter 321300 Done. | loss1: 0.8450 | loss_class: 0.8442 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|07:31:51] 	Iter 321400 Done. | loss1: 0.5856 | loss_class: 0.5850 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:32:41] 	Iter 321500 Done. | loss1: 1.2254 | loss_class: 1.2248 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:33:31] 	Iter 321600 Done. | loss1: 1.7035 | loss_class: 1.7029 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:34:21] 	Iter 321700 Done. | loss1: 1.9634 | loss_class: 1.9629 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:35:11] 	Iter 321800 Done. | loss1: 0.6718 | loss_class: 0.6710 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|07:36:01] 	Iter 321900 Done. | loss1: 1.9407 | loss_class: 1.9402 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:36:51] 	Iter 322000 Done. | loss1: 0.0779 | loss_class: 0.0773 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:37:41] 	Iter 322100 Done. | loss1: 1.3926 | loss_class: 1.3919 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|07:38:31] 	Iter 322200 Done. | loss1: 0.9697 | loss_class: 0.9689 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|07:39:22] 	Iter 322300 Done. | loss1: 1.0512 | loss_class: 1.0506 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:40:13] 	Iter 322400 Done. | loss1: 0.1708 | loss_class: 0.1702 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:41:08] 	Iter 322500 Done. | loss1: 2.5505 | loss_class: 2.5500 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:41:59] 	Iter 322600 Done. | loss1: 1.0180 | loss_class: 1.0173 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|07:42:49] 	Iter 322700 Done. | loss1: 0.9657 | loss_class: 0.9651 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:43:39] 	Iter 322800 Done. | loss1: 1.4531 | loss_class: 1.4523 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|07:44:30] 	Iter 322900 Done. | loss1: 0.6786 | loss_class: 0.6779 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|07:45:20] 	Iter 323000 Done. | loss1: 0.1485 | loss_class: 0.1477 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|07:46:10] 	Iter 323100 Done. | loss1: 2.6592 | loss_class: 2.6587 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|07:47:00] 	Iter 323200 Done. | loss1: 0.8416 | loss_class: 0.8410 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:47:50] 	Iter 323300 Done. | loss1: 2.9273 | loss_class: 2.9267 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:48:40] 	Iter 323400 Done. | loss1: 1.9861 | loss_class: 1.9854 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|07:49:30] 	Iter 323500 Done. | loss1: 1.3684 | loss_class: 1.3675 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|07:50:20] 	Iter 323600 Done. | loss1: 1.3479 | loss_class: 1.3475 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|07:51:11] 	Iter 323700 Done. | loss1: 1.6080 | loss_class: 1.6072 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|07:52:01] 	Iter 323800 Done. | loss1: 1.6436 | loss_class: 1.6430 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:52:51] 	Iter 323900 Done. | loss1: 1.9132 | loss_class: 1.9126 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:53:41] 	Iter 324000 Done. | loss1: 1.7114 | loss_class: 1.7110 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|07:54:33] 	Iter 324100 Done. | loss1: 2.1495 | loss_class: 2.1489 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|07:55:27] 	Iter 324200 Done. | loss1: 1.0961 | loss_class: 1.0955 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:56:17] 	Iter 324300 Done. | loss1: 2.9916 | loss_class: 2.9910 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:57:07] 	Iter 324400 Done. | loss1: 0.6581 | loss_class: 0.6573 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|07:57:58] 	Iter 324500 Done. | loss1: 1.0583 | loss_class: 1.0575 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|07:58:50] 	Iter 324600 Done. | loss1: 1.7810 | loss_class: 1.7804 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|07:59:41] 	Iter 324700 Done. | loss1: 2.5364 | loss_class: 2.5357 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:00:31] 	Iter 324800 Done. | loss1: 0.4823 | loss_class: 0.4815 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:01:22] 	Iter 324900 Done. | loss1: 0.3379 | loss_class: 0.3372 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:02:12] 	Iter 325000 Done. | loss1: 1.5709 | loss_class: 1.5703 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:03:03] 	Iter 325100 Done. | loss1: 1.3405 | loss_class: 1.3399 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:03:53] 	Iter 325200 Done. | loss1: 1.2297 | loss_class: 1.2290 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:04:44] 	Iter 325300 Done. | loss1: 3.7753 | loss_class: 3.7747 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:05:34] 	Iter 325400 Done. | loss1: 0.2123 | loss_class: 0.2118 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:06:24] 	Iter 325500 Done. | loss1: 2.1485 | loss_class: 2.1476 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|08:07:15] 	Iter 325600 Done. | loss1: 0.8274 | loss_class: 0.8269 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:08:05] 	Iter 325700 Done. | loss1: 1.9300 | loss_class: 1.9293 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|08:08:56] 	Iter 325800 Done. | loss1: 0.5401 | loss_class: 0.5395 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:09:46] 	Iter 325900 Done. | loss1: 1.2303 | loss_class: 1.2296 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:10:37] 	Iter 326000 Done. | loss1: 0.4699 | loss_class: 0.4693 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:11:27] 	Iter 326100 Done. | loss1: 0.3680 | loss_class: 0.3673 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:12:22] 	Iter 326200 Done. | loss1: 0.7491 | loss_class: 0.7482 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|08:13:12] 	Iter 326300 Done. | loss1: 1.5231 | loss_class: 1.5226 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:14:04] 	Iter 326400 Done. | loss1: 1.0669 | loss_class: 1.0664 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:14:55] 	Iter 326500 Done. | loss1: 1.5120 | loss_class: 1.5115 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:15:45] 	Iter 326600 Done. | loss1: 0.2130 | loss_class: 0.2124 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:16:36] 	Iter 326700 Done. | loss1: 0.7881 | loss_class: 0.7876 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:17:26] 	Iter 326800 Done. | loss1: 2.4107 | loss_class: 2.4101 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:18:17] 	Iter 326900 Done. | loss1: 0.3187 | loss_class: 0.3179 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|08:19:07] 	Iter 327000 Done. | loss1: 0.2084 | loss_class: 0.2079 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:19:58] 	Iter 327100 Done. | loss1: 1.1872 | loss_class: 1.1867 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:20:49] 	Iter 327200 Done. | loss1: 2.3348 | loss_class: 2.3342 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:21:39] 	Iter 327300 Done. | loss1: 0.4359 | loss_class: 0.4353 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:22:32] 	Iter 327400 Done. | loss1: 1.1248 | loss_class: 1.1242 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:23:22] 	Iter 327500 Done. | loss1: 0.4776 | loss_class: 0.4769 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:24:13] 	Iter 327600 Done. | loss1: 0.6626 | loss_class: 0.6620 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:25:03] 	Iter 327700 Done. | loss1: 1.0736 | loss_class: 1.0729 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:25:53] 	Iter 327800 Done. | loss1: 0.5669 | loss_class: 0.5663 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:26:43] 	Iter 327900 Done. | loss1: 0.9308 | loss_class: 0.9303 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:27:33] 	Iter 328000 Done. | loss1: 0.9709 | loss_class: 0.9703 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:28:23] 	Iter 328100 Done. | loss1: 1.3162 | loss_class: 1.3156 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:29:13] 	Iter 328200 Done. | loss1: 0.8868 | loss_class: 0.8861 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:30:04] 	Iter 328300 Done. | loss1: 0.6734 | loss_class: 0.6727 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:30:54] 	Iter 328400 Done. | loss1: 2.1260 | loss_class: 2.1252 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|08:31:45] 	Iter 328500 Done. | loss1: 2.5518 | loss_class: 2.5513 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:32:40] 	Iter 328600 Done. | loss1: 0.5547 | loss_class: 0.5542 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|08:33:31] 	Iter 328700 Done. | loss1: 1.2361 | loss_class: 1.2355 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:34:22] 	Iter 328800 Done. | loss1: 1.2295 | loss_class: 1.2290 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:35:13] 	Iter 328900 Done. | loss1: 0.9380 | loss_class: 0.9375 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:36:04] 	Iter 329000 Done. | loss1: 0.9538 | loss_class: 0.9531 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:36:55] 	Iter 329100 Done. | loss1: 2.7707 | loss_class: 2.7699 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|08:37:45] 	Iter 329200 Done. | loss1: 0.2695 | loss_class: 0.2690 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:38:35] 	Iter 329300 Done. | loss1: 1.0294 | loss_class: 1.0288 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:39:25] 	Iter 329400 Done. | loss1: 0.9154 | loss_class: 0.9148 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:40:15] 	Iter 329500 Done. | loss1: 0.3513 | loss_class: 0.3506 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:41:10] 	Iter 329600 Done. | loss1: 0.6805 | loss_class: 0.6798 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:42:01] 	Iter 329700 Done. | loss1: 0.7433 | loss_class: 0.7429 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:42:53] 	Iter 329800 Done. | loss1: 1.6181 | loss_class: 1.6171 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|08:43:44] 	Iter 329900 Done. | loss1: 0.8959 | loss_class: 0.8952 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:44:35] 	Iter 330000 Done. | loss1: 1.2838 | loss_class: 1.2833 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:45:26] 	Iter 330100 Done. | loss1: 0.1063 | loss_class: 0.1058 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:46:17] 	Iter 330200 Done. | loss1: 3.0126 | loss_class: 3.0121 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:47:07] 	Iter 330300 Done. | loss1: 2.2945 | loss_class: 2.2939 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:47:58] 	Iter 330400 Done. | loss1: 0.4369 | loss_class: 0.4363 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:48:48] 	Iter 330500 Done. | loss1: 1.5905 | loss_class: 1.5897 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|08:49:37] 	Iter 330600 Done. | loss1: 0.2798 | loss_class: 0.2791 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|08:50:26] 	Iter 330700 Done. | loss1: 0.7102 | loss_class: 0.7097 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:50:39] 	mean_loss1: 1.1443920890718229
[06.24.21|08:50:39] 	mean_loss_class: 1.1437454262777957
[06.24.21|08:50:39] 	mean_loss_recon: 0.0006466626812897383
[06.24.21|08:50:39] Time consumption:
[06.24.21|08:50:39] Done.
[06.24.21|08:50:39] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch42_model1.pt.
[06.24.21|08:50:39] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch42_model2.pt.
[06.24.21|08:50:39] Training epoch: 43
[06.24.21|08:51:17] 	Iter 330800 Done. | loss1: 1.1925 | loss_class: 1.1918 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:52:07] 	Iter 330900 Done. | loss1: 1.9953 | loss_class: 1.9947 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:52:59] 	Iter 331000 Done. | loss1: 0.4956 | loss_class: 0.4950 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:53:53] 	Iter 331100 Done. | loss1: 1.7024 | loss_class: 1.7017 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:54:43] 	Iter 331200 Done. | loss1: 1.1327 | loss_class: 1.1321 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:55:32] 	Iter 331300 Done. | loss1: 0.7810 | loss_class: 0.7801 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|08:56:22] 	Iter 331400 Done. | loss1: 1.6369 | loss_class: 1.6363 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|08:57:13] 	Iter 331500 Done. | loss1: 0.6477 | loss_class: 0.6470 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:58:03] 	Iter 331600 Done. | loss1: 1.4105 | loss_class: 1.4099 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|08:58:53] 	Iter 331700 Done. | loss1: 0.9522 | loss_class: 0.9515 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|08:59:43] 	Iter 331800 Done. | loss1: 0.7282 | loss_class: 0.7273 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|09:00:32] 	Iter 331900 Done. | loss1: 0.9275 | loss_class: 0.9269 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:01:23] 	Iter 332000 Done. | loss1: 0.6144 | loss_class: 0.6136 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|09:02:13] 	Iter 332100 Done. | loss1: 0.3969 | loss_class: 0.3962 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:03:03] 	Iter 332200 Done. | loss1: 0.1197 | loss_class: 0.1188 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|09:03:59] 	Iter 332300 Done. | loss1: 0.6053 | loss_class: 0.6047 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:04:53] 	Iter 332400 Done. | loss1: 2.1482 | loss_class: 2.1477 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|09:05:44] 	Iter 332500 Done. | loss1: 0.6390 | loss_class: 0.6385 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:06:34] 	Iter 332600 Done. | loss1: 0.9766 | loss_class: 0.9758 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|09:07:26] 	Iter 332700 Done. | loss1: 1.5263 | loss_class: 1.5256 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:08:16] 	Iter 332800 Done. | loss1: 0.5853 | loss_class: 0.5845 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|09:09:06] 	Iter 332900 Done. | loss1: 0.6427 | loss_class: 0.6420 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:09:56] 	Iter 333000 Done. | loss1: 2.0207 | loss_class: 2.0198 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|09:10:47] 	Iter 333100 Done. | loss1: 1.9000 | loss_class: 1.8996 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|09:11:36] 	Iter 333200 Done. | loss1: 0.9397 | loss_class: 0.9391 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:12:26] 	Iter 333300 Done. | loss1: 1.5297 | loss_class: 1.5292 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:13:16] 	Iter 333400 Done. | loss1: 3.4546 | loss_class: 3.4539 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:14:06] 	Iter 333500 Done. | loss1: 0.7598 | loss_class: 0.7591 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:14:57] 	Iter 333600 Done. | loss1: 1.9374 | loss_class: 1.9367 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:15:47] 	Iter 333700 Done. | loss1: 1.4755 | loss_class: 1.4750 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:16:37] 	Iter 333800 Done. | loss1: 0.5730 | loss_class: 0.5723 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:17:29] 	Iter 333900 Done. | loss1: 0.6113 | loss_class: 0.6105 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|09:18:21] 	Iter 334000 Done. | loss1: 5.1254 | loss_class: 5.1247 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:19:14] 	Iter 334100 Done. | loss1: 1.3607 | loss_class: 1.3601 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:20:04] 	Iter 334200 Done. | loss1: 2.0025 | loss_class: 2.0020 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:20:54] 	Iter 334300 Done. | loss1: 1.1554 | loss_class: 1.1548 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:21:44] 	Iter 334400 Done. | loss1: 0.6062 | loss_class: 0.6055 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:22:34] 	Iter 334500 Done. | loss1: 0.5323 | loss_class: 0.5317 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:23:23] 	Iter 334600 Done. | loss1: 1.6866 | loss_class: 1.6859 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:24:13] 	Iter 334700 Done. | loss1: 1.4177 | loss_class: 1.4173 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:25:07] 	Iter 334800 Done. | loss1: 1.8811 | loss_class: 1.8807 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:25:57] 	Iter 334900 Done. | loss1: 1.3298 | loss_class: 1.3292 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:26:48] 	Iter 335000 Done. | loss1: 0.6210 | loss_class: 0.6203 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:27:38] 	Iter 335100 Done. | loss1: 1.8138 | loss_class: 1.8132 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:28:27] 	Iter 335200 Done. | loss1: 0.4528 | loss_class: 0.4523 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:29:17] 	Iter 335300 Done. | loss1: 1.4508 | loss_class: 1.4501 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:30:07] 	Iter 335400 Done. | loss1: 0.9992 | loss_class: 0.9983 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|09:30:57] 	Iter 335500 Done. | loss1: 0.6086 | loss_class: 0.6078 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:31:47] 	Iter 335600 Done. | loss1: 0.2712 | loss_class: 0.2704 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:32:37] 	Iter 335700 Done. | loss1: 1.0393 | loss_class: 1.0388 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:33:27] 	Iter 335800 Done. | loss1: 0.5640 | loss_class: 0.5634 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:34:17] 	Iter 335900 Done. | loss1: 1.7098 | loss_class: 1.7091 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:35:07] 	Iter 336000 Done. | loss1: 0.8793 | loss_class: 0.8785 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|09:35:57] 	Iter 336100 Done. | loss1: 2.4044 | loss_class: 2.4037 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:36:47] 	Iter 336200 Done. | loss1: 1.0269 | loss_class: 1.0263 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:37:39] 	Iter 336300 Done. | loss1: 2.8762 | loss_class: 2.8756 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:38:30] 	Iter 336400 Done. | loss1: 0.4965 | loss_class: 0.4957 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:39:19] 	Iter 336500 Done. | loss1: 1.2540 | loss_class: 1.2532 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|09:40:09] 	Iter 336600 Done. | loss1: 0.4420 | loss_class: 0.4415 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:40:59] 	Iter 336700 Done. | loss1: 1.5418 | loss_class: 1.5412 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:41:51] 	Iter 336800 Done. | loss1: 0.3536 | loss_class: 0.3529 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:42:43] 	Iter 336900 Done. | loss1: 0.9726 | loss_class: 0.9720 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:43:33] 	Iter 337000 Done. | loss1: 1.3297 | loss_class: 1.3292 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:44:24] 	Iter 337100 Done. | loss1: 0.8028 | loss_class: 0.8022 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:45:14] 	Iter 337200 Done. | loss1: 1.2806 | loss_class: 1.2799 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:46:06] 	Iter 337300 Done. | loss1: 0.3183 | loss_class: 0.3178 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:46:58] 	Iter 337400 Done. | loss1: 0.4066 | loss_class: 0.4061 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|09:47:48] 	Iter 337500 Done. | loss1: 0.6121 | loss_class: 0.6114 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:48:38] 	Iter 337600 Done. | loss1: 1.4343 | loss_class: 1.4338 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:49:28] 	Iter 337700 Done. | loss1: 0.2006 | loss_class: 0.2000 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:50:18] 	Iter 337800 Done. | loss1: 1.5709 | loss_class: 1.5703 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:51:08] 	Iter 337900 Done. | loss1: 0.7681 | loss_class: 0.7674 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|09:51:57] 	Iter 338000 Done. | loss1: 2.1045 | loss_class: 2.1039 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|09:52:47] 	Iter 338100 Done. | loss1: 0.5031 | loss_class: 0.5027 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:53:37] 	Iter 338200 Done. | loss1: 0.8088 | loss_class: 0.8083 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|09:54:27] 	Iter 338300 Done. | loss1: 2.3034 | loss_class: 2.3028 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:55:17] 	Iter 338400 Done. | loss1: 0.4883 | loss_class: 0.4878 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|09:56:07] 	Iter 338500 Done. | loss1: 1.6802 | loss_class: 1.6796 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:56:57] 	Iter 338600 Done. | loss1: 0.2654 | loss_class: 0.2648 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:57:47] 	Iter 338700 Done. | loss1: 2.5347 | loss_class: 2.5340 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|09:58:38] 	Iter 338800 Done. | loss1: 1.4201 | loss_class: 1.4195 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|09:59:28] 	Iter 338900 Done. | loss1: 0.3563 | loss_class: 0.3555 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|10:00:18] 	Iter 339000 Done. | loss1: 0.5999 | loss_class: 0.5994 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:01:08] 	Iter 339100 Done. | loss1: 1.6677 | loss_class: 1.6671 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:01:58] 	Iter 339200 Done. | loss1: 0.6018 | loss_class: 0.6012 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:02:48] 	Iter 339300 Done. | loss1: 1.0894 | loss_class: 1.0888 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:03:38] 	Iter 339400 Done. | loss1: 1.0231 | loss_class: 1.0223 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|10:04:28] 	Iter 339500 Done. | loss1: 0.5507 | loss_class: 0.5502 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:05:19] 	Iter 339600 Done. | loss1: 0.8144 | loss_class: 0.8138 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:06:10] 	Iter 339700 Done. | loss1: 1.2400 | loss_class: 1.2395 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:07:00] 	Iter 339800 Done. | loss1: 1.3960 | loss_class: 1.3953 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:07:51] 	Iter 339900 Done. | loss1: 1.8296 | loss_class: 1.8290 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:08:42] 	Iter 340000 Done. | loss1: 1.4910 | loss_class: 1.4903 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|10:09:32] 	Iter 340100 Done. | loss1: 0.7865 | loss_class: 0.7859 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:10:23] 	Iter 340200 Done. | loss1: 1.6784 | loss_class: 1.6777 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:11:13] 	Iter 340300 Done. | loss1: 1.2106 | loss_class: 1.2101 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:12:04] 	Iter 340400 Done. | loss1: 0.3992 | loss_class: 0.3986 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:12:55] 	Iter 340500 Done. | loss1: 2.3389 | loss_class: 2.3384 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:13:48] 	Iter 340600 Done. | loss1: 1.7175 | loss_class: 1.7170 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:14:38] 	Iter 340700 Done. | loss1: 0.9524 | loss_class: 0.9518 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:15:02] 	mean_loss1: 1.145655486245036
[06.24.21|10:15:02] 	mean_loss_class: 1.14500821955316
[06.24.21|10:15:02] 	mean_loss_recon: 0.0006472667601911863
[06.24.21|10:15:02] Time consumption:
[06.24.21|10:15:02] Done.
[06.24.21|10:15:02] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch43_model1.pt.
[06.24.21|10:15:02] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch43_model2.pt.
[06.24.21|10:15:02] Training epoch: 44
[06.24.21|10:15:30] 	Iter 340800 Done. | loss1: 1.9286 | loss_class: 1.9280 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:16:20] 	Iter 340900 Done. | loss1: 0.3501 | loss_class: 0.3495 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:17:10] 	Iter 341000 Done. | loss1: 1.9393 | loss_class: 1.9387 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:18:00] 	Iter 341100 Done. | loss1: 0.4809 | loss_class: 0.4802 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:18:49] 	Iter 341200 Done. | loss1: 0.7214 | loss_class: 0.7206 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|10:19:39] 	Iter 341300 Done. | loss1: 1.3860 | loss_class: 1.3855 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:20:29] 	Iter 341400 Done. | loss1: 1.0005 | loss_class: 0.9998 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:21:19] 	Iter 341500 Done. | loss1: 1.9138 | loss_class: 1.9132 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:22:08] 	Iter 341600 Done. | loss1: 0.5178 | loss_class: 0.5172 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:22:57] 	Iter 341700 Done. | loss1: 1.5111 | loss_class: 1.5103 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|10:23:47] 	Iter 341800 Done. | loss1: 1.6545 | loss_class: 1.6539 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:24:37] 	Iter 341900 Done. | loss1: 1.8161 | loss_class: 1.8154 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:25:27] 	Iter 342000 Done. | loss1: 1.4700 | loss_class: 1.4694 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:26:16] 	Iter 342100 Done. | loss1: 3.2882 | loss_class: 3.2876 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:27:07] 	Iter 342200 Done. | loss1: 0.2870 | loss_class: 0.2861 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|10:27:57] 	Iter 342300 Done. | loss1: 2.5673 | loss_class: 2.5666 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:28:47] 	Iter 342400 Done. | loss1: 0.8113 | loss_class: 0.8108 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:29:37] 	Iter 342500 Done. | loss1: 2.0160 | loss_class: 2.0152 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|10:30:27] 	Iter 342600 Done. | loss1: 0.3023 | loss_class: 0.3018 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:31:17] 	Iter 342700 Done. | loss1: 0.9721 | loss_class: 0.9716 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:32:07] 	Iter 342800 Done. | loss1: 1.0399 | loss_class: 1.0392 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:32:57] 	Iter 342900 Done. | loss1: 0.9230 | loss_class: 0.9222 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|10:33:47] 	Iter 343000 Done. | loss1: 1.0226 | loss_class: 1.0221 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:34:38] 	Iter 343100 Done. | loss1: 2.8147 | loss_class: 2.8142 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:35:28] 	Iter 343200 Done. | loss1: 0.9622 | loss_class: 0.9617 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:36:19] 	Iter 343300 Done. | loss1: 2.0813 | loss_class: 2.0808 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:37:09] 	Iter 343400 Done. | loss1: 1.0508 | loss_class: 1.0501 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:37:59] 	Iter 343500 Done. | loss1: 1.5785 | loss_class: 1.5779 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:38:50] 	Iter 343600 Done. | loss1: 0.4397 | loss_class: 0.4390 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|10:39:40] 	Iter 343700 Done. | loss1: 1.4053 | loss_class: 1.4047 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:40:30] 	Iter 343800 Done. | loss1: 0.0536 | loss_class: 0.0528 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:41:21] 	Iter 343900 Done. | loss1: 0.3813 | loss_class: 0.3803 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|10:42:11] 	Iter 344000 Done. | loss1: 0.8886 | loss_class: 0.8880 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|10:43:01] 	Iter 344100 Done. | loss1: 0.2503 | loss_class: 0.2497 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:43:52] 	Iter 344200 Done. | loss1: 1.3805 | loss_class: 1.3799 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:44:42] 	Iter 344300 Done. | loss1: 1.3816 | loss_class: 1.3809 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:45:32] 	Iter 344400 Done. | loss1: 0.5457 | loss_class: 0.5451 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:46:23] 	Iter 344500 Done. | loss1: 1.9420 | loss_class: 1.9411 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|10:47:13] 	Iter 344600 Done. | loss1: 2.6653 | loss_class: 2.6646 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:48:03] 	Iter 344700 Done. | loss1: 1.2955 | loss_class: 1.2945 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|10:48:53] 	Iter 344800 Done. | loss1: 1.3697 | loss_class: 1.3692 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|10:49:44] 	Iter 344900 Done. | loss1: 1.3419 | loss_class: 1.3411 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:50:36] 	Iter 345000 Done. | loss1: 0.2684 | loss_class: 0.2676 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|10:51:27] 	Iter 345100 Done. | loss1: 1.1911 | loss_class: 1.1904 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:52:17] 	Iter 345200 Done. | loss1: 1.8635 | loss_class: 1.8628 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:53:08] 	Iter 345300 Done. | loss1: 0.7599 | loss_class: 0.7593 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:53:58] 	Iter 345400 Done. | loss1: 0.3790 | loss_class: 0.3783 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:54:48] 	Iter 345500 Done. | loss1: 1.7749 | loss_class: 1.7745 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|10:55:38] 	Iter 345600 Done. | loss1: 1.7365 | loss_class: 1.7358 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:56:29] 	Iter 345700 Done. | loss1: 2.5697 | loss_class: 2.5688 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|10:57:19] 	Iter 345800 Done. | loss1: 0.1831 | loss_class: 0.1824 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:58:09] 	Iter 345900 Done. | loss1: 0.7120 | loss_class: 0.7113 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|10:59:00] 	Iter 346000 Done. | loss1: 2.1701 | loss_class: 2.1694 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|10:59:50] 	Iter 346100 Done. | loss1: 0.2673 | loss_class: 0.2668 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:00:40] 	Iter 346200 Done. | loss1: 0.2298 | loss_class: 0.2292 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:01:31] 	Iter 346300 Done. | loss1: 0.0583 | loss_class: 0.0576 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:02:21] 	Iter 346400 Done. | loss1: 1.2939 | loss_class: 1.2932 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|11:03:11] 	Iter 346500 Done. | loss1: 0.6772 | loss_class: 0.6767 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:04:01] 	Iter 346600 Done. | loss1: 0.7770 | loss_class: 0.7765 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:04:52] 	Iter 346700 Done. | loss1: 0.2595 | loss_class: 0.2586 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|11:05:43] 	Iter 346800 Done. | loss1: 1.0185 | loss_class: 1.0177 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|11:06:33] 	Iter 346900 Done. | loss1: 0.3086 | loss_class: 0.3077 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|11:07:24] 	Iter 347000 Done. | loss1: 0.6696 | loss_class: 0.6689 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:08:14] 	Iter 347100 Done. | loss1: 0.8349 | loss_class: 0.8342 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:09:04] 	Iter 347200 Done. | loss1: 0.7416 | loss_class: 0.7409 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:09:54] 	Iter 347300 Done. | loss1: 2.0226 | loss_class: 2.0221 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:10:44] 	Iter 347400 Done. | loss1: 0.2186 | loss_class: 0.2179 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:11:34] 	Iter 347500 Done. | loss1: 0.6927 | loss_class: 0.6921 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:12:24] 	Iter 347600 Done. | loss1: 0.0778 | loss_class: 0.0771 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:13:14] 	Iter 347700 Done. | loss1: 0.9746 | loss_class: 0.9740 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:14:04] 	Iter 347800 Done. | loss1: 0.5581 | loss_class: 0.5576 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:14:54] 	Iter 347900 Done. | loss1: 0.6275 | loss_class: 0.6266 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|11:15:45] 	Iter 348000 Done. | loss1: 0.7143 | loss_class: 0.7137 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:16:35] 	Iter 348100 Done. | loss1: 0.4229 | loss_class: 0.4222 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|11:17:25] 	Iter 348200 Done. | loss1: 2.1451 | loss_class: 2.1446 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:18:16] 	Iter 348300 Done. | loss1: 1.2901 | loss_class: 1.2895 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:19:06] 	Iter 348400 Done. | loss1: 0.7963 | loss_class: 0.7956 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:19:56] 	Iter 348500 Done. | loss1: 2.0013 | loss_class: 2.0008 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:20:46] 	Iter 348600 Done. | loss1: 0.5338 | loss_class: 0.5332 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:21:35] 	Iter 348700 Done. | loss1: 1.5893 | loss_class: 1.5888 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:22:26] 	Iter 348800 Done. | loss1: 0.2363 | loss_class: 0.2357 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:23:16] 	Iter 348900 Done. | loss1: 1.0488 | loss_class: 1.0481 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:24:06] 	Iter 349000 Done. | loss1: 0.2236 | loss_class: 0.2231 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:24:56] 	Iter 349100 Done. | loss1: 0.7501 | loss_class: 0.7495 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:25:47] 	Iter 349200 Done. | loss1: 1.5657 | loss_class: 1.5650 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:26:36] 	Iter 349300 Done. | loss1: 0.2693 | loss_class: 0.2685 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|11:27:26] 	Iter 349400 Done. | loss1: 0.9614 | loss_class: 0.9607 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:28:16] 	Iter 349500 Done. | loss1: 1.3522 | loss_class: 1.3516 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:29:06] 	Iter 349600 Done. | loss1: 2.5568 | loss_class: 2.5561 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:29:56] 	Iter 349700 Done. | loss1: 2.4864 | loss_class: 2.4859 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:30:46] 	Iter 349800 Done. | loss1: 1.7299 | loss_class: 1.7291 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|11:31:36] 	Iter 349900 Done. | loss1: 0.4290 | loss_class: 0.4282 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|11:32:27] 	Iter 350000 Done. | loss1: 2.0402 | loss_class: 2.0393 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|11:33:17] 	Iter 350100 Done. | loss1: 0.8887 | loss_class: 0.8878 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|11:34:07] 	Iter 350200 Done. | loss1: 0.1627 | loss_class: 0.1620 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:34:57] 	Iter 350300 Done. | loss1: 1.9077 | loss_class: 1.9073 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:35:47] 	Iter 350400 Done. | loss1: 0.3429 | loss_class: 0.3422 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:36:37] 	Iter 350500 Done. | loss1: 2.2189 | loss_class: 2.2185 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:37:27] 	Iter 350600 Done. | loss1: 2.0564 | loss_class: 2.0556 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|11:38:17] 	Iter 350700 Done. | loss1: 0.3577 | loss_class: 0.3571 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:38:51] 	mean_loss1: 1.1392248308394703
[06.24.21|11:38:51] 	mean_loss_class: 1.1385777687200969
[06.24.21|11:38:51] 	mean_loss_recon: 0.0006470623744008243
[06.24.21|11:38:51] Time consumption:
[06.24.21|11:38:51] Done.
[06.24.21|11:38:51] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch44_model1.pt.
[06.24.21|11:38:51] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch44_model2.pt.
[06.24.21|11:38:51] Eval epoch: 44
[06.24.21|11:45:28] 	mean_loss1: 1.158095855699029
[06.24.21|11:45:28] 	mean_loss_class: 1.1577415256075156
[06.24.21|11:45:28] 	mean_loss_recon: 0.03543307423562735
[06.24.21|11:45:28] 

[06.24.21|11:45:28] 	Top1: 67.84%
[06.24.21|11:45:28] 

[06.24.21|11:45:28] 	Top5: 92.45%
[06.24.21|11:45:28] Done.
[06.24.21|11:45:28] Training epoch: 45
[06.24.21|11:45:44] 	Iter 350800 Done. | loss1: 0.9042 | loss_class: 0.9035 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:46:35] 	Iter 350900 Done. | loss1: 1.2850 | loss_class: 1.2844 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:47:26] 	Iter 351000 Done. | loss1: 0.5177 | loss_class: 0.5172 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:48:16] 	Iter 351100 Done. | loss1: 1.2246 | loss_class: 1.2239 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:49:06] 	Iter 351200 Done. | loss1: 1.8415 | loss_class: 1.8410 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:49:56] 	Iter 351300 Done. | loss1: 0.3802 | loss_class: 0.3795 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:50:47] 	Iter 351400 Done. | loss1: 1.2809 | loss_class: 1.2803 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:51:37] 	Iter 351500 Done. | loss1: 0.7348 | loss_class: 0.7340 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|11:52:26] 	Iter 351600 Done. | loss1: 1.1522 | loss_class: 1.1517 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|11:53:18] 	Iter 351700 Done. | loss1: 1.5672 | loss_class: 1.5663 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|11:54:11] 	Iter 351800 Done. | loss1: 0.3068 | loss_class: 0.3062 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:55:02] 	Iter 351900 Done. | loss1: 0.7392 | loss_class: 0.7386 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:55:53] 	Iter 352000 Done. | loss1: 1.1403 | loss_class: 1.1396 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:56:43] 	Iter 352100 Done. | loss1: 3.2903 | loss_class: 3.2896 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|11:57:35] 	Iter 352200 Done. | loss1: 0.3226 | loss_class: 0.3220 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:58:26] 	Iter 352300 Done. | loss1: 1.4927 | loss_class: 1.4921 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|11:59:17] 	Iter 352400 Done. | loss1: 1.9217 | loss_class: 1.9212 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|12:00:07] 	Iter 352500 Done. | loss1: 1.6715 | loss_class: 1.6710 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|12:00:57] 	Iter 352600 Done. | loss1: 0.7933 | loss_class: 0.7926 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:01:48] 	Iter 352700 Done. | loss1: 1.4114 | loss_class: 1.4108 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:02:38] 	Iter 352800 Done. | loss1: 1.5788 | loss_class: 1.5781 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:03:29] 	Iter 352900 Done. | loss1: 1.4810 | loss_class: 1.4804 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:04:19] 	Iter 353000 Done. | loss1: 1.1312 | loss_class: 1.1305 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|12:05:10] 	Iter 353100 Done. | loss1: 1.5601 | loss_class: 1.5595 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:06:00] 	Iter 353200 Done. | loss1: 1.7442 | loss_class: 1.7435 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:06:50] 	Iter 353300 Done. | loss1: 1.0446 | loss_class: 1.0441 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|12:07:40] 	Iter 353400 Done. | loss1: 1.3408 | loss_class: 1.3399 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|12:08:31] 	Iter 353500 Done. | loss1: 1.5565 | loss_class: 1.5559 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:09:21] 	Iter 353600 Done. | loss1: 0.8132 | loss_class: 0.8124 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|12:10:12] 	Iter 353700 Done. | loss1: 1.7147 | loss_class: 1.7139 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|12:11:02] 	Iter 353800 Done. | loss1: 0.2301 | loss_class: 0.2292 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|12:11:52] 	Iter 353900 Done. | loss1: 0.7480 | loss_class: 0.7473 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:12:43] 	Iter 354000 Done. | loss1: 0.0989 | loss_class: 0.0982 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:13:33] 	Iter 354100 Done. | loss1: 0.4212 | loss_class: 0.4203 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|12:14:23] 	Iter 354200 Done. | loss1: 1.4613 | loss_class: 1.4605 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|12:15:14] 	Iter 354300 Done. | loss1: 1.9580 | loss_class: 1.9572 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|12:16:05] 	Iter 354400 Done. | loss1: 1.4324 | loss_class: 1.4317 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:16:55] 	Iter 354500 Done. | loss1: 1.7109 | loss_class: 1.7103 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:17:46] 	Iter 354600 Done. | loss1: 0.4213 | loss_class: 0.4207 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:18:37] 	Iter 354700 Done. | loss1: 1.4758 | loss_class: 1.4753 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:19:27] 	Iter 354800 Done. | loss1: 2.2256 | loss_class: 2.2251 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|12:20:18] 	Iter 354900 Done. | loss1: 1.8579 | loss_class: 1.8572 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:21:08] 	Iter 355000 Done. | loss1: 0.4161 | loss_class: 0.4152 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|12:21:59] 	Iter 355100 Done. | loss1: 0.6138 | loss_class: 0.6133 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|12:22:50] 	Iter 355200 Done. | loss1: 0.9114 | loss_class: 0.9110 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|12:23:40] 	Iter 355300 Done. | loss1: 1.7039 | loss_class: 1.7035 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|12:24:31] 	Iter 355400 Done. | loss1: 1.4347 | loss_class: 1.4326 | loss_recon: 0.0022 | lr: 0.100000
[06.24.21|12:25:22] 	Iter 355500 Done. | loss1: 1.3300 | loss_class: 1.3294 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:26:12] 	Iter 355600 Done. | loss1: 1.1944 | loss_class: 1.1934 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|12:27:03] 	Iter 355700 Done. | loss1: 2.0562 | loss_class: 2.0554 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|12:27:53] 	Iter 355800 Done. | loss1: 2.4018 | loss_class: 2.4011 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:28:44] 	Iter 355900 Done. | loss1: 0.4786 | loss_class: 0.4781 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|12:29:34] 	Iter 356000 Done. | loss1: 0.4430 | loss_class: 0.4420 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|12:30:27] 	Iter 356100 Done. | loss1: 3.2018 | loss_class: 3.2013 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|12:31:17] 	Iter 356200 Done. | loss1: 0.9781 | loss_class: 0.9774 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:32:08] 	Iter 356300 Done. | loss1: 1.3608 | loss_class: 1.3602 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:32:57] 	Iter 356400 Done. | loss1: 1.0823 | loss_class: 1.0815 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|12:33:47] 	Iter 356500 Done. | loss1: 1.4436 | loss_class: 1.4430 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:34:37] 	Iter 356600 Done. | loss1: 1.2233 | loss_class: 1.2226 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:35:27] 	Iter 356700 Done. | loss1: 1.4182 | loss_class: 1.4176 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:36:18] 	Iter 356800 Done. | loss1: 1.0892 | loss_class: 1.0886 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:37:08] 	Iter 356900 Done. | loss1: 0.6804 | loss_class: 0.6796 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|12:37:58] 	Iter 357000 Done. | loss1: 1.4145 | loss_class: 1.4138 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:38:48] 	Iter 357100 Done. | loss1: 0.7627 | loss_class: 0.7620 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|12:39:37] 	Iter 357200 Done. | loss1: 0.7715 | loss_class: 0.7709 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:40:27] 	Iter 357300 Done. | loss1: 0.9816 | loss_class: 0.9810 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:41:19] 	Iter 357400 Done. | loss1: 1.5037 | loss_class: 1.5033 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|12:42:11] 	Iter 357500 Done. | loss1: 1.2893 | loss_class: 1.2888 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|12:43:02] 	Iter 357600 Done. | loss1: 0.4200 | loss_class: 0.4193 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:43:52] 	Iter 357700 Done. | loss1: 2.2636 | loss_class: 2.2631 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|12:44:41] 	Iter 357800 Done. | loss1: 0.9250 | loss_class: 0.9243 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:45:32] 	Iter 357900 Done. | loss1: 1.6431 | loss_class: 1.6425 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:46:22] 	Iter 358000 Done. | loss1: 0.6475 | loss_class: 0.6469 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:47:14] 	Iter 358100 Done. | loss1: 0.4057 | loss_class: 0.4052 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|12:48:04] 	Iter 358200 Done. | loss1: 1.3221 | loss_class: 1.3213 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|12:48:55] 	Iter 358300 Done. | loss1: 1.4143 | loss_class: 1.4136 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:49:46] 	Iter 358400 Done. | loss1: 1.3865 | loss_class: 1.3858 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:50:37] 	Iter 358500 Done. | loss1: 0.9119 | loss_class: 0.9115 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|12:51:28] 	Iter 358600 Done. | loss1: 0.6678 | loss_class: 0.6671 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:52:21] 	Iter 358700 Done. | loss1: 0.7158 | loss_class: 0.7152 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|12:53:17] 	Iter 358800 Done. | loss1: 0.8167 | loss_class: 0.8160 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:54:09] 	Iter 358900 Done. | loss1: 3.2171 | loss_class: 3.2163 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|12:55:00] 	Iter 359000 Done. | loss1: 0.1297 | loss_class: 0.1290 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:55:50] 	Iter 359100 Done. | loss1: 0.8852 | loss_class: 0.8847 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|12:56:40] 	Iter 359200 Done. | loss1: 0.0421 | loss_class: 0.0411 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|12:57:30] 	Iter 359300 Done. | loss1: 1.9304 | loss_class: 1.9297 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|12:58:21] 	Iter 359400 Done. | loss1: 0.3210 | loss_class: 0.3202 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|12:59:11] 	Iter 359500 Done. | loss1: 1.7514 | loss_class: 1.7508 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:00:01] 	Iter 359600 Done. | loss1: 1.5236 | loss_class: 1.5231 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:00:52] 	Iter 359700 Done. | loss1: 1.1760 | loss_class: 1.1752 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|13:01:42] 	Iter 359800 Done. | loss1: 0.0435 | loss_class: 0.0428 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:02:32] 	Iter 359900 Done. | loss1: 0.9797 | loss_class: 0.9792 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|13:03:23] 	Iter 360000 Done. | loss1: 2.4087 | loss_class: 2.4077 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|13:04:13] 	Iter 360100 Done. | loss1: 0.8191 | loss_class: 0.8186 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|13:05:03] 	Iter 360200 Done. | loss1: 0.5023 | loss_class: 0.5017 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:05:53] 	Iter 360300 Done. | loss1: 2.9994 | loss_class: 2.9987 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:06:43] 	Iter 360400 Done. | loss1: 0.4763 | loss_class: 0.4757 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:07:33] 	Iter 360500 Done. | loss1: 1.3415 | loss_class: 1.3409 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:08:23] 	Iter 360600 Done. | loss1: 2.4268 | loss_class: 2.4261 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:09:15] 	Iter 360700 Done. | loss1: 1.0254 | loss_class: 1.0245 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|13:10:01] 	mean_loss1: 1.1367521230748427
[06.24.21|13:10:01] 	mean_loss_class: 1.1361048527606867
[06.24.21|13:10:01] 	mean_loss_recon: 0.0006472707775796311
[06.24.21|13:10:01] Time consumption:
[06.24.21|13:10:01] Done.
[06.24.21|13:10:01] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch45_model1.pt.
[06.24.21|13:10:01] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch45_model2.pt.
[06.24.21|13:10:01] Training epoch: 46
[06.24.21|13:10:06] 	Iter 360800 Done. | loss1: 1.2861 | loss_class: 1.2854 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:10:57] 	Iter 360900 Done. | loss1: 1.6738 | loss_class: 1.6733 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|13:11:47] 	Iter 361000 Done. | loss1: 0.7022 | loss_class: 0.7015 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:12:38] 	Iter 361100 Done. | loss1: 2.4821 | loss_class: 2.4812 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|13:13:28] 	Iter 361200 Done. | loss1: 0.4481 | loss_class: 0.4473 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|13:14:22] 	Iter 361300 Done. | loss1: 1.1920 | loss_class: 1.1912 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|13:15:12] 	Iter 361400 Done. | loss1: 0.4817 | loss_class: 0.4811 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:16:03] 	Iter 361500 Done. | loss1: 0.5422 | loss_class: 0.5415 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:16:53] 	Iter 361600 Done. | loss1: 0.4352 | loss_class: 0.4345 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:17:43] 	Iter 361700 Done. | loss1: 0.3304 | loss_class: 0.3296 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|13:18:35] 	Iter 361800 Done. | loss1: 0.8592 | loss_class: 0.8587 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:19:27] 	Iter 361900 Done. | loss1: 0.2751 | loss_class: 0.2744 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:20:17] 	Iter 362000 Done. | loss1: 0.6684 | loss_class: 0.6678 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:21:08] 	Iter 362100 Done. | loss1: 2.4406 | loss_class: 2.4402 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|13:21:58] 	Iter 362200 Done. | loss1: 1.1896 | loss_class: 1.1890 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:22:49] 	Iter 362300 Done. | loss1: 0.4742 | loss_class: 0.4736 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:23:40] 	Iter 362400 Done. | loss1: 2.9038 | loss_class: 2.9034 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|13:24:29] 	Iter 362500 Done. | loss1: 1.1821 | loss_class: 1.1816 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:25:19] 	Iter 362600 Done. | loss1: 1.0217 | loss_class: 1.0210 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:26:10] 	Iter 362700 Done. | loss1: 1.7707 | loss_class: 1.7702 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|13:27:00] 	Iter 362800 Done. | loss1: 0.7508 | loss_class: 0.7502 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:27:51] 	Iter 362900 Done. | loss1: 0.4267 | loss_class: 0.4260 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:28:44] 	Iter 363000 Done. | loss1: 1.5777 | loss_class: 1.5768 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|13:29:35] 	Iter 363100 Done. | loss1: 0.9704 | loss_class: 0.9695 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|13:30:26] 	Iter 363200 Done. | loss1: 1.3221 | loss_class: 1.3215 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:31:17] 	Iter 363300 Done. | loss1: 3.4752 | loss_class: 3.4744 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|13:32:07] 	Iter 363400 Done. | loss1: 1.0458 | loss_class: 1.0451 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:32:57] 	Iter 363500 Done. | loss1: 1.2290 | loss_class: 1.2284 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:33:48] 	Iter 363600 Done. | loss1: 0.8077 | loss_class: 0.8069 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|13:34:39] 	Iter 363700 Done. | loss1: 1.2336 | loss_class: 1.2331 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:35:30] 	Iter 363800 Done. | loss1: 0.9754 | loss_class: 0.9749 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|13:36:21] 	Iter 363900 Done. | loss1: 1.4433 | loss_class: 1.4426 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:37:13] 	Iter 364000 Done. | loss1: 1.1500 | loss_class: 1.1493 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:38:04] 	Iter 364100 Done. | loss1: 1.2345 | loss_class: 1.2340 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|13:38:55] 	Iter 364200 Done. | loss1: 0.7034 | loss_class: 0.7030 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|13:39:46] 	Iter 364300 Done. | loss1: 1.4519 | loss_class: 1.4513 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:40:37] 	Iter 364400 Done. | loss1: 0.5672 | loss_class: 0.5665 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:41:28] 	Iter 364500 Done. | loss1: 1.5490 | loss_class: 1.5482 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|13:42:20] 	Iter 364600 Done. | loss1: 1.5705 | loss_class: 1.5698 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:43:10] 	Iter 364700 Done. | loss1: 0.6172 | loss_class: 0.6166 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:44:00] 	Iter 364800 Done. | loss1: 0.8805 | loss_class: 0.8798 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:44:53] 	Iter 364900 Done. | loss1: 1.1669 | loss_class: 1.1664 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|13:45:43] 	Iter 365000 Done. | loss1: 0.9340 | loss_class: 0.9335 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|13:46:33] 	Iter 365100 Done. | loss1: 0.1220 | loss_class: 0.1214 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:47:24] 	Iter 365200 Done. | loss1: 0.8028 | loss_class: 0.8023 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|13:48:14] 	Iter 365300 Done. | loss1: 0.8202 | loss_class: 0.8191 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|13:49:04] 	Iter 365400 Done. | loss1: 0.4825 | loss_class: 0.4821 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|13:49:54] 	Iter 365500 Done. | loss1: 1.3902 | loss_class: 1.3897 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|13:50:45] 	Iter 365600 Done. | loss1: 0.4546 | loss_class: 0.4540 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:51:38] 	Iter 365700 Done. | loss1: 1.0641 | loss_class: 1.0634 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|13:52:29] 	Iter 365800 Done. | loss1: 1.8063 | loss_class: 1.8054 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|13:53:19] 	Iter 365900 Done. | loss1: 1.6839 | loss_class: 1.6832 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:54:09] 	Iter 366000 Done. | loss1: 1.3643 | loss_class: 1.3637 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:54:59] 	Iter 366100 Done. | loss1: 0.8963 | loss_class: 0.8957 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:55:49] 	Iter 366200 Done. | loss1: 0.7107 | loss_class: 0.7102 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:56:39] 	Iter 366300 Done. | loss1: 1.6608 | loss_class: 1.6601 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|13:57:30] 	Iter 366400 Done. | loss1: 2.3195 | loss_class: 2.3188 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|13:58:20] 	Iter 366500 Done. | loss1: 0.4722 | loss_class: 0.4715 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|13:59:11] 	Iter 366600 Done. | loss1: 0.9347 | loss_class: 0.9340 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:00:03] 	Iter 366700 Done. | loss1: 3.8316 | loss_class: 3.8310 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:00:53] 	Iter 366800 Done. | loss1: 0.0576 | loss_class: 0.0570 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:01:43] 	Iter 366900 Done. | loss1: 1.0145 | loss_class: 1.0137 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|14:02:34] 	Iter 367000 Done. | loss1: 0.7441 | loss_class: 0.7430 | loss_recon: 0.0011 | lr: 0.100000
[06.24.21|14:03:24] 	Iter 367100 Done. | loss1: 0.7999 | loss_class: 0.7993 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:04:14] 	Iter 367200 Done. | loss1: 3.4902 | loss_class: 3.4895 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:05:03] 	Iter 367300 Done. | loss1: 3.1507 | loss_class: 3.1500 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:05:53] 	Iter 367400 Done. | loss1: 1.3544 | loss_class: 1.3535 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|14:06:42] 	Iter 367500 Done. | loss1: 0.0373 | loss_class: 0.0364 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|14:07:32] 	Iter 367600 Done. | loss1: 2.5534 | loss_class: 2.5529 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|14:08:23] 	Iter 367700 Done. | loss1: 2.9664 | loss_class: 2.9656 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|14:09:12] 	Iter 367800 Done. | loss1: 0.8058 | loss_class: 0.8052 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:10:06] 	Iter 367900 Done. | loss1: 0.9133 | loss_class: 0.9124 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|14:10:56] 	Iter 368000 Done. | loss1: 0.4621 | loss_class: 0.4614 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:11:46] 	Iter 368100 Done. | loss1: 2.1570 | loss_class: 2.1562 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|14:12:36] 	Iter 368200 Done. | loss1: 0.9068 | loss_class: 0.9061 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:13:27] 	Iter 368300 Done. | loss1: 0.9795 | loss_class: 0.9790 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|14:14:17] 	Iter 368400 Done. | loss1: 1.7914 | loss_class: 1.7904 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|14:15:07] 	Iter 368500 Done. | loss1: 0.2632 | loss_class: 0.2627 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|14:15:57] 	Iter 368600 Done. | loss1: 0.6031 | loss_class: 0.6025 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:16:47] 	Iter 368700 Done. | loss1: 1.4866 | loss_class: 1.4861 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|14:17:37] 	Iter 368800 Done. | loss1: 0.4256 | loss_class: 0.4249 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:18:27] 	Iter 368900 Done. | loss1: 2.4027 | loss_class: 2.4021 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|14:19:17] 	Iter 369000 Done. | loss1: 2.0576 | loss_class: 2.0569 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:20:07] 	Iter 369100 Done. | loss1: 1.2139 | loss_class: 1.2133 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:20:58] 	Iter 369200 Done. | loss1: 0.6760 | loss_class: 0.6754 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:21:48] 	Iter 369300 Done. | loss1: 0.4967 | loss_class: 0.4963 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|14:22:38] 	Iter 369400 Done. | loss1: 2.0438 | loss_class: 2.0433 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|14:23:29] 	Iter 369500 Done. | loss1: 0.5298 | loss_class: 0.5292 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:24:20] 	Iter 369600 Done. | loss1: 0.1684 | loss_class: 0.1678 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:25:10] 	Iter 369700 Done. | loss1: 0.5634 | loss_class: 0.5628 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:26:00] 	Iter 369800 Done. | loss1: 0.8346 | loss_class: 0.8340 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:26:51] 	Iter 369900 Done. | loss1: 1.4538 | loss_class: 1.4531 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:27:41] 	Iter 370000 Done. | loss1: 0.3862 | loss_class: 0.3856 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:28:31] 	Iter 370100 Done. | loss1: 1.3027 | loss_class: 1.3020 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:29:21] 	Iter 370200 Done. | loss1: 2.4474 | loss_class: 2.4467 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:30:11] 	Iter 370300 Done. | loss1: 0.1059 | loss_class: 0.1054 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|14:31:01] 	Iter 370400 Done. | loss1: 1.1236 | loss_class: 1.1228 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|14:31:51] 	Iter 370500 Done. | loss1: 0.4157 | loss_class: 0.4145 | loss_recon: 0.0012 | lr: 0.100000
[06.24.21|14:32:40] 	Iter 370600 Done. | loss1: 0.4619 | loss_class: 0.4611 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:33:30] 	Iter 370700 Done. | loss1: 2.3661 | loss_class: 2.3654 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:34:20] 	Iter 370800 Done. | loss1: 1.7092 | loss_class: 1.7085 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:34:27] 	mean_loss1: 1.1268692764029746
[06.24.21|14:34:27] 	mean_loss_class: 1.1262221063986346
[06.24.21|14:34:27] 	mean_loss_recon: 0.0006471698559267546
[06.24.21|14:34:27] Time consumption:
[06.24.21|14:34:27] Done.
[06.24.21|14:34:27] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch46_model1.pt.
[06.24.21|14:34:27] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch46_model2.pt.
[06.24.21|14:34:27] Training epoch: 47
[06.24.21|14:35:11] 	Iter 370900 Done. | loss1: 1.2724 | loss_class: 1.2717 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:36:02] 	Iter 371000 Done. | loss1: 1.9435 | loss_class: 1.9429 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:36:52] 	Iter 371100 Done. | loss1: 1.8838 | loss_class: 1.8832 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:37:43] 	Iter 371200 Done. | loss1: 0.5665 | loss_class: 0.5657 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|14:38:33] 	Iter 371300 Done. | loss1: 0.0947 | loss_class: 0.0941 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:39:24] 	Iter 371400 Done. | loss1: 0.6560 | loss_class: 0.6553 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:40:14] 	Iter 371500 Done. | loss1: 0.9438 | loss_class: 0.9431 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:41:05] 	Iter 371600 Done. | loss1: 0.7814 | loss_class: 0.7808 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:41:55] 	Iter 371700 Done. | loss1: 1.5226 | loss_class: 1.5218 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|14:42:45] 	Iter 371800 Done. | loss1: 0.0560 | loss_class: 0.0554 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:43:35] 	Iter 371900 Done. | loss1: 1.0902 | loss_class: 1.0896 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:44:26] 	Iter 372000 Done. | loss1: 0.9324 | loss_class: 0.9316 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|14:45:17] 	Iter 372100 Done. | loss1: 1.6009 | loss_class: 1.6002 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:46:08] 	Iter 372200 Done. | loss1: 0.9788 | loss_class: 0.9783 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|14:46:58] 	Iter 372300 Done. | loss1: 0.9354 | loss_class: 0.9347 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:47:49] 	Iter 372400 Done. | loss1: 0.8401 | loss_class: 0.8396 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|14:48:39] 	Iter 372500 Done. | loss1: 0.0676 | loss_class: 0.0666 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|14:49:30] 	Iter 372600 Done. | loss1: 0.7515 | loss_class: 0.7509 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:50:21] 	Iter 372700 Done. | loss1: 0.5876 | loss_class: 0.5867 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|14:51:12] 	Iter 372800 Done. | loss1: 1.0581 | loss_class: 1.0575 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:52:01] 	Iter 372900 Done. | loss1: 1.9181 | loss_class: 1.9172 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|14:52:51] 	Iter 373000 Done. | loss1: 1.4750 | loss_class: 1.4744 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|14:53:41] 	Iter 373100 Done. | loss1: 1.2088 | loss_class: 1.2079 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|14:54:31] 	Iter 373200 Done. | loss1: 1.9471 | loss_class: 1.9466 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|14:55:26] 	Iter 373300 Done. | loss1: 0.4501 | loss_class: 0.4493 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|14:56:16] 	Iter 373400 Done. | loss1: 1.5252 | loss_class: 1.5247 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|14:57:06] 	Iter 373500 Done. | loss1: 0.8001 | loss_class: 0.7996 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|14:57:56] 	Iter 373600 Done. | loss1: 1.5014 | loss_class: 1.5007 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|14:58:46] 	Iter 373700 Done. | loss1: 0.8974 | loss_class: 0.8969 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|14:59:38] 	Iter 373800 Done. | loss1: 0.6842 | loss_class: 0.6835 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:00:29] 	Iter 373900 Done. | loss1: 1.8955 | loss_class: 1.8950 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|15:01:19] 	Iter 374000 Done. | loss1: 1.0423 | loss_class: 1.0419 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|15:02:09] 	Iter 374100 Done. | loss1: 0.5139 | loss_class: 0.5133 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:03:00] 	Iter 374200 Done. | loss1: 0.4463 | loss_class: 0.4459 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|15:03:53] 	Iter 374300 Done. | loss1: 1.1329 | loss_class: 1.1323 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:04:44] 	Iter 374400 Done. | loss1: 0.7150 | loss_class: 0.7143 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:05:34] 	Iter 374500 Done. | loss1: 1.4581 | loss_class: 1.4574 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:06:24] 	Iter 374600 Done. | loss1: 0.4455 | loss_class: 0.4449 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:07:14] 	Iter 374700 Done. | loss1: 1.8987 | loss_class: 1.8982 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|15:08:03] 	Iter 374800 Done. | loss1: 0.2509 | loss_class: 0.2502 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:08:54] 	Iter 374900 Done. | loss1: 1.0450 | loss_class: 1.0443 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:09:44] 	Iter 375000 Done. | loss1: 0.5297 | loss_class: 0.5289 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|15:10:34] 	Iter 375100 Done. | loss1: 0.4559 | loss_class: 0.4552 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:11:24] 	Iter 375200 Done. | loss1: 1.4468 | loss_class: 1.4461 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:12:14] 	Iter 375300 Done. | loss1: 1.5585 | loss_class: 1.5579 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:13:04] 	Iter 375400 Done. | loss1: 2.8289 | loss_class: 2.8278 | loss_recon: 0.0011 | lr: 0.100000
[06.24.21|15:13:54] 	Iter 375500 Done. | loss1: 1.1063 | loss_class: 1.1058 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|15:14:44] 	Iter 375600 Done. | loss1: 1.1075 | loss_class: 1.1070 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|15:15:34] 	Iter 375700 Done. | loss1: 1.4642 | loss_class: 1.4634 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|15:16:24] 	Iter 375800 Done. | loss1: 1.1982 | loss_class: 1.1977 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|15:17:14] 	Iter 375900 Done. | loss1: 0.7154 | loss_class: 0.7148 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:18:04] 	Iter 376000 Done. | loss1: 2.3701 | loss_class: 2.3694 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:18:54] 	Iter 376100 Done. | loss1: 0.2509 | loss_class: 0.2502 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:19:45] 	Iter 376200 Done. | loss1: 2.2551 | loss_class: 2.2546 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:20:35] 	Iter 376300 Done. | loss1: 0.2487 | loss_class: 0.2479 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|15:21:26] 	Iter 376400 Done. | loss1: 0.6299 | loss_class: 0.6292 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:22:18] 	Iter 376500 Done. | loss1: 0.0517 | loss_class: 0.0511 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:23:10] 	Iter 376600 Done. | loss1: 1.1726 | loss_class: 1.1720 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:24:03] 	Iter 376700 Done. | loss1: 1.0044 | loss_class: 1.0038 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:24:57] 	Iter 376800 Done. | loss1: 1.6345 | loss_class: 1.6340 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|15:25:50] 	Iter 376900 Done. | loss1: 0.1913 | loss_class: 0.1905 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|15:26:40] 	Iter 377000 Done. | loss1: 1.1683 | loss_class: 1.1676 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:27:35] 	Iter 377100 Done. | loss1: 0.7444 | loss_class: 0.7438 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:28:29] 	Iter 377200 Done. | loss1: 0.2895 | loss_class: 0.2886 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|15:29:19] 	Iter 377300 Done. | loss1: 0.3462 | loss_class: 0.3454 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|15:30:09] 	Iter 377400 Done. | loss1: 0.6440 | loss_class: 0.6433 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:30:59] 	Iter 377500 Done. | loss1: 0.6666 | loss_class: 0.6659 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:31:49] 	Iter 377600 Done. | loss1: 0.0899 | loss_class: 0.0893 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:32:40] 	Iter 377700 Done. | loss1: 1.9213 | loss_class: 1.9208 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|15:33:30] 	Iter 377800 Done. | loss1: 0.7209 | loss_class: 0.7203 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:34:20] 	Iter 377900 Done. | loss1: 2.5070 | loss_class: 2.5064 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:35:11] 	Iter 378000 Done. | loss1: 0.7923 | loss_class: 0.7916 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|15:36:00] 	Iter 378100 Done. | loss1: 1.9419 | loss_class: 1.9413 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:36:50] 	Iter 378200 Done. | loss1: 1.6217 | loss_class: 1.6212 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|15:37:41] 	Iter 378300 Done. | loss1: 0.7627 | loss_class: 0.7621 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:38:32] 	Iter 378400 Done. | loss1: 0.5994 | loss_class: 0.5987 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|15:39:22] 	Iter 378500 Done. | loss1: 0.6244 | loss_class: 0.6240 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|15:40:12] 	Iter 378600 Done. | loss1: 2.1754 | loss_class: 2.1750 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|15:41:01] 	Iter 378700 Done. | loss1: 0.0498 | loss_class: 0.0493 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|15:41:50] 	Iter 378800 Done. | loss1: 0.8034 | loss_class: 0.8028 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:42:40] 	Iter 378900 Done. | loss1: 3.2153 | loss_class: 3.2146 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:43:30] 	Iter 379000 Done. | loss1: 0.2311 | loss_class: 0.2305 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:44:20] 	Iter 379100 Done. | loss1: 2.8828 | loss_class: 2.8823 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|15:45:10] 	Iter 379200 Done. | loss1: 0.4941 | loss_class: 0.4935 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:46:00] 	Iter 379300 Done. | loss1: 0.9351 | loss_class: 0.9343 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:46:50] 	Iter 379400 Done. | loss1: 1.0044 | loss_class: 1.0039 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:47:40] 	Iter 379500 Done. | loss1: 1.3778 | loss_class: 1.3771 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:48:31] 	Iter 379600 Done. | loss1: 0.6361 | loss_class: 0.6355 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:49:21] 	Iter 379700 Done. | loss1: 1.3906 | loss_class: 1.3900 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|15:50:12] 	Iter 379800 Done. | loss1: 0.7562 | loss_class: 0.7557 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:51:04] 	Iter 379900 Done. | loss1: 1.2125 | loss_class: 1.2116 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|15:51:55] 	Iter 380000 Done. | loss1: 0.4965 | loss_class: 0.4958 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:52:46] 	Iter 380100 Done. | loss1: 1.1751 | loss_class: 1.1742 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|15:53:36] 	Iter 380200 Done. | loss1: 0.2856 | loss_class: 0.2849 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:54:26] 	Iter 380300 Done. | loss1: 1.1485 | loss_class: 1.1476 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|15:55:20] 	Iter 380400 Done. | loss1: 1.4936 | loss_class: 1.4930 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:56:11] 	Iter 380500 Done. | loss1: 1.5038 | loss_class: 1.5032 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:57:01] 	Iter 380600 Done. | loss1: 0.8038 | loss_class: 0.8031 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:57:52] 	Iter 380700 Done. | loss1: 1.7150 | loss_class: 1.7144 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|15:58:42] 	Iter 380800 Done. | loss1: 0.6157 | loss_class: 0.6152 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|15:59:00] 	mean_loss1: 1.131512662682982
[06.24.21|15:59:00] 	mean_loss_class: 1.1308655134876426
[06.24.21|15:59:00] 	mean_loss_recon: 0.0006471495086565462
[06.24.21|15:59:00] Time consumption:
[06.24.21|15:59:00] Done.
[06.24.21|15:59:00] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch47_model1.pt.
[06.24.21|15:59:00] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch47_model2.pt.
[06.24.21|15:59:00] Training epoch: 48
[06.24.21|15:59:36] 	Iter 380900 Done. | loss1: 1.7447 | loss_class: 1.7440 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:00:32] 	Iter 381000 Done. | loss1: 2.4270 | loss_class: 2.4262 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|16:01:24] 	Iter 381100 Done. | loss1: 1.8901 | loss_class: 1.8896 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|16:02:14] 	Iter 381200 Done. | loss1: 0.6474 | loss_class: 0.6468 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:03:04] 	Iter 381300 Done. | loss1: 1.4955 | loss_class: 1.4949 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:03:55] 	Iter 381400 Done. | loss1: 1.3790 | loss_class: 1.3784 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:04:45] 	Iter 381500 Done. | loss1: 0.3706 | loss_class: 0.3700 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:05:36] 	Iter 381600 Done. | loss1: 1.3299 | loss_class: 1.3291 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|16:06:26] 	Iter 381700 Done. | loss1: 1.1896 | loss_class: 1.1889 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:07:16] 	Iter 381800 Done. | loss1: 0.8943 | loss_class: 0.8934 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|16:08:06] 	Iter 381900 Done. | loss1: 1.4169 | loss_class: 1.4163 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:08:57] 	Iter 382000 Done. | loss1: 1.3919 | loss_class: 1.3912 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:09:47] 	Iter 382100 Done. | loss1: 1.2784 | loss_class: 1.2779 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|16:10:38] 	Iter 382200 Done. | loss1: 1.2444 | loss_class: 1.2438 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:11:28] 	Iter 382300 Done. | loss1: 2.7294 | loss_class: 2.7287 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:12:19] 	Iter 382400 Done. | loss1: 0.9622 | loss_class: 0.9617 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:13:09] 	Iter 382500 Done. | loss1: 0.2782 | loss_class: 0.2776 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:14:00] 	Iter 382600 Done. | loss1: 3.0295 | loss_class: 3.0289 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:14:51] 	Iter 382700 Done. | loss1: 1.4434 | loss_class: 1.4428 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:15:41] 	Iter 382800 Done. | loss1: 0.6102 | loss_class: 0.6095 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|16:16:31] 	Iter 382900 Done. | loss1: 0.5571 | loss_class: 0.5566 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|16:17:22] 	Iter 383000 Done. | loss1: 2.8103 | loss_class: 2.8095 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|16:18:12] 	Iter 383100 Done. | loss1: 1.0760 | loss_class: 1.0755 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|16:19:02] 	Iter 383200 Done. | loss1: 1.5715 | loss_class: 1.5706 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|16:19:52] 	Iter 383300 Done. | loss1: 1.7747 | loss_class: 1.7741 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:20:42] 	Iter 383400 Done. | loss1: 1.1321 | loss_class: 1.1314 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:21:32] 	Iter 383500 Done. | loss1: 0.0886 | loss_class: 0.0877 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|16:22:24] 	Iter 383600 Done. | loss1: 1.1097 | loss_class: 1.1092 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|16:23:15] 	Iter 383700 Done. | loss1: 0.2013 | loss_class: 0.2008 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|16:24:05] 	Iter 383800 Done. | loss1: 2.1658 | loss_class: 2.1651 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:24:55] 	Iter 383900 Done. | loss1: 0.1954 | loss_class: 0.1946 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|16:25:45] 	Iter 384000 Done. | loss1: 1.9957 | loss_class: 1.9952 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|16:26:40] 	Iter 384100 Done. | loss1: 1.0964 | loss_class: 1.0959 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:27:29] 	Iter 384200 Done. | loss1: 0.6030 | loss_class: 0.6024 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:28:19] 	Iter 384300 Done. | loss1: 0.2836 | loss_class: 0.2829 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:29:09] 	Iter 384400 Done. | loss1: 2.0742 | loss_class: 2.0736 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:29:59] 	Iter 384500 Done. | loss1: 0.5755 | loss_class: 0.5748 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:30:50] 	Iter 384600 Done. | loss1: 1.4359 | loss_class: 1.4350 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|16:31:41] 	Iter 384700 Done. | loss1: 1.2830 | loss_class: 1.2825 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:32:31] 	Iter 384800 Done. | loss1: 0.4654 | loss_class: 0.4647 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:33:21] 	Iter 384900 Done. | loss1: 0.6588 | loss_class: 0.6582 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:34:11] 	Iter 385000 Done. | loss1: 0.7962 | loss_class: 0.7956 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:35:01] 	Iter 385100 Done. | loss1: 1.0589 | loss_class: 1.0582 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:35:52] 	Iter 385200 Done. | loss1: 0.6816 | loss_class: 0.6807 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|16:36:42] 	Iter 385300 Done. | loss1: 1.3487 | loss_class: 1.3480 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:37:33] 	Iter 385400 Done. | loss1: 1.3279 | loss_class: 1.3273 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:38:24] 	Iter 385500 Done. | loss1: 0.8949 | loss_class: 0.8943 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:39:17] 	Iter 385600 Done. | loss1: 0.6156 | loss_class: 0.6152 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|16:40:07] 	Iter 385700 Done. | loss1: 1.2864 | loss_class: 1.2858 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:40:57] 	Iter 385800 Done. | loss1: 0.5377 | loss_class: 0.5371 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:41:47] 	Iter 385900 Done. | loss1: 0.2879 | loss_class: 0.2870 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|16:42:37] 	Iter 386000 Done. | loss1: 1.7305 | loss_class: 1.7299 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:43:27] 	Iter 386100 Done. | loss1: 0.8912 | loss_class: 0.8904 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:44:17] 	Iter 386200 Done. | loss1: 2.6468 | loss_class: 2.6460 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|16:45:07] 	Iter 386300 Done. | loss1: 1.6207 | loss_class: 1.6203 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|16:45:58] 	Iter 386400 Done. | loss1: 1.1258 | loss_class: 1.1252 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:46:47] 	Iter 386500 Done. | loss1: 1.3844 | loss_class: 1.3839 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|16:47:37] 	Iter 386600 Done. | loss1: 1.0008 | loss_class: 1.0001 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:48:27] 	Iter 386700 Done. | loss1: 0.2911 | loss_class: 0.2905 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:49:16] 	Iter 386800 Done. | loss1: 1.1033 | loss_class: 1.1025 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|16:50:06] 	Iter 386900 Done. | loss1: 0.5914 | loss_class: 0.5908 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:50:56] 	Iter 387000 Done. | loss1: 0.9916 | loss_class: 0.9908 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:51:46] 	Iter 387100 Done. | loss1: 2.7838 | loss_class: 2.7832 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:52:36] 	Iter 387200 Done. | loss1: 1.1651 | loss_class: 1.1642 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|16:53:26] 	Iter 387300 Done. | loss1: 1.1990 | loss_class: 1.1983 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:54:16] 	Iter 387400 Done. | loss1: 0.7775 | loss_class: 0.7769 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:55:06] 	Iter 387500 Done. | loss1: 0.6605 | loss_class: 0.6598 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|16:55:58] 	Iter 387600 Done. | loss1: 0.6479 | loss_class: 0.6473 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:56:50] 	Iter 387700 Done. | loss1: 1.6365 | loss_class: 1.6360 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|16:57:40] 	Iter 387800 Done. | loss1: 0.9817 | loss_class: 0.9810 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|16:58:30] 	Iter 387900 Done. | loss1: 1.8061 | loss_class: 1.8056 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|16:59:20] 	Iter 388000 Done. | loss1: 2.2906 | loss_class: 2.2901 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|17:00:11] 	Iter 388100 Done. | loss1: 0.5853 | loss_class: 0.5844 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|17:01:01] 	Iter 388200 Done. | loss1: 0.7529 | loss_class: 0.7525 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|17:01:51] 	Iter 388300 Done. | loss1: 0.8973 | loss_class: 0.8964 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|17:02:42] 	Iter 388400 Done. | loss1: 1.1155 | loss_class: 1.1149 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|17:03:32] 	Iter 388500 Done. | loss1: 0.8375 | loss_class: 0.8368 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:04:24] 	Iter 388600 Done. | loss1: 0.9657 | loss_class: 0.9651 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:05:16] 	Iter 388700 Done. | loss1: 1.6910 | loss_class: 1.6903 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:06:06] 	Iter 388800 Done. | loss1: 1.6162 | loss_class: 1.6155 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:06:59] 	Iter 388900 Done. | loss1: 1.4678 | loss_class: 1.4671 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:07:51] 	Iter 389000 Done. | loss1: 0.7127 | loss_class: 0.7123 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|17:08:41] 	Iter 389100 Done. | loss1: 1.8906 | loss_class: 1.8900 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:09:32] 	Iter 389200 Done. | loss1: 0.7384 | loss_class: 0.7374 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|17:10:22] 	Iter 389300 Done. | loss1: 1.5023 | loss_class: 1.5016 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:11:12] 	Iter 389400 Done. | loss1: 1.7388 | loss_class: 1.7383 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:12:03] 	Iter 389500 Done. | loss1: 1.6731 | loss_class: 1.6727 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|17:12:53] 	Iter 389600 Done. | loss1: 1.8531 | loss_class: 1.8525 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:13:44] 	Iter 389700 Done. | loss1: 1.9859 | loss_class: 1.9854 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|17:14:34] 	Iter 389800 Done. | loss1: 0.3489 | loss_class: 0.3483 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:15:23] 	Iter 389900 Done. | loss1: 0.3967 | loss_class: 0.3961 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:16:13] 	Iter 390000 Done. | loss1: 2.6061 | loss_class: 2.6054 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:17:03] 	Iter 390100 Done. | loss1: 0.0465 | loss_class: 0.0457 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|17:17:54] 	Iter 390200 Done. | loss1: 0.3880 | loss_class: 0.3872 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|17:18:44] 	Iter 390300 Done. | loss1: 0.4842 | loss_class: 0.4836 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:19:35] 	Iter 390400 Done. | loss1: 1.2697 | loss_class: 1.2690 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|17:20:25] 	Iter 390500 Done. | loss1: 0.3094 | loss_class: 0.3087 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:21:16] 	Iter 390600 Done. | loss1: 2.5166 | loss_class: 2.5161 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|17:22:06] 	Iter 390700 Done. | loss1: 1.0680 | loss_class: 1.0674 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:22:57] 	Iter 390800 Done. | loss1: 1.5189 | loss_class: 1.5182 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:23:26] 	mean_loss1: 1.1235981746490769
[06.24.21|17:23:26] 	mean_loss_class: 1.1229496447889527
[06.24.21|17:23:26] 	mean_loss_recon: 0.0006485302006962223
[06.24.21|17:23:26] Time consumption:
[06.24.21|17:23:26] Done.
[06.24.21|17:23:26] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch48_model1.pt.
[06.24.21|17:23:26] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch48_model2.pt.
[06.24.21|17:23:26] Training epoch: 49
[06.24.21|17:23:48] 	Iter 390900 Done. | loss1: 0.7645 | loss_class: 0.7639 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:24:39] 	Iter 391000 Done. | loss1: 1.2369 | loss_class: 1.2363 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:25:28] 	Iter 391100 Done. | loss1: 0.9225 | loss_class: 0.9219 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:26:18] 	Iter 391200 Done. | loss1: 1.4128 | loss_class: 1.4121 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:27:08] 	Iter 391300 Done. | loss1: 0.3942 | loss_class: 0.3934 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|17:27:59] 	Iter 391400 Done. | loss1: 3.3789 | loss_class: 3.3783 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:28:49] 	Iter 391500 Done. | loss1: 0.3810 | loss_class: 0.3804 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:29:40] 	Iter 391600 Done. | loss1: 0.5410 | loss_class: 0.5403 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:30:31] 	Iter 391700 Done. | loss1: 1.0046 | loss_class: 1.0039 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:31:21] 	Iter 391800 Done. | loss1: 1.2356 | loss_class: 1.2349 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:32:12] 	Iter 391900 Done. | loss1: 1.4514 | loss_class: 1.4508 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:33:02] 	Iter 392000 Done. | loss1: 1.7113 | loss_class: 1.7104 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|17:33:53] 	Iter 392100 Done. | loss1: 1.3404 | loss_class: 1.3398 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:34:44] 	Iter 392200 Done. | loss1: 1.9411 | loss_class: 1.9404 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:35:34] 	Iter 392300 Done. | loss1: 0.6509 | loss_class: 0.6504 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|17:36:24] 	Iter 392400 Done. | loss1: 1.3319 | loss_class: 1.3311 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|17:37:13] 	Iter 392500 Done. | loss1: 1.3581 | loss_class: 1.3575 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:38:04] 	Iter 392600 Done. | loss1: 1.7017 | loss_class: 1.7010 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:38:54] 	Iter 392700 Done. | loss1: 1.0379 | loss_class: 1.0372 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|17:39:46] 	Iter 392800 Done. | loss1: 0.3785 | loss_class: 0.3780 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:40:36] 	Iter 392900 Done. | loss1: 0.9174 | loss_class: 0.9169 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|17:41:26] 	Iter 393000 Done. | loss1: 0.8098 | loss_class: 0.8093 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|17:42:16] 	Iter 393100 Done. | loss1: 2.0308 | loss_class: 2.0299 | loss_recon: 0.0009 | lr: 0.100000
[06.24.21|17:43:06] 	Iter 393200 Done. | loss1: 1.1190 | loss_class: 1.1184 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:43:57] 	Iter 393300 Done. | loss1: 0.5708 | loss_class: 0.5703 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|17:44:47] 	Iter 393400 Done. | loss1: 0.8016 | loss_class: 0.8009 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:45:37] 	Iter 393500 Done. | loss1: 0.1815 | loss_class: 0.1810 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|17:46:29] 	Iter 393600 Done. | loss1: 1.4151 | loss_class: 1.4143 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|17:47:20] 	Iter 393700 Done. | loss1: 1.4557 | loss_class: 1.4551 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:48:10] 	Iter 393800 Done. | loss1: 1.5539 | loss_class: 1.5531 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|17:49:00] 	Iter 393900 Done. | loss1: 2.1420 | loss_class: 2.1415 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|17:49:52] 	Iter 394000 Done. | loss1: 0.3763 | loss_class: 0.3757 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:50:45] 	Iter 394100 Done. | loss1: 1.2038 | loss_class: 1.2031 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:51:35] 	Iter 394200 Done. | loss1: 1.8913 | loss_class: 1.8906 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:52:26] 	Iter 394300 Done. | loss1: 1.1316 | loss_class: 1.1309 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:53:16] 	Iter 394400 Done. | loss1: 1.1242 | loss_class: 1.1236 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:54:06] 	Iter 394500 Done. | loss1: 0.6974 | loss_class: 0.6969 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|17:54:56] 	Iter 394600 Done. | loss1: 2.2862 | loss_class: 2.2854 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|17:55:46] 	Iter 394700 Done. | loss1: 0.6374 | loss_class: 0.6367 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:56:36] 	Iter 394800 Done. | loss1: 1.4964 | loss_class: 1.4957 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:57:27] 	Iter 394900 Done. | loss1: 2.2479 | loss_class: 2.2474 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|17:58:17] 	Iter 395000 Done. | loss1: 1.1003 | loss_class: 1.0995 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|17:59:08] 	Iter 395100 Done. | loss1: 1.9295 | loss_class: 1.9287 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|17:59:58] 	Iter 395200 Done. | loss1: 1.3280 | loss_class: 1.3270 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|18:00:49] 	Iter 395300 Done. | loss1: 0.1948 | loss_class: 0.1942 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|18:01:40] 	Iter 395400 Done. | loss1: 0.4044 | loss_class: 0.4038 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:02:30] 	Iter 395500 Done. | loss1: 2.8863 | loss_class: 2.8857 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:03:21] 	Iter 395600 Done. | loss1: 0.3876 | loss_class: 0.3870 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:04:11] 	Iter 395700 Done. | loss1: 0.7500 | loss_class: 0.7495 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|18:05:02] 	Iter 395800 Done. | loss1: 0.4103 | loss_class: 0.4098 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|18:05:52] 	Iter 395900 Done. | loss1: 1.9228 | loss_class: 1.9222 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:06:43] 	Iter 396000 Done. | loss1: 1.8766 | loss_class: 1.8759 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|18:07:33] 	Iter 396100 Done. | loss1: 0.9868 | loss_class: 0.9862 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:08:24] 	Iter 396200 Done. | loss1: 0.5969 | loss_class: 0.5963 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:09:14] 	Iter 396300 Done. | loss1: 0.0914 | loss_class: 0.0908 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:10:04] 	Iter 396400 Done. | loss1: 0.5982 | loss_class: 0.5975 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|18:10:55] 	Iter 396500 Done. | loss1: 0.7282 | loss_class: 0.7274 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|18:11:46] 	Iter 396600 Done. | loss1: 0.5773 | loss_class: 0.5766 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:12:37] 	Iter 396700 Done. | loss1: 0.6849 | loss_class: 0.6842 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|18:13:27] 	Iter 396800 Done. | loss1: 0.1739 | loss_class: 0.1733 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|18:14:17] 	Iter 396900 Done. | loss1: 2.3266 | loss_class: 2.3260 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:15:08] 	Iter 397000 Done. | loss1: 1.5222 | loss_class: 1.5217 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|18:15:59] 	Iter 397100 Done. | loss1: 0.1440 | loss_class: 0.1435 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|18:16:49] 	Iter 397200 Done. | loss1: 1.3752 | loss_class: 1.3746 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:17:39] 	Iter 397300 Done. | loss1: 0.0376 | loss_class: 0.0366 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|18:18:30] 	Iter 397400 Done. | loss1: 1.7832 | loss_class: 1.7826 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:19:20] 	Iter 397500 Done. | loss1: 2.6130 | loss_class: 2.6123 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|18:20:10] 	Iter 397600 Done. | loss1: 2.5197 | loss_class: 2.5191 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:21:00] 	Iter 397700 Done. | loss1: 2.2271 | loss_class: 2.2264 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|18:21:50] 	Iter 397800 Done. | loss1: 0.8801 | loss_class: 0.8795 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:22:40] 	Iter 397900 Done. | loss1: 0.5595 | loss_class: 0.5588 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:23:30] 	Iter 398000 Done. | loss1: 0.3419 | loss_class: 0.3413 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:24:20] 	Iter 398100 Done. | loss1: 0.7591 | loss_class: 0.7583 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|18:25:10] 	Iter 398200 Done. | loss1: 0.0162 | loss_class: 0.0157 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|18:26:00] 	Iter 398300 Done. | loss1: 1.2441 | loss_class: 1.2433 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|18:26:50] 	Iter 398400 Done. | loss1: 0.2423 | loss_class: 0.2415 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|18:27:40] 	Iter 398500 Done. | loss1: 1.0667 | loss_class: 1.0659 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|18:28:30] 	Iter 398600 Done. | loss1: 0.6237 | loss_class: 0.6232 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|18:29:20] 	Iter 398700 Done. | loss1: 1.4355 | loss_class: 1.4349 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|18:30:13] 	Iter 398800 Done. | loss1: 2.0485 | loss_class: 2.0480 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|18:31:03] 	Iter 398900 Done. | loss1: 0.9138 | loss_class: 0.9129 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|18:31:54] 	Iter 399000 Done. | loss1: 0.5610 | loss_class: 0.5604 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:32:45] 	Iter 399100 Done. | loss1: 1.6391 | loss_class: 1.6384 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:33:35] 	Iter 399200 Done. | loss1: 1.1988 | loss_class: 1.1982 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:34:25] 	Iter 399300 Done. | loss1: 0.5051 | loss_class: 0.5043 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|18:35:16] 	Iter 399400 Done. | loss1: 1.0011 | loss_class: 1.0006 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|18:36:07] 	Iter 399500 Done. | loss1: 1.1197 | loss_class: 1.1190 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|18:36:57] 	Iter 399600 Done. | loss1: 0.1883 | loss_class: 0.1876 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|18:37:48] 	Iter 399700 Done. | loss1: 0.7870 | loss_class: 0.7865 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|18:38:38] 	Iter 399800 Done. | loss1: 2.2693 | loss_class: 2.2688 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:39:29] 	Iter 399900 Done. | loss1: 0.4292 | loss_class: 0.4285 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|18:40:20] 	Iter 400000 Done. | loss1: 0.6753 | loss_class: 0.6746 | loss_recon: 0.0007 | lr: 0.100000
[06.24.21|18:41:10] 	Iter 400100 Done. | loss1: 0.1807 | loss_class: 0.1802 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|18:42:03] 	Iter 400200 Done. | loss1: 1.9918 | loss_class: 1.9914 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|18:42:54] 	Iter 400300 Done. | loss1: 0.8649 | loss_class: 0.8643 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:43:45] 	Iter 400400 Done. | loss1: 1.0506 | loss_class: 1.0500 | loss_recon: 0.0006 | lr: 0.100000
[06.24.21|18:44:35] 	Iter 400500 Done. | loss1: 0.7023 | loss_class: 0.7018 | loss_recon: 0.0005 | lr: 0.100000
[06.24.21|18:45:26] 	Iter 400600 Done. | loss1: 1.0628 | loss_class: 1.0618 | loss_recon: 0.0010 | lr: 0.100000
[06.24.21|18:46:16] 	Iter 400700 Done. | loss1: 0.9975 | loss_class: 0.9971 | loss_recon: 0.0004 | lr: 0.100000
[06.24.21|18:47:09] 	Iter 400800 Done. | loss1: 1.3949 | loss_class: 1.3941 | loss_recon: 0.0008 | lr: 0.100000
[06.24.21|18:47:52] 	mean_loss1: 1.1242977563511376
[06.24.21|18:47:52] 	mean_loss_class: 1.123650363664708
[06.24.21|18:47:52] 	mean_loss_recon: 0.0006473922647455511
[06.24.21|18:47:52] Time consumption:
[06.24.21|18:47:52] Done.
[06.24.21|18:47:52] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch49_model1.pt.
[06.24.21|18:47:52] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch49_model2.pt.
[06.24.21|18:47:52] Eval epoch: 49
[06.24.21|18:54:31] 	mean_loss1: 1.2674281641032346
[06.24.21|18:54:31] 	mean_loss_class: 1.267068473752155
[06.24.21|18:54:31] 	mean_loss_recon: 0.035969172763212244
[06.24.21|18:54:31] 

[06.24.21|18:54:31] 	Top1: 65.29%
[06.24.21|18:54:32] 

[06.24.21|18:54:32] 	Top5: 91.41%
[06.24.21|18:54:32] Done.
[06.24.21|18:54:32] Training epoch: 50
[06.24.21|18:54:43] 	Iter 400900 Done. | loss1: 1.7767 | loss_class: 1.7763 | loss_recon: 0.0004 | lr: 0.010000
[06.24.21|18:55:33] 	Iter 401000 Done. | loss1: 2.0943 | loss_class: 2.0938 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|18:56:23] 	Iter 401100 Done. | loss1: 0.1474 | loss_class: 0.1467 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|18:57:12] 	Iter 401200 Done. | loss1: 0.6352 | loss_class: 0.6345 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|18:58:02] 	Iter 401300 Done. | loss1: 0.4715 | loss_class: 0.4708 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|18:58:53] 	Iter 401400 Done. | loss1: 0.2033 | loss_class: 0.2026 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|18:59:44] 	Iter 401500 Done. | loss1: 0.3129 | loss_class: 0.3122 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:00:36] 	Iter 401600 Done. | loss1: 0.4000 | loss_class: 0.3993 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:01:32] 	Iter 401700 Done. | loss1: 0.2051 | loss_class: 0.2045 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:02:25] 	Iter 401800 Done. | loss1: 1.3014 | loss_class: 1.3008 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:03:15] 	Iter 401900 Done. | loss1: 0.4064 | loss_class: 0.4057 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:04:06] 	Iter 402000 Done. | loss1: 0.4822 | loss_class: 0.4816 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|19:04:56] 	Iter 402100 Done. | loss1: 0.3037 | loss_class: 0.3031 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|19:05:47] 	Iter 402200 Done. | loss1: 0.1077 | loss_class: 0.1071 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|19:06:37] 	Iter 402300 Done. | loss1: 0.7003 | loss_class: 0.6995 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|19:07:27] 	Iter 402400 Done. | loss1: 0.2759 | loss_class: 0.2753 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|19:08:17] 	Iter 402500 Done. | loss1: 1.5798 | loss_class: 1.5791 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|19:09:07] 	Iter 402600 Done. | loss1: 0.1418 | loss_class: 0.1413 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:09:57] 	Iter 402700 Done. | loss1: 0.4232 | loss_class: 0.4226 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|19:10:48] 	Iter 402800 Done. | loss1: 0.0975 | loss_class: 0.0967 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|19:11:39] 	Iter 402900 Done. | loss1: 1.0290 | loss_class: 1.0285 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:12:29] 	Iter 403000 Done. | loss1: 0.3702 | loss_class: 0.3697 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:13:20] 	Iter 403100 Done. | loss1: 0.4300 | loss_class: 0.4295 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:14:11] 	Iter 403200 Done. | loss1: 0.1776 | loss_class: 0.1771 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:15:01] 	Iter 403300 Done. | loss1: 0.6044 | loss_class: 0.6037 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:15:52] 	Iter 403400 Done. | loss1: 0.8573 | loss_class: 0.8566 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:16:42] 	Iter 403500 Done. | loss1: 0.2094 | loss_class: 0.2087 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|19:17:33] 	Iter 403600 Done. | loss1: 0.0868 | loss_class: 0.0861 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|19:18:23] 	Iter 403700 Done. | loss1: 0.2221 | loss_class: 0.2213 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:19:13] 	Iter 403800 Done. | loss1: 0.1788 | loss_class: 0.1782 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:20:03] 	Iter 403900 Done. | loss1: 0.8189 | loss_class: 0.8183 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:20:53] 	Iter 404000 Done. | loss1: 0.3446 | loss_class: 0.3439 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:21:44] 	Iter 404100 Done. | loss1: 0.0300 | loss_class: 0.0293 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:22:34] 	Iter 404200 Done. | loss1: 0.5279 | loss_class: 0.5272 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:23:25] 	Iter 404300 Done. | loss1: 0.6730 | loss_class: 0.6725 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:24:16] 	Iter 404400 Done. | loss1: 0.4235 | loss_class: 0.4230 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:25:08] 	Iter 404500 Done. | loss1: 0.1740 | loss_class: 0.1732 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|19:25:59] 	Iter 404600 Done. | loss1: 3.4392 | loss_class: 3.4385 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:26:50] 	Iter 404700 Done. | loss1: 0.3264 | loss_class: 0.3259 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:27:41] 	Iter 404800 Done. | loss1: 2.1783 | loss_class: 2.1777 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|19:28:31] 	Iter 404900 Done. | loss1: 0.4516 | loss_class: 0.4509 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:29:21] 	Iter 405000 Done. | loss1: 1.5970 | loss_class: 1.5965 | loss_recon: 0.0004 | lr: 0.010000
[06.24.21|19:30:13] 	Iter 405100 Done. | loss1: 0.0542 | loss_class: 0.0535 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:31:04] 	Iter 405200 Done. | loss1: 0.5216 | loss_class: 0.5211 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:31:54] 	Iter 405300 Done. | loss1: 0.2109 | loss_class: 0.2101 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|19:32:45] 	Iter 405400 Done. | loss1: 0.4938 | loss_class: 0.4931 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:33:35] 	Iter 405500 Done. | loss1: 1.5890 | loss_class: 1.5886 | loss_recon: 0.0004 | lr: 0.010000
[06.24.21|19:34:25] 	Iter 405600 Done. | loss1: 0.1655 | loss_class: 0.1648 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:35:15] 	Iter 405700 Done. | loss1: 0.0554 | loss_class: 0.0547 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:36:05] 	Iter 405800 Done. | loss1: 1.2009 | loss_class: 1.2002 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:36:55] 	Iter 405900 Done. | loss1: 0.2471 | loss_class: 0.2464 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:37:45] 	Iter 406000 Done. | loss1: 0.0120 | loss_class: 0.0113 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:38:35] 	Iter 406100 Done. | loss1: 1.1225 | loss_class: 1.1219 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:39:26] 	Iter 406200 Done. | loss1: 0.3535 | loss_class: 0.3530 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|19:40:17] 	Iter 406300 Done. | loss1: 0.6584 | loss_class: 0.6579 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:41:08] 	Iter 406400 Done. | loss1: 0.5706 | loss_class: 0.5699 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:41:58] 	Iter 406500 Done. | loss1: 0.8845 | loss_class: 0.8840 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|19:42:48] 	Iter 406600 Done. | loss1: 0.7121 | loss_class: 0.7116 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:43:38] 	Iter 406700 Done. | loss1: 0.8947 | loss_class: 0.8941 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|19:44:29] 	Iter 406800 Done. | loss1: 0.0178 | loss_class: 0.0171 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:45:19] 	Iter 406900 Done. | loss1: 0.4562 | loss_class: 0.4555 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:46:09] 	Iter 407000 Done. | loss1: 0.2944 | loss_class: 0.2936 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|19:46:59] 	Iter 407100 Done. | loss1: 0.3330 | loss_class: 0.3323 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:47:52] 	Iter 407200 Done. | loss1: 0.8678 | loss_class: 0.8672 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|19:48:43] 	Iter 407300 Done. | loss1: 0.0081 | loss_class: 0.0072 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|19:49:33] 	Iter 407400 Done. | loss1: 0.6509 | loss_class: 0.6501 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|19:50:24] 	Iter 407500 Done. | loss1: 1.9034 | loss_class: 1.9029 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:51:14] 	Iter 407600 Done. | loss1: 0.1421 | loss_class: 0.1414 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:52:05] 	Iter 407700 Done. | loss1: 1.0707 | loss_class: 1.0701 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|19:52:55] 	Iter 407800 Done. | loss1: 0.3180 | loss_class: 0.3174 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|19:53:46] 	Iter 407900 Done. | loss1: 2.1233 | loss_class: 2.1228 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:54:37] 	Iter 408000 Done. | loss1: 0.9837 | loss_class: 0.9830 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:55:27] 	Iter 408100 Done. | loss1: 0.2201 | loss_class: 0.2195 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|19:56:17] 	Iter 408200 Done. | loss1: 0.4163 | loss_class: 0.4157 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|19:57:08] 	Iter 408300 Done. | loss1: 0.5255 | loss_class: 0.5247 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|19:58:01] 	Iter 408400 Done. | loss1: 0.7022 | loss_class: 0.7016 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|19:58:51] 	Iter 408500 Done. | loss1: 0.0435 | loss_class: 0.0427 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|19:59:41] 	Iter 408600 Done. | loss1: 1.8632 | loss_class: 1.8627 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:00:31] 	Iter 408700 Done. | loss1: 0.1160 | loss_class: 0.1155 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:01:21] 	Iter 408800 Done. | loss1: 0.1811 | loss_class: 0.1804 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:02:11] 	Iter 408900 Done. | loss1: 0.2750 | loss_class: 0.2744 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:03:00] 	Iter 409000 Done. | loss1: 0.3482 | loss_class: 0.3475 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:03:50] 	Iter 409100 Done. | loss1: 0.6120 | loss_class: 0.6115 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:04:41] 	Iter 409200 Done. | loss1: 0.6689 | loss_class: 0.6685 | loss_recon: 0.0004 | lr: 0.010000
[06.24.21|20:05:32] 	Iter 409300 Done. | loss1: 1.0705 | loss_class: 1.0699 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:06:23] 	Iter 409400 Done. | loss1: 0.6086 | loss_class: 0.6078 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|20:07:17] 	Iter 409500 Done. | loss1: 0.4725 | loss_class: 0.4716 | loss_recon: 0.0010 | lr: 0.010000
[06.24.21|20:08:07] 	Iter 409600 Done. | loss1: 0.3243 | loss_class: 0.3236 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:08:58] 	Iter 409700 Done. | loss1: 0.7577 | loss_class: 0.7570 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:09:47] 	Iter 409800 Done. | loss1: 0.4745 | loss_class: 0.4737 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|20:10:37] 	Iter 409900 Done. | loss1: 0.0556 | loss_class: 0.0550 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:11:26] 	Iter 410000 Done. | loss1: 0.8353 | loss_class: 0.8348 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:12:16] 	Iter 410100 Done. | loss1: 0.9724 | loss_class: 0.9719 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:13:05] 	Iter 410200 Done. | loss1: 0.5108 | loss_class: 0.5100 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|20:13:54] 	Iter 410300 Done. | loss1: 0.8594 | loss_class: 0.8589 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:14:45] 	Iter 410400 Done. | loss1: 0.5254 | loss_class: 0.5249 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:15:36] 	Iter 410500 Done. | loss1: 0.0029 | loss_class: 0.0024 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:16:26] 	Iter 410600 Done. | loss1: 0.5916 | loss_class: 0.5912 | loss_recon: 0.0004 | lr: 0.010000
[06.24.21|20:17:16] 	Iter 410700 Done. | loss1: 1.4048 | loss_class: 1.4044 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:18:06] 	Iter 410800 Done. | loss1: 0.8627 | loss_class: 0.8620 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:18:56] 	Iter 410900 Done. | loss1: 1.2894 | loss_class: 1.2888 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:18:57] 	mean_loss1: 0.6105061912606624
[06.24.21|20:18:57] 	mean_loss_class: 0.609860159029622
[06.24.21|20:18:57] 	mean_loss_recon: 0.0006460323913319873
[06.24.21|20:18:57] Time consumption:
[06.24.21|20:18:57] Done.
[06.24.21|20:18:57] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch50_model1.pt.
[06.24.21|20:18:57] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch50_model2.pt.
[06.24.21|20:18:57] Training epoch: 51
[06.24.21|20:19:47] 	Iter 411000 Done. | loss1: 0.0330 | loss_class: 0.0326 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:20:37] 	Iter 411100 Done. | loss1: 0.3381 | loss_class: 0.3374 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:21:27] 	Iter 411200 Done. | loss1: 0.6621 | loss_class: 0.6615 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:22:17] 	Iter 411300 Done. | loss1: 0.4710 | loss_class: 0.4704 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:23:09] 	Iter 411400 Done. | loss1: 0.0932 | loss_class: 0.0925 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:23:59] 	Iter 411500 Done. | loss1: 0.3868 | loss_class: 0.3859 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|20:24:49] 	Iter 411600 Done. | loss1: 0.0033 | loss_class: 0.0028 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:25:40] 	Iter 411700 Done. | loss1: 0.8096 | loss_class: 0.8090 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:26:31] 	Iter 411800 Done. | loss1: 0.4389 | loss_class: 0.4384 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:27:21] 	Iter 411900 Done. | loss1: 0.1690 | loss_class: 0.1684 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:28:11] 	Iter 412000 Done. | loss1: 0.7373 | loss_class: 0.7363 | loss_recon: 0.0010 | lr: 0.010000
[06.24.21|20:29:00] 	Iter 412100 Done. | loss1: 0.2520 | loss_class: 0.2512 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|20:29:50] 	Iter 412200 Done. | loss1: 0.0742 | loss_class: 0.0735 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:30:40] 	Iter 412300 Done. | loss1: 0.4303 | loss_class: 0.4297 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:31:30] 	Iter 412400 Done. | loss1: 0.6668 | loss_class: 0.6661 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:32:21] 	Iter 412500 Done. | loss1: 0.8932 | loss_class: 0.8923 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|20:33:11] 	Iter 412600 Done. | loss1: 0.0414 | loss_class: 0.0406 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|20:34:00] 	Iter 412700 Done. | loss1: 0.5652 | loss_class: 0.5643 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|20:34:50] 	Iter 412800 Done. | loss1: 0.1016 | loss_class: 0.1011 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:35:40] 	Iter 412900 Done. | loss1: 0.3283 | loss_class: 0.3277 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:36:30] 	Iter 413000 Done. | loss1: 0.4896 | loss_class: 0.4889 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:37:20] 	Iter 413100 Done. | loss1: 0.8070 | loss_class: 0.8060 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|20:38:10] 	Iter 413200 Done. | loss1: 0.5034 | loss_class: 0.5028 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:39:00] 	Iter 413300 Done. | loss1: 0.5742 | loss_class: 0.5734 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|20:39:50] 	Iter 413400 Done. | loss1: 1.2333 | loss_class: 1.2326 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|20:40:40] 	Iter 413500 Done. | loss1: 0.0602 | loss_class: 0.0595 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:41:30] 	Iter 413600 Done. | loss1: 0.2653 | loss_class: 0.2646 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:42:20] 	Iter 413700 Done. | loss1: 0.2184 | loss_class: 0.2177 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:43:11] 	Iter 413800 Done. | loss1: 0.0532 | loss_class: 0.0526 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:44:01] 	Iter 413900 Done. | loss1: 0.6760 | loss_class: 0.6753 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:44:52] 	Iter 414000 Done. | loss1: 1.1012 | loss_class: 1.1003 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|20:45:42] 	Iter 414100 Done. | loss1: 0.3453 | loss_class: 0.3448 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:46:33] 	Iter 414200 Done. | loss1: 0.3434 | loss_class: 0.3429 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:47:23] 	Iter 414300 Done. | loss1: 1.2241 | loss_class: 1.2232 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|20:48:13] 	Iter 414400 Done. | loss1: 0.6298 | loss_class: 0.6290 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|20:49:03] 	Iter 414500 Done. | loss1: 0.0139 | loss_class: 0.0134 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:49:54] 	Iter 414600 Done. | loss1: 0.0341 | loss_class: 0.0336 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:50:44] 	Iter 414700 Done. | loss1: 0.0372 | loss_class: 0.0365 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:51:34] 	Iter 414800 Done. | loss1: 0.2087 | loss_class: 0.2081 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|20:52:25] 	Iter 414900 Done. | loss1: 0.8712 | loss_class: 0.8705 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:53:15] 	Iter 415000 Done. | loss1: 0.0713 | loss_class: 0.0705 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|20:54:05] 	Iter 415100 Done. | loss1: 0.0481 | loss_class: 0.0475 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:54:55] 	Iter 415200 Done. | loss1: 0.5081 | loss_class: 0.5076 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:55:46] 	Iter 415300 Done. | loss1: 0.7080 | loss_class: 0.7075 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:56:39] 	Iter 415400 Done. | loss1: 0.3119 | loss_class: 0.3111 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|20:57:34] 	Iter 415500 Done. | loss1: 0.6342 | loss_class: 0.6334 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|20:58:23] 	Iter 415600 Done. | loss1: 0.4039 | loss_class: 0.4034 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|20:59:13] 	Iter 415700 Done. | loss1: 0.4747 | loss_class: 0.4739 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|21:00:03] 	Iter 415800 Done. | loss1: 0.6709 | loss_class: 0.6702 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:00:53] 	Iter 415900 Done. | loss1: 0.3468 | loss_class: 0.3462 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:01:43] 	Iter 416000 Done. | loss1: 0.3341 | loss_class: 0.3334 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:02:36] 	Iter 416100 Done. | loss1: 0.2846 | loss_class: 0.2840 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:03:26] 	Iter 416200 Done. | loss1: 0.0346 | loss_class: 0.0341 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:04:16] 	Iter 416300 Done. | loss1: 0.1467 | loss_class: 0.1460 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:05:09] 	Iter 416400 Done. | loss1: 0.0028 | loss_class: 0.0021 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:06:03] 	Iter 416500 Done. | loss1: 0.7437 | loss_class: 0.7432 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:06:53] 	Iter 416600 Done. | loss1: 3.2397 | loss_class: 3.2389 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|21:07:43] 	Iter 416700 Done. | loss1: 0.9081 | loss_class: 0.9074 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:08:34] 	Iter 416800 Done. | loss1: 1.6951 | loss_class: 1.6943 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|21:09:23] 	Iter 416900 Done. | loss1: 0.0492 | loss_class: 0.0485 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:10:13] 	Iter 417000 Done. | loss1: 0.3509 | loss_class: 0.3503 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:11:03] 	Iter 417100 Done. | loss1: 0.4768 | loss_class: 0.4761 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:11:55] 	Iter 417200 Done. | loss1: 0.9465 | loss_class: 0.9459 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:12:46] 	Iter 417300 Done. | loss1: 0.4253 | loss_class: 0.4248 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:13:36] 	Iter 417400 Done. | loss1: 0.2164 | loss_class: 0.2157 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:14:27] 	Iter 417500 Done. | loss1: 0.4457 | loss_class: 0.4449 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|21:15:17] 	Iter 417600 Done. | loss1: 0.2026 | loss_class: 0.2021 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:16:08] 	Iter 417700 Done. | loss1: 0.1085 | loss_class: 0.1078 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|21:17:00] 	Iter 417800 Done. | loss1: 0.7287 | loss_class: 0.7282 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:17:55] 	Iter 417900 Done. | loss1: 0.8047 | loss_class: 0.8042 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:18:48] 	Iter 418000 Done. | loss1: 0.4569 | loss_class: 0.4562 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:19:39] 	Iter 418100 Done. | loss1: 0.1033 | loss_class: 0.1026 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:20:29] 	Iter 418200 Done. | loss1: 0.2653 | loss_class: 0.2647 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:21:20] 	Iter 418300 Done. | loss1: 0.1287 | loss_class: 0.1283 | loss_recon: 0.0004 | lr: 0.010000
[06.24.21|21:22:10] 	Iter 418400 Done. | loss1: 0.3796 | loss_class: 0.3790 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:23:01] 	Iter 418500 Done. | loss1: 0.7293 | loss_class: 0.7288 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:23:51] 	Iter 418600 Done. | loss1: 0.6463 | loss_class: 0.6456 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:24:46] 	Iter 418700 Done. | loss1: 0.1516 | loss_class: 0.1509 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:25:39] 	Iter 418800 Done. | loss1: 0.1722 | loss_class: 0.1715 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:26:31] 	Iter 418900 Done. | loss1: 0.9296 | loss_class: 0.9291 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:27:21] 	Iter 419000 Done. | loss1: 0.1737 | loss_class: 0.1732 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:28:12] 	Iter 419100 Done. | loss1: 0.7631 | loss_class: 0.7625 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:29:02] 	Iter 419200 Done. | loss1: 0.5087 | loss_class: 0.5082 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:29:53] 	Iter 419300 Done. | loss1: 1.2032 | loss_class: 1.2026 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:30:43] 	Iter 419400 Done. | loss1: 0.4359 | loss_class: 0.4353 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:31:33] 	Iter 419500 Done. | loss1: 0.2587 | loss_class: 0.2580 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:32:24] 	Iter 419600 Done. | loss1: 0.1732 | loss_class: 0.1726 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:33:14] 	Iter 419700 Done. | loss1: 0.1151 | loss_class: 0.1145 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:34:04] 	Iter 419800 Done. | loss1: 0.1291 | loss_class: 0.1284 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:34:56] 	Iter 419900 Done. | loss1: 0.7269 | loss_class: 0.7265 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:35:49] 	Iter 420000 Done. | loss1: 0.1735 | loss_class: 0.1725 | loss_recon: 0.0010 | lr: 0.010000
[06.24.21|21:36:41] 	Iter 420100 Done. | loss1: 1.1186 | loss_class: 1.1180 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:37:31] 	Iter 420200 Done. | loss1: 0.4728 | loss_class: 0.4721 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:38:21] 	Iter 420300 Done. | loss1: 0.4230 | loss_class: 0.4226 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:39:11] 	Iter 420400 Done. | loss1: 0.2438 | loss_class: 0.2432 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:40:02] 	Iter 420500 Done. | loss1: 0.5856 | loss_class: 0.5850 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:40:53] 	Iter 420600 Done. | loss1: 1.2800 | loss_class: 1.2794 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:41:43] 	Iter 420700 Done. | loss1: 0.4219 | loss_class: 0.4213 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:42:33] 	Iter 420800 Done. | loss1: 0.4372 | loss_class: 0.4366 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:43:23] 	Iter 420900 Done. | loss1: 0.0105 | loss_class: 0.0096 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|21:43:35] 	mean_loss1: 0.5217668346129916
[06.24.21|21:43:35] 	mean_loss_class: 0.5211197568208714
[06.24.21|21:43:35] 	mean_loss_recon: 0.0006470781615005059
[06.24.21|21:43:35] Time consumption:
[06.24.21|21:43:35] Done.
[06.24.21|21:43:35] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch51_model1.pt.
[06.24.21|21:43:35] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch51_model2.pt.
[06.24.21|21:43:35] Training epoch: 52
[06.24.21|21:44:14] 	Iter 421000 Done. | loss1: 0.3928 | loss_class: 0.3920 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|21:45:05] 	Iter 421100 Done. | loss1: 0.0838 | loss_class: 0.0831 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:45:57] 	Iter 421200 Done. | loss1: 0.0320 | loss_class: 0.0311 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|21:46:47] 	Iter 421300 Done. | loss1: 0.3609 | loss_class: 0.3601 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|21:47:36] 	Iter 421400 Done. | loss1: 0.2694 | loss_class: 0.2688 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:48:25] 	Iter 421500 Done. | loss1: 0.1721 | loss_class: 0.1716 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:49:16] 	Iter 421600 Done. | loss1: 0.1642 | loss_class: 0.1636 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:50:06] 	Iter 421700 Done. | loss1: 0.4700 | loss_class: 0.4695 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:50:56] 	Iter 421800 Done. | loss1: 1.4035 | loss_class: 1.4030 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|21:51:46] 	Iter 421900 Done. | loss1: 0.1168 | loss_class: 0.1161 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:52:37] 	Iter 422000 Done. | loss1: 2.0873 | loss_class: 2.0864 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|21:53:27] 	Iter 422100 Done. | loss1: 0.3476 | loss_class: 0.3469 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:54:17] 	Iter 422200 Done. | loss1: 0.1913 | loss_class: 0.1907 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:55:08] 	Iter 422300 Done. | loss1: 0.0591 | loss_class: 0.0584 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|21:55:58] 	Iter 422400 Done. | loss1: 0.1962 | loss_class: 0.1955 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|21:56:48] 	Iter 422500 Done. | loss1: 0.1107 | loss_class: 0.1100 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:57:39] 	Iter 422600 Done. | loss1: 0.3394 | loss_class: 0.3386 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|21:58:33] 	Iter 422700 Done. | loss1: 0.5265 | loss_class: 0.5259 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|21:59:25] 	Iter 422800 Done. | loss1: 0.5282 | loss_class: 0.5275 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:00:15] 	Iter 422900 Done. | loss1: 0.0492 | loss_class: 0.0486 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:01:05] 	Iter 423000 Done. | loss1: 0.6589 | loss_class: 0.6582 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:01:57] 	Iter 423100 Done. | loss1: 2.3095 | loss_class: 2.3090 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|22:02:50] 	Iter 423200 Done. | loss1: 0.3545 | loss_class: 0.3538 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:03:42] 	Iter 423300 Done. | loss1: 0.5890 | loss_class: 0.5884 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:04:34] 	Iter 423400 Done. | loss1: 0.6198 | loss_class: 0.6192 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:05:27] 	Iter 423500 Done. | loss1: 1.1689 | loss_class: 1.1682 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:06:18] 	Iter 423600 Done. | loss1: 0.2155 | loss_class: 0.2148 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:07:08] 	Iter 423700 Done. | loss1: 0.8850 | loss_class: 0.8846 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|22:07:58] 	Iter 423800 Done. | loss1: 0.4105 | loss_class: 0.4097 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:08:49] 	Iter 423900 Done. | loss1: 0.3544 | loss_class: 0.3537 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:09:40] 	Iter 424000 Done. | loss1: 1.0545 | loss_class: 1.0539 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:10:30] 	Iter 424100 Done. | loss1: 0.4252 | loss_class: 0.4245 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:11:21] 	Iter 424200 Done. | loss1: 0.6795 | loss_class: 0.6790 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|22:12:12] 	Iter 424300 Done. | loss1: 0.0285 | loss_class: 0.0277 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|22:13:02] 	Iter 424400 Done. | loss1: 0.0039 | loss_class: 0.0030 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|22:13:53] 	Iter 424500 Done. | loss1: 0.0389 | loss_class: 0.0382 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:14:43] 	Iter 424600 Done. | loss1: 0.0155 | loss_class: 0.0149 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:15:33] 	Iter 424700 Done. | loss1: 0.0993 | loss_class: 0.0987 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:16:24] 	Iter 424800 Done. | loss1: 0.0861 | loss_class: 0.0855 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:17:14] 	Iter 424900 Done. | loss1: 0.0458 | loss_class: 0.0453 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|22:18:05] 	Iter 425000 Done. | loss1: 0.3501 | loss_class: 0.3496 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:18:55] 	Iter 425100 Done. | loss1: 0.2956 | loss_class: 0.2950 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:19:45] 	Iter 425200 Done. | loss1: 0.2636 | loss_class: 0.2627 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|22:20:35] 	Iter 425300 Done. | loss1: 0.2478 | loss_class: 0.2472 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|22:21:26] 	Iter 425400 Done. | loss1: 0.0327 | loss_class: 0.0322 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|22:22:17] 	Iter 425500 Done. | loss1: 0.6513 | loss_class: 0.6506 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|22:23:07] 	Iter 425600 Done. | loss1: 0.0229 | loss_class: 0.0220 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|22:23:58] 	Iter 425700 Done. | loss1: 0.0387 | loss_class: 0.0380 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:24:48] 	Iter 425800 Done. | loss1: 1.0249 | loss_class: 1.0243 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:25:38] 	Iter 425900 Done. | loss1: 0.4263 | loss_class: 0.4256 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:26:29] 	Iter 426000 Done. | loss1: 1.3235 | loss_class: 1.3229 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:27:20] 	Iter 426100 Done. | loss1: 0.0534 | loss_class: 0.0526 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|22:28:10] 	Iter 426200 Done. | loss1: 0.0169 | loss_class: 0.0162 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:29:00] 	Iter 426300 Done. | loss1: 0.2778 | loss_class: 0.2772 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:29:50] 	Iter 426400 Done. | loss1: 0.3824 | loss_class: 0.3818 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:30:40] 	Iter 426500 Done. | loss1: 1.0823 | loss_class: 1.0817 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:31:30] 	Iter 426600 Done. | loss1: 0.1136 | loss_class: 0.1129 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:32:20] 	Iter 426700 Done. | loss1: 0.4059 | loss_class: 0.4053 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:33:10] 	Iter 426800 Done. | loss1: 0.5851 | loss_class: 0.5844 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:34:00] 	Iter 426900 Done. | loss1: 0.1655 | loss_class: 0.1650 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|22:34:50] 	Iter 427000 Done. | loss1: 0.1451 | loss_class: 0.1444 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:35:40] 	Iter 427100 Done. | loss1: 0.5920 | loss_class: 0.5912 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|22:36:31] 	Iter 427200 Done. | loss1: 0.2865 | loss_class: 0.2859 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:37:22] 	Iter 427300 Done. | loss1: 0.2539 | loss_class: 0.2534 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|22:38:12] 	Iter 427400 Done. | loss1: 0.2056 | loss_class: 0.2047 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|22:39:02] 	Iter 427500 Done. | loss1: 0.8680 | loss_class: 0.8675 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|22:39:52] 	Iter 427600 Done. | loss1: 0.1694 | loss_class: 0.1687 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:40:47] 	Iter 427700 Done. | loss1: 0.3122 | loss_class: 0.3116 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:41:42] 	Iter 427800 Done. | loss1: 0.1654 | loss_class: 0.1648 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:42:32] 	Iter 427900 Done. | loss1: 0.0347 | loss_class: 0.0341 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:43:22] 	Iter 428000 Done. | loss1: 0.2113 | loss_class: 0.2107 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:44:12] 	Iter 428100 Done. | loss1: 0.0344 | loss_class: 0.0339 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:45:03] 	Iter 428200 Done. | loss1: 1.1345 | loss_class: 1.1337 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|22:45:54] 	Iter 428300 Done. | loss1: 1.5214 | loss_class: 1.5206 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:46:44] 	Iter 428400 Done. | loss1: 0.0048 | loss_class: 0.0041 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:47:35] 	Iter 428500 Done. | loss1: 0.0063 | loss_class: 0.0056 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:48:25] 	Iter 428600 Done. | loss1: 0.5253 | loss_class: 0.5246 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:49:16] 	Iter 428700 Done. | loss1: 1.0630 | loss_class: 1.0622 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|22:50:07] 	Iter 428800 Done. | loss1: 0.9069 | loss_class: 0.9062 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:50:56] 	Iter 428900 Done. | loss1: 0.3105 | loss_class: 0.3099 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|22:51:46] 	Iter 429000 Done. | loss1: 1.1250 | loss_class: 1.1244 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:52:35] 	Iter 429100 Done. | loss1: 0.5097 | loss_class: 0.5091 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:53:25] 	Iter 429200 Done. | loss1: 0.4625 | loss_class: 0.4617 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|22:54:15] 	Iter 429300 Done. | loss1: 1.8130 | loss_class: 1.8123 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:55:05] 	Iter 429400 Done. | loss1: 0.1067 | loss_class: 0.1060 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|22:55:56] 	Iter 429500 Done. | loss1: 0.6441 | loss_class: 0.6435 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|22:56:46] 	Iter 429600 Done. | loss1: 0.0781 | loss_class: 0.0773 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|22:57:37] 	Iter 429700 Done. | loss1: 0.0270 | loss_class: 0.0264 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:58:28] 	Iter 429800 Done. | loss1: 0.3142 | loss_class: 0.3136 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|22:59:18] 	Iter 429900 Done. | loss1: 0.0376 | loss_class: 0.0369 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:00:08] 	Iter 430000 Done. | loss1: 0.0519 | loss_class: 0.0512 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:00:58] 	Iter 430100 Done. | loss1: 0.8296 | loss_class: 0.8287 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|23:01:48] 	Iter 430200 Done. | loss1: 0.0327 | loss_class: 0.0319 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|23:02:38] 	Iter 430300 Done. | loss1: 0.1461 | loss_class: 0.1456 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|23:03:29] 	Iter 430400 Done. | loss1: 0.6212 | loss_class: 0.6204 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|23:04:19] 	Iter 430500 Done. | loss1: 0.1459 | loss_class: 0.1453 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:05:09] 	Iter 430600 Done. | loss1: 0.0847 | loss_class: 0.0842 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|23:05:59] 	Iter 430700 Done. | loss1: 0.0727 | loss_class: 0.0721 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:06:49] 	Iter 430800 Done. | loss1: 1.6256 | loss_class: 1.6250 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:07:39] 	Iter 430900 Done. | loss1: 0.2046 | loss_class: 0.2040 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:08:02] 	mean_loss1: 0.4917170496424846
[06.24.21|23:08:02] 	mean_loss_class: 0.49107115547406854
[06.24.21|23:08:02] 	mean_loss_recon: 0.0006458938004818118
[06.24.21|23:08:02] Time consumption:
[06.24.21|23:08:02] Done.
[06.24.21|23:08:02] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch52_model1.pt.
[06.24.21|23:08:02] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch52_model2.pt.
[06.24.21|23:08:02] Training epoch: 53
[06.24.21|23:08:32] 	Iter 431000 Done. | loss1: 0.0388 | loss_class: 0.0382 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:09:22] 	Iter 431100 Done. | loss1: 0.6116 | loss_class: 0.6106 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|23:10:12] 	Iter 431200 Done. | loss1: 0.1384 | loss_class: 0.1379 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|23:11:02] 	Iter 431300 Done. | loss1: 0.3084 | loss_class: 0.3076 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|23:11:52] 	Iter 431400 Done. | loss1: 0.2788 | loss_class: 0.2783 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|23:12:42] 	Iter 431500 Done. | loss1: 0.8730 | loss_class: 0.8724 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:13:32] 	Iter 431600 Done. | loss1: 0.0119 | loss_class: 0.0111 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:14:24] 	Iter 431700 Done. | loss1: 0.0816 | loss_class: 0.0810 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:15:14] 	Iter 431800 Done. | loss1: 0.1571 | loss_class: 0.1566 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|23:16:05] 	Iter 431900 Done. | loss1: 0.0546 | loss_class: 0.0539 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:16:55] 	Iter 432000 Done. | loss1: 0.0230 | loss_class: 0.0223 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:17:45] 	Iter 432100 Done. | loss1: 0.3053 | loss_class: 0.3048 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|23:18:36] 	Iter 432200 Done. | loss1: 0.4046 | loss_class: 0.4040 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:19:26] 	Iter 432300 Done. | loss1: 0.0450 | loss_class: 0.0444 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:20:16] 	Iter 432400 Done. | loss1: 0.3967 | loss_class: 0.3960 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:21:06] 	Iter 432500 Done. | loss1: 0.3053 | loss_class: 0.3045 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:21:55] 	Iter 432600 Done. | loss1: 0.1858 | loss_class: 0.1851 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:22:45] 	Iter 432700 Done. | loss1: 0.8359 | loss_class: 0.8354 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|23:23:35] 	Iter 432800 Done. | loss1: 0.2086 | loss_class: 0.2079 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:24:25] 	Iter 432900 Done. | loss1: 0.9997 | loss_class: 0.9991 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|23:25:14] 	Iter 433000 Done. | loss1: 0.0409 | loss_class: 0.0402 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:26:04] 	Iter 433100 Done. | loss1: 0.4140 | loss_class: 0.4136 | loss_recon: 0.0004 | lr: 0.010000
[06.24.21|23:26:54] 	Iter 433200 Done. | loss1: 0.0119 | loss_class: 0.0112 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:27:43] 	Iter 433300 Done. | loss1: 0.3794 | loss_class: 0.3786 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:28:35] 	Iter 433400 Done. | loss1: 0.1935 | loss_class: 0.1928 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:29:26] 	Iter 433500 Done. | loss1: 0.5913 | loss_class: 0.5908 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:30:16] 	Iter 433600 Done. | loss1: 1.6395 | loss_class: 1.6387 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:31:05] 	Iter 433700 Done. | loss1: 0.0046 | loss_class: 0.0041 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|23:31:55] 	Iter 433800 Done. | loss1: 1.1925 | loss_class: 1.1919 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:32:44] 	Iter 433900 Done. | loss1: 0.4359 | loss_class: 0.4353 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:33:34] 	Iter 434000 Done. | loss1: 0.2545 | loss_class: 0.2539 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:34:26] 	Iter 434100 Done. | loss1: 0.1879 | loss_class: 0.1872 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:35:21] 	Iter 434200 Done. | loss1: 0.1292 | loss_class: 0.1285 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:36:15] 	Iter 434300 Done. | loss1: 0.0710 | loss_class: 0.0703 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:37:05] 	Iter 434400 Done. | loss1: 0.5543 | loss_class: 0.5537 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:37:55] 	Iter 434500 Done. | loss1: 0.0686 | loss_class: 0.0679 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:38:46] 	Iter 434600 Done. | loss1: 0.3148 | loss_class: 0.3139 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|23:39:35] 	Iter 434700 Done. | loss1: 0.2257 | loss_class: 0.2250 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:40:25] 	Iter 434800 Done. | loss1: 0.0821 | loss_class: 0.0817 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|23:41:15] 	Iter 434900 Done. | loss1: 0.3380 | loss_class: 0.3374 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:42:05] 	Iter 435000 Done. | loss1: 0.1024 | loss_class: 0.1017 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:42:55] 	Iter 435100 Done. | loss1: 0.7040 | loss_class: 0.7032 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|23:43:45] 	Iter 435200 Done. | loss1: 0.0812 | loss_class: 0.0805 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:44:35] 	Iter 435300 Done. | loss1: 0.0257 | loss_class: 0.0250 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:45:25] 	Iter 435400 Done. | loss1: 0.1115 | loss_class: 0.1108 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:46:15] 	Iter 435500 Done. | loss1: 0.9291 | loss_class: 0.9285 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:47:05] 	Iter 435600 Done. | loss1: 0.4579 | loss_class: 0.4573 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:47:55] 	Iter 435700 Done. | loss1: 0.1004 | loss_class: 0.0998 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:48:45] 	Iter 435800 Done. | loss1: 0.5539 | loss_class: 0.5529 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|23:49:35] 	Iter 435900 Done. | loss1: 0.0439 | loss_class: 0.0430 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|23:50:25] 	Iter 436000 Done. | loss1: 2.6528 | loss_class: 2.6521 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:51:15] 	Iter 436100 Done. | loss1: 0.5055 | loss_class: 0.5048 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:52:05] 	Iter 436200 Done. | loss1: 0.6109 | loss_class: 0.6101 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|23:52:56] 	Iter 436300 Done. | loss1: 0.1028 | loss_class: 0.1022 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:53:49] 	Iter 436400 Done. | loss1: 0.5474 | loss_class: 0.5468 | loss_recon: 0.0006 | lr: 0.010000
[06.24.21|23:54:39] 	Iter 436500 Done. | loss1: 0.1978 | loss_class: 0.1972 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:55:30] 	Iter 436600 Done. | loss1: 0.1193 | loss_class: 0.1185 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|23:56:22] 	Iter 436700 Done. | loss1: 0.1348 | loss_class: 0.1340 | loss_recon: 0.0009 | lr: 0.010000
[06.24.21|23:57:13] 	Iter 436800 Done. | loss1: 0.0725 | loss_class: 0.0719 | loss_recon: 0.0007 | lr: 0.010000
[06.24.21|23:58:06] 	Iter 436900 Done. | loss1: 0.7432 | loss_class: 0.7424 | loss_recon: 0.0008 | lr: 0.010000
[06.24.21|23:58:58] 	Iter 437000 Done. | loss1: 0.0994 | loss_class: 0.0989 | loss_recon: 0.0005 | lr: 0.010000
[06.24.21|23:59:48] 	Iter 437100 Done. | loss1: 1.1913 | loss_class: 1.1906 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:00:40] 	Iter 437200 Done. | loss1: 1.2355 | loss_class: 1.2348 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:01:31] 	Iter 437300 Done. | loss1: 1.7051 | loss_class: 1.7044 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:02:22] 	Iter 437400 Done. | loss1: 0.4252 | loss_class: 0.4247 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|00:03:12] 	Iter 437500 Done. | loss1: 0.9004 | loss_class: 0.8999 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|00:04:03] 	Iter 437600 Done. | loss1: 0.1832 | loss_class: 0.1825 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:04:53] 	Iter 437700 Done. | loss1: 0.3243 | loss_class: 0.3236 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:05:43] 	Iter 437800 Done. | loss1: 0.0576 | loss_class: 0.0570 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:06:33] 	Iter 437900 Done. | loss1: 0.8434 | loss_class: 0.8428 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:07:27] 	Iter 438000 Done. | loss1: 0.0676 | loss_class: 0.0669 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:08:20] 	Iter 438100 Done. | loss1: 0.3960 | loss_class: 0.3956 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|00:09:11] 	Iter 438200 Done. | loss1: 0.2818 | loss_class: 0.2813 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|00:10:01] 	Iter 438300 Done. | loss1: 0.1049 | loss_class: 0.1043 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:10:52] 	Iter 438400 Done. | loss1: 0.0272 | loss_class: 0.0266 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:11:45] 	Iter 438500 Done. | loss1: 0.0283 | loss_class: 0.0276 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:12:40] 	Iter 438600 Done. | loss1: 0.6485 | loss_class: 0.6480 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|00:13:32] 	Iter 438700 Done. | loss1: 0.0457 | loss_class: 0.0449 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|00:14:22] 	Iter 438800 Done. | loss1: 0.6508 | loss_class: 0.6503 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|00:15:13] 	Iter 438900 Done. | loss1: 0.2509 | loss_class: 0.2501 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|00:16:03] 	Iter 439000 Done. | loss1: 0.7977 | loss_class: 0.7971 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:16:54] 	Iter 439100 Done. | loss1: 0.0451 | loss_class: 0.0443 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|00:17:44] 	Iter 439200 Done. | loss1: 0.8532 | loss_class: 0.8524 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:18:35] 	Iter 439300 Done. | loss1: 0.5412 | loss_class: 0.5407 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:19:25] 	Iter 439400 Done. | loss1: 1.9801 | loss_class: 1.9795 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:20:20] 	Iter 439500 Done. | loss1: 0.2211 | loss_class: 0.2204 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:21:11] 	Iter 439600 Done. | loss1: 0.4233 | loss_class: 0.4226 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|00:22:01] 	Iter 439700 Done. | loss1: 0.5171 | loss_class: 0.5165 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:22:50] 	Iter 439800 Done. | loss1: 0.2631 | loss_class: 0.2624 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:23:41] 	Iter 439900 Done. | loss1: 1.4614 | loss_class: 1.4608 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:24:32] 	Iter 440000 Done. | loss1: 0.6414 | loss_class: 0.6406 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:25:23] 	Iter 440100 Done. | loss1: 0.5072 | loss_class: 0.5067 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:26:14] 	Iter 440200 Done. | loss1: 0.1763 | loss_class: 0.1756 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:27:04] 	Iter 440300 Done. | loss1: 0.5031 | loss_class: 0.5023 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|00:27:54] 	Iter 440400 Done. | loss1: 0.4099 | loss_class: 0.4094 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:28:44] 	Iter 440500 Done. | loss1: 0.1668 | loss_class: 0.1662 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:29:34] 	Iter 440600 Done. | loss1: 0.1501 | loss_class: 0.1496 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|00:30:25] 	Iter 440700 Done. | loss1: 0.0674 | loss_class: 0.0665 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|00:31:15] 	Iter 440800 Done. | loss1: 0.0755 | loss_class: 0.0748 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:32:05] 	Iter 440900 Done. | loss1: 0.4181 | loss_class: 0.4175 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|00:32:39] 	mean_loss1: 0.47755253358464467
[06.25.21|00:32:39] 	mean_loss_class: 0.4769057938257965
[06.25.21|00:32:39] 	mean_loss_recon: 0.0006467399431600337
[06.25.21|00:32:39] Time consumption:
[06.25.21|00:32:39] Done.
[06.25.21|00:32:39] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch53_model1.pt.
[06.25.21|00:32:39] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch53_model2.pt.
[06.25.21|00:32:39] Training epoch: 54
[06.25.21|00:32:55] 	Iter 441000 Done. | loss1: 0.0465 | loss_class: 0.0459 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|00:33:45] 	Iter 441100 Done. | loss1: 0.8529 | loss_class: 0.8524 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:34:34] 	Iter 441200 Done. | loss1: 0.2543 | loss_class: 0.2537 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:35:24] 	Iter 441300 Done. | loss1: 0.1416 | loss_class: 0.1409 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:36:14] 	Iter 441400 Done. | loss1: 0.0665 | loss_class: 0.0658 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:37:06] 	Iter 441500 Done. | loss1: 0.3713 | loss_class: 0.3706 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:37:56] 	Iter 441600 Done. | loss1: 0.6304 | loss_class: 0.6296 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|00:38:46] 	Iter 441700 Done. | loss1: 0.5332 | loss_class: 0.5322 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|00:39:37] 	Iter 441800 Done. | loss1: 0.4147 | loss_class: 0.4142 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|00:40:28] 	Iter 441900 Done. | loss1: 0.5013 | loss_class: 0.5006 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:41:18] 	Iter 442000 Done. | loss1: 0.2728 | loss_class: 0.2723 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|00:42:09] 	Iter 442100 Done. | loss1: 0.0382 | loss_class: 0.0376 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:42:59] 	Iter 442200 Done. | loss1: 0.7407 | loss_class: 0.7400 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:43:49] 	Iter 442300 Done. | loss1: 0.8624 | loss_class: 0.8616 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|00:44:41] 	Iter 442400 Done. | loss1: 0.0686 | loss_class: 0.0679 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:45:31] 	Iter 442500 Done. | loss1: 0.5761 | loss_class: 0.5755 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:46:21] 	Iter 442600 Done. | loss1: 0.0043 | loss_class: 0.0038 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|00:47:12] 	Iter 442700 Done. | loss1: 0.0606 | loss_class: 0.0602 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|00:48:02] 	Iter 442800 Done. | loss1: 0.3798 | loss_class: 0.3792 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:48:53] 	Iter 442900 Done. | loss1: 0.7136 | loss_class: 0.7131 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|00:49:44] 	Iter 443000 Done. | loss1: 0.8426 | loss_class: 0.8420 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:50:34] 	Iter 443100 Done. | loss1: 0.2513 | loss_class: 0.2507 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:51:25] 	Iter 443200 Done. | loss1: 0.1185 | loss_class: 0.1178 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:52:15] 	Iter 443300 Done. | loss1: 0.5460 | loss_class: 0.5454 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:53:06] 	Iter 443400 Done. | loss1: 0.7997 | loss_class: 0.7990 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|00:53:56] 	Iter 443500 Done. | loss1: 0.1950 | loss_class: 0.1943 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:54:47] 	Iter 443600 Done. | loss1: 0.0623 | loss_class: 0.0617 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:55:36] 	Iter 443700 Done. | loss1: 0.4979 | loss_class: 0.4974 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:56:26] 	Iter 443800 Done. | loss1: 0.7834 | loss_class: 0.7826 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|00:57:16] 	Iter 443900 Done. | loss1: 0.4849 | loss_class: 0.4844 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|00:58:06] 	Iter 444000 Done. | loss1: 0.5579 | loss_class: 0.5572 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:58:56] 	Iter 444100 Done. | loss1: 0.5397 | loss_class: 0.5391 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|00:59:46] 	Iter 444200 Done. | loss1: 0.4197 | loss_class: 0.4189 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:00:37] 	Iter 444300 Done. | loss1: 0.0715 | loss_class: 0.0709 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:01:27] 	Iter 444400 Done. | loss1: 0.1186 | loss_class: 0.1181 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|01:02:18] 	Iter 444500 Done. | loss1: 0.0616 | loss_class: 0.0610 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:03:08] 	Iter 444600 Done. | loss1: 0.3119 | loss_class: 0.3114 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:03:59] 	Iter 444700 Done. | loss1: 0.0467 | loss_class: 0.0460 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:04:49] 	Iter 444800 Done. | loss1: 1.3080 | loss_class: 1.3074 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:05:39] 	Iter 444900 Done. | loss1: 0.1206 | loss_class: 0.1197 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|01:06:30] 	Iter 445000 Done. | loss1: 0.0616 | loss_class: 0.0608 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:07:20] 	Iter 445100 Done. | loss1: 0.4508 | loss_class: 0.4499 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:08:11] 	Iter 445200 Done. | loss1: 0.5361 | loss_class: 0.5355 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:09:01] 	Iter 445300 Done. | loss1: 1.9635 | loss_class: 1.9626 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:09:51] 	Iter 445400 Done. | loss1: 0.5624 | loss_class: 0.5616 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:10:41] 	Iter 445500 Done. | loss1: 0.4316 | loss_class: 0.4311 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|01:11:31] 	Iter 445600 Done. | loss1: 0.0891 | loss_class: 0.0881 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|01:12:21] 	Iter 445700 Done. | loss1: 0.8789 | loss_class: 0.8783 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|01:13:12] 	Iter 445800 Done. | loss1: 0.1336 | loss_class: 0.1330 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:14:02] 	Iter 445900 Done. | loss1: 0.5905 | loss_class: 0.5898 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:14:52] 	Iter 446000 Done. | loss1: 0.1131 | loss_class: 0.1123 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:15:42] 	Iter 446100 Done. | loss1: 0.0099 | loss_class: 0.0093 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:16:32] 	Iter 446200 Done. | loss1: 0.7448 | loss_class: 0.7442 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:17:22] 	Iter 446300 Done. | loss1: 1.0607 | loss_class: 1.0599 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:18:12] 	Iter 446400 Done. | loss1: 0.0389 | loss_class: 0.0384 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|01:19:02] 	Iter 446500 Done. | loss1: 0.2398 | loss_class: 0.2389 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:19:53] 	Iter 446600 Done. | loss1: 0.6168 | loss_class: 0.6162 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:20:43] 	Iter 446700 Done. | loss1: 0.6901 | loss_class: 0.6896 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|01:21:34] 	Iter 446800 Done. | loss1: 0.1192 | loss_class: 0.1184 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:22:24] 	Iter 446900 Done. | loss1: 0.8381 | loss_class: 0.8377 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|01:23:14] 	Iter 447000 Done. | loss1: 0.7684 | loss_class: 0.7678 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:24:04] 	Iter 447100 Done. | loss1: 0.3870 | loss_class: 0.3864 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:24:54] 	Iter 447200 Done. | loss1: 0.3345 | loss_class: 0.3338 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:25:46] 	Iter 447300 Done. | loss1: 0.8830 | loss_class: 0.8822 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:26:36] 	Iter 447400 Done. | loss1: 0.2167 | loss_class: 0.2162 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|01:27:26] 	Iter 447500 Done. | loss1: 0.1356 | loss_class: 0.1349 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:28:16] 	Iter 447600 Done. | loss1: 1.6974 | loss_class: 1.6968 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:29:07] 	Iter 447700 Done. | loss1: 0.1352 | loss_class: 0.1345 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:29:57] 	Iter 447800 Done. | loss1: 0.7704 | loss_class: 0.7697 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:30:46] 	Iter 447900 Done. | loss1: 0.0551 | loss_class: 0.0542 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|01:31:36] 	Iter 448000 Done. | loss1: 0.4007 | loss_class: 0.4001 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:32:26] 	Iter 448100 Done. | loss1: 0.0716 | loss_class: 0.0711 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|01:33:16] 	Iter 448200 Done. | loss1: 0.1041 | loss_class: 0.1034 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:34:06] 	Iter 448300 Done. | loss1: 1.5828 | loss_class: 1.5823 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|01:34:55] 	Iter 448400 Done. | loss1: 0.1270 | loss_class: 0.1265 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|01:35:45] 	Iter 448500 Done. | loss1: 0.7944 | loss_class: 0.7933 | loss_recon: 0.0011 | lr: 0.010000
[06.25.21|01:36:35] 	Iter 448600 Done. | loss1: 0.8275 | loss_class: 0.8270 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:37:25] 	Iter 448700 Done. | loss1: 0.0303 | loss_class: 0.0294 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|01:38:14] 	Iter 448800 Done. | loss1: 0.2128 | loss_class: 0.2122 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:39:04] 	Iter 448900 Done. | loss1: 0.6987 | loss_class: 0.6981 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:39:54] 	Iter 449000 Done. | loss1: 1.0760 | loss_class: 1.0754 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:40:43] 	Iter 449100 Done. | loss1: 0.1450 | loss_class: 0.1445 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|01:41:34] 	Iter 449200 Done. | loss1: 0.0789 | loss_class: 0.0780 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|01:42:24] 	Iter 449300 Done. | loss1: 0.0147 | loss_class: 0.0140 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:43:15] 	Iter 449400 Done. | loss1: 0.0040 | loss_class: 0.0033 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:44:06] 	Iter 449500 Done. | loss1: 0.3542 | loss_class: 0.3536 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:44:57] 	Iter 449600 Done. | loss1: 1.3104 | loss_class: 1.3097 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:45:47] 	Iter 449700 Done. | loss1: 0.4759 | loss_class: 0.4752 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:46:38] 	Iter 449800 Done. | loss1: 0.9496 | loss_class: 0.9489 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:47:28] 	Iter 449900 Done. | loss1: 0.8938 | loss_class: 0.8929 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:48:20] 	Iter 450000 Done. | loss1: 0.8462 | loss_class: 0.8456 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:49:13] 	Iter 450100 Done. | loss1: 1.0608 | loss_class: 1.0602 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:50:04] 	Iter 450200 Done. | loss1: 0.1290 | loss_class: 0.1285 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|01:50:55] 	Iter 450300 Done. | loss1: 0.4497 | loss_class: 0.4491 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:51:46] 	Iter 450400 Done. | loss1: 0.0892 | loss_class: 0.0885 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:52:37] 	Iter 450500 Done. | loss1: 0.7403 | loss_class: 0.7397 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:53:28] 	Iter 450600 Done. | loss1: 0.8100 | loss_class: 0.8093 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|01:54:19] 	Iter 450700 Done. | loss1: 0.7557 | loss_class: 0.7551 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:55:10] 	Iter 450800 Done. | loss1: 0.5387 | loss_class: 0.5380 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|01:56:01] 	Iter 450900 Done. | loss1: 0.7159 | loss_class: 0.7154 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|01:56:46] 	mean_loss1: 0.47039241475798693
[06.25.21|01:56:46] 	mean_loss_class: 0.4697459831703793
[06.25.21|01:56:46] 	mean_loss_recon: 0.0006464315126637954
[06.25.21|01:56:46] Time consumption:
[06.25.21|01:56:46] Done.
[06.25.21|01:56:46] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch54_model1.pt.
[06.25.21|01:56:46] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch54_model2.pt.
[06.25.21|01:56:46] Eval epoch: 54
[06.25.21|02:03:20] 	mean_loss1: 0.7443601865783449
[06.25.21|02:03:20] 	mean_loss_class: 0.7439889732365633
[06.25.21|02:03:20] 	mean_loss_recon: 0.03712141007011713
[06.25.21|02:03:20] 

[06.25.21|02:03:20] 	Top1: 78.49%
[06.25.21|02:03:20] 

[06.25.21|02:03:20] 	Top5: 95.78%
[06.25.21|02:03:20] Done.
[06.25.21|02:03:20] Training epoch: 55
[06.25.21|02:03:26] 	Iter 451000 Done. | loss1: 0.2003 | loss_class: 0.1995 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|02:04:18] 	Iter 451100 Done. | loss1: 0.0109 | loss_class: 0.0101 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:05:10] 	Iter 451200 Done. | loss1: 0.6971 | loss_class: 0.6967 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|02:06:02] 	Iter 451300 Done. | loss1: 0.4843 | loss_class: 0.4836 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:06:54] 	Iter 451400 Done. | loss1: 0.4956 | loss_class: 0.4952 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|02:07:47] 	Iter 451500 Done. | loss1: 0.1386 | loss_class: 0.1379 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:08:39] 	Iter 451600 Done. | loss1: 0.7961 | loss_class: 0.7956 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|02:09:31] 	Iter 451700 Done. | loss1: 0.3335 | loss_class: 0.3330 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|02:10:23] 	Iter 451800 Done. | loss1: 2.7502 | loss_class: 2.7495 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:11:15] 	Iter 451900 Done. | loss1: 0.7651 | loss_class: 0.7642 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|02:12:07] 	Iter 452000 Done. | loss1: 0.1671 | loss_class: 0.1664 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:12:58] 	Iter 452100 Done. | loss1: 0.3561 | loss_class: 0.3554 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:13:50] 	Iter 452200 Done. | loss1: 1.3662 | loss_class: 1.3655 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|02:14:41] 	Iter 452300 Done. | loss1: 0.5189 | loss_class: 0.5184 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|02:15:33] 	Iter 452400 Done. | loss1: 0.4283 | loss_class: 0.4275 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|02:16:25] 	Iter 452500 Done. | loss1: 0.0295 | loss_class: 0.0286 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|02:17:17] 	Iter 452600 Done. | loss1: 0.2413 | loss_class: 0.2406 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|02:18:10] 	Iter 452700 Done. | loss1: 0.4499 | loss_class: 0.4492 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|02:19:01] 	Iter 452800 Done. | loss1: 0.1331 | loss_class: 0.1323 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|02:19:53] 	Iter 452900 Done. | loss1: 0.3884 | loss_class: 0.3877 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:20:44] 	Iter 453000 Done. | loss1: 0.5918 | loss_class: 0.5911 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:21:36] 	Iter 453100 Done. | loss1: 0.4153 | loss_class: 0.4147 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:22:28] 	Iter 453200 Done. | loss1: 1.2172 | loss_class: 1.2165 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:23:19] 	Iter 453300 Done. | loss1: 1.7170 | loss_class: 1.7165 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|02:24:11] 	Iter 453400 Done. | loss1: 1.1733 | loss_class: 1.1728 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|02:25:02] 	Iter 453500 Done. | loss1: 0.6481 | loss_class: 0.6474 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:25:53] 	Iter 453600 Done. | loss1: 0.9452 | loss_class: 0.9446 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:26:45] 	Iter 453700 Done. | loss1: 0.4646 | loss_class: 0.4638 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|02:27:37] 	Iter 453800 Done. | loss1: 0.2933 | loss_class: 0.2927 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|02:28:29] 	Iter 453900 Done. | loss1: 0.5851 | loss_class: 0.5843 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:29:20] 	Iter 454000 Done. | loss1: 0.0061 | loss_class: 0.0056 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|02:30:12] 	Iter 454100 Done. | loss1: 0.5511 | loss_class: 0.5504 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:31:04] 	Iter 454200 Done. | loss1: 0.2730 | loss_class: 0.2723 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:31:56] 	Iter 454300 Done. | loss1: 0.1203 | loss_class: 0.1198 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|02:32:47] 	Iter 454400 Done. | loss1: 0.6208 | loss_class: 0.6200 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|02:33:39] 	Iter 454500 Done. | loss1: 0.3076 | loss_class: 0.3070 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|02:34:31] 	Iter 454600 Done. | loss1: 0.7448 | loss_class: 0.7443 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|02:35:23] 	Iter 454700 Done. | loss1: 0.2982 | loss_class: 0.2974 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|02:36:15] 	Iter 454800 Done. | loss1: 0.5522 | loss_class: 0.5516 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|02:37:07] 	Iter 454900 Done. | loss1: 0.0331 | loss_class: 0.0327 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|02:37:59] 	Iter 455000 Done. | loss1: 0.3828 | loss_class: 0.3820 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:38:51] 	Iter 455100 Done. | loss1: 0.2631 | loss_class: 0.2625 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|02:39:43] 	Iter 455200 Done. | loss1: 1.0266 | loss_class: 1.0261 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|02:40:35] 	Iter 455300 Done. | loss1: 0.3808 | loss_class: 0.3802 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|02:41:26] 	Iter 455400 Done. | loss1: 0.5505 | loss_class: 0.5499 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:42:18] 	Iter 455500 Done. | loss1: 0.0339 | loss_class: 0.0332 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|02:43:10] 	Iter 455600 Done. | loss1: 1.2130 | loss_class: 1.2121 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|02:44:01] 	Iter 455700 Done. | loss1: 0.2829 | loss_class: 0.2822 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:44:53] 	Iter 455800 Done. | loss1: 0.0315 | loss_class: 0.0310 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|02:45:44] 	Iter 455900 Done. | loss1: 0.0452 | loss_class: 0.0446 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:46:37] 	Iter 456000 Done. | loss1: 0.3002 | loss_class: 0.2995 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:47:29] 	Iter 456100 Done. | loss1: 1.5868 | loss_class: 1.5861 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:48:20] 	Iter 456200 Done. | loss1: 0.1591 | loss_class: 0.1584 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|02:49:13] 	Iter 456300 Done. | loss1: 0.0800 | loss_class: 0.0792 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|02:50:04] 	Iter 456400 Done. | loss1: 1.5342 | loss_class: 1.5335 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:50:56] 	Iter 456500 Done. | loss1: 0.9097 | loss_class: 0.9089 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|02:51:48] 	Iter 456600 Done. | loss1: 0.0204 | loss_class: 0.0199 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|02:52:40] 	Iter 456700 Done. | loss1: 0.2299 | loss_class: 0.2292 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:53:32] 	Iter 456800 Done. | loss1: 0.4055 | loss_class: 0.4049 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|02:54:24] 	Iter 456900 Done. | loss1: 0.5548 | loss_class: 0.5542 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|02:55:15] 	Iter 457000 Done. | loss1: 1.6267 | loss_class: 1.6261 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|02:56:07] 	Iter 457100 Done. | loss1: 0.3531 | loss_class: 0.3526 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|02:56:59] 	Iter 457200 Done. | loss1: 0.0357 | loss_class: 0.0353 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|02:57:50] 	Iter 457300 Done. | loss1: 0.1793 | loss_class: 0.1786 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|02:58:42] 	Iter 457400 Done. | loss1: 0.0381 | loss_class: 0.0375 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|02:59:34] 	Iter 457500 Done. | loss1: 0.0253 | loss_class: 0.0248 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:00:26] 	Iter 457600 Done. | loss1: 0.0270 | loss_class: 0.0263 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:01:18] 	Iter 457700 Done. | loss1: 0.0682 | loss_class: 0.0677 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|03:02:09] 	Iter 457800 Done. | loss1: 0.5755 | loss_class: 0.5749 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:03:01] 	Iter 457900 Done. | loss1: 0.4767 | loss_class: 0.4760 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:03:53] 	Iter 458000 Done. | loss1: 0.5487 | loss_class: 0.5481 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:04:45] 	Iter 458100 Done. | loss1: 0.0281 | loss_class: 0.0275 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:05:36] 	Iter 458200 Done. | loss1: 0.4195 | loss_class: 0.4190 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|03:06:28] 	Iter 458300 Done. | loss1: 0.1027 | loss_class: 0.1020 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:07:20] 	Iter 458400 Done. | loss1: 0.5227 | loss_class: 0.5221 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|03:08:12] 	Iter 458500 Done. | loss1: 0.1934 | loss_class: 0.1928 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:09:04] 	Iter 458600 Done. | loss1: 0.0695 | loss_class: 0.0689 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:09:56] 	Iter 458700 Done. | loss1: 0.4179 | loss_class: 0.4172 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:10:47] 	Iter 458800 Done. | loss1: 0.3358 | loss_class: 0.3353 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|03:11:39] 	Iter 458900 Done. | loss1: 0.5419 | loss_class: 0.5414 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|03:12:31] 	Iter 459000 Done. | loss1: 0.3730 | loss_class: 0.3720 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|03:13:22] 	Iter 459100 Done. | loss1: 0.2280 | loss_class: 0.2274 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:14:14] 	Iter 459200 Done. | loss1: 0.5622 | loss_class: 0.5616 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:15:06] 	Iter 459300 Done. | loss1: 0.0631 | loss_class: 0.0624 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:15:58] 	Iter 459400 Done. | loss1: 0.2353 | loss_class: 0.2346 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:16:50] 	Iter 459500 Done. | loss1: 0.9558 | loss_class: 0.9552 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:17:42] 	Iter 459600 Done. | loss1: 0.1500 | loss_class: 0.1492 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|03:18:35] 	Iter 459700 Done. | loss1: 0.0114 | loss_class: 0.0105 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|03:19:27] 	Iter 459800 Done. | loss1: 0.4657 | loss_class: 0.4649 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|03:20:18] 	Iter 459900 Done. | loss1: 0.7830 | loss_class: 0.7826 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|03:21:10] 	Iter 460000 Done. | loss1: 0.2864 | loss_class: 0.2858 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:22:02] 	Iter 460100 Done. | loss1: 0.9847 | loss_class: 0.9839 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:22:53] 	Iter 460200 Done. | loss1: 0.1927 | loss_class: 0.1920 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:23:45] 	Iter 460300 Done. | loss1: 0.8724 | loss_class: 0.8719 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:24:37] 	Iter 460400 Done. | loss1: 0.0066 | loss_class: 0.0060 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:25:29] 	Iter 460500 Done. | loss1: 1.1153 | loss_class: 1.1148 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|03:26:21] 	Iter 460600 Done. | loss1: 0.0665 | loss_class: 0.0658 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:27:13] 	Iter 460700 Done. | loss1: 0.7206 | loss_class: 0.7199 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:28:05] 	Iter 460800 Done. | loss1: 0.5027 | loss_class: 0.5021 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:28:56] 	Iter 460900 Done. | loss1: 0.1505 | loss_class: 0.1500 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|03:29:48] 	Iter 461000 Done. | loss1: 0.4251 | loss_class: 0.4243 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|03:29:54] 	mean_loss1: 0.4625884705631956
[06.25.21|03:29:54] 	mean_loss_class: 0.46194216553067907
[06.25.21|03:29:54] 	mean_loss_recon: 0.0006463051419162454
[06.25.21|03:29:54] Time consumption:
[06.25.21|03:29:54] Done.
[06.25.21|03:29:55] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch55_model1.pt.
[06.25.21|03:29:55] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch55_model2.pt.
[06.25.21|03:29:55] Training epoch: 56
[06.25.21|03:30:41] 	Iter 461100 Done. | loss1: 0.7779 | loss_class: 0.7773 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:31:33] 	Iter 461200 Done. | loss1: 1.1033 | loss_class: 1.1029 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|03:32:25] 	Iter 461300 Done. | loss1: 0.1416 | loss_class: 0.1409 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|03:33:17] 	Iter 461400 Done. | loss1: 0.1495 | loss_class: 0.1489 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:34:09] 	Iter 461500 Done. | loss1: 0.0199 | loss_class: 0.0192 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:35:01] 	Iter 461600 Done. | loss1: 0.6882 | loss_class: 0.6875 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:35:53] 	Iter 461700 Done. | loss1: 1.2181 | loss_class: 1.2175 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:36:45] 	Iter 461800 Done. | loss1: 0.1005 | loss_class: 0.0999 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:37:37] 	Iter 461900 Done. | loss1: 0.8911 | loss_class: 0.8902 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|03:38:29] 	Iter 462000 Done. | loss1: 0.0466 | loss_class: 0.0460 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:39:21] 	Iter 462100 Done. | loss1: 0.7844 | loss_class: 0.7841 | loss_recon: 0.0003 | lr: 0.010000
[06.25.21|03:40:13] 	Iter 462200 Done. | loss1: 0.8263 | loss_class: 0.8255 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|03:41:05] 	Iter 462300 Done. | loss1: 0.5226 | loss_class: 0.5220 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|03:41:57] 	Iter 462400 Done. | loss1: 0.1286 | loss_class: 0.1280 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:42:49] 	Iter 462500 Done. | loss1: 0.4464 | loss_class: 0.4458 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:43:41] 	Iter 462600 Done. | loss1: 0.9241 | loss_class: 0.9235 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:44:33] 	Iter 462700 Done. | loss1: 0.4745 | loss_class: 0.4738 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:45:24] 	Iter 462800 Done. | loss1: 0.3114 | loss_class: 0.3110 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|03:46:16] 	Iter 462900 Done. | loss1: 1.1347 | loss_class: 1.1341 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:47:08] 	Iter 463000 Done. | loss1: 0.3083 | loss_class: 0.3075 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|03:48:00] 	Iter 463100 Done. | loss1: 0.0262 | loss_class: 0.0255 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:48:51] 	Iter 463200 Done. | loss1: 0.6819 | loss_class: 0.6815 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|03:49:43] 	Iter 463300 Done. | loss1: 0.7659 | loss_class: 0.7653 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:50:36] 	Iter 463400 Done. | loss1: 0.0364 | loss_class: 0.0359 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|03:51:27] 	Iter 463500 Done. | loss1: 0.0749 | loss_class: 0.0741 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|03:52:19] 	Iter 463600 Done. | loss1: 0.0624 | loss_class: 0.0616 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|03:53:10] 	Iter 463700 Done. | loss1: 0.6317 | loss_class: 0.6312 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|03:54:02] 	Iter 463800 Done. | loss1: 0.0045 | loss_class: 0.0037 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|03:54:54] 	Iter 463900 Done. | loss1: 0.9348 | loss_class: 0.9343 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|03:55:46] 	Iter 464000 Done. | loss1: 1.4904 | loss_class: 1.4898 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:56:38] 	Iter 464100 Done. | loss1: 0.8967 | loss_class: 0.8961 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|03:57:30] 	Iter 464200 Done. | loss1: 0.0115 | loss_class: 0.0108 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:58:21] 	Iter 464300 Done. | loss1: 0.9296 | loss_class: 0.9289 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|03:59:13] 	Iter 464400 Done. | loss1: 1.6019 | loss_class: 1.6014 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:00:04] 	Iter 464500 Done. | loss1: 1.7435 | loss_class: 1.7430 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|04:00:55] 	Iter 464600 Done. | loss1: 0.3480 | loss_class: 0.3475 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:01:47] 	Iter 464700 Done. | loss1: 0.0654 | loss_class: 0.0647 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:02:38] 	Iter 464800 Done. | loss1: 0.6193 | loss_class: 0.6188 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|04:03:29] 	Iter 464900 Done. | loss1: 0.3710 | loss_class: 0.3706 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|04:04:21] 	Iter 465000 Done. | loss1: 0.0587 | loss_class: 0.0579 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|04:05:13] 	Iter 465100 Done. | loss1: 1.3423 | loss_class: 1.3417 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:06:05] 	Iter 465200 Done. | loss1: 0.2414 | loss_class: 0.2408 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:06:57] 	Iter 465300 Done. | loss1: 0.1724 | loss_class: 0.1717 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:07:48] 	Iter 465400 Done. | loss1: 1.1388 | loss_class: 1.1381 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:08:40] 	Iter 465500 Done. | loss1: 0.1626 | loss_class: 0.1621 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:09:31] 	Iter 465600 Done. | loss1: 1.2773 | loss_class: 1.2767 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:10:23] 	Iter 465700 Done. | loss1: 0.1982 | loss_class: 0.1977 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:11:15] 	Iter 465800 Done. | loss1: 0.1418 | loss_class: 0.1413 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:12:06] 	Iter 465900 Done. | loss1: 0.3334 | loss_class: 0.3328 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:12:58] 	Iter 466000 Done. | loss1: 0.3802 | loss_class: 0.3797 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:13:49] 	Iter 466100 Done. | loss1: 0.3580 | loss_class: 0.3574 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:14:41] 	Iter 466200 Done. | loss1: 0.1213 | loss_class: 0.1205 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|04:15:34] 	Iter 466300 Done. | loss1: 0.3801 | loss_class: 0.3795 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:16:26] 	Iter 466400 Done. | loss1: 0.3612 | loss_class: 0.3607 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:17:17] 	Iter 466500 Done. | loss1: 0.6728 | loss_class: 0.6721 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:18:09] 	Iter 466600 Done. | loss1: 0.1100 | loss_class: 0.1095 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:19:01] 	Iter 466700 Done. | loss1: 0.0037 | loss_class: 0.0030 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:19:53] 	Iter 466800 Done. | loss1: 1.1335 | loss_class: 1.1329 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:20:44] 	Iter 466900 Done. | loss1: 0.0234 | loss_class: 0.0228 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:21:37] 	Iter 467000 Done. | loss1: 0.0269 | loss_class: 0.0264 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:22:29] 	Iter 467100 Done. | loss1: 0.0952 | loss_class: 0.0943 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|04:23:20] 	Iter 467200 Done. | loss1: 0.4342 | loss_class: 0.4337 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:24:13] 	Iter 467300 Done. | loss1: 0.1575 | loss_class: 0.1570 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:25:05] 	Iter 467400 Done. | loss1: 0.1263 | loss_class: 0.1257 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:25:57] 	Iter 467500 Done. | loss1: 0.1058 | loss_class: 0.1052 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:26:48] 	Iter 467600 Done. | loss1: 0.8282 | loss_class: 0.8275 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:27:40] 	Iter 467700 Done. | loss1: 0.4059 | loss_class: 0.4051 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|04:28:31] 	Iter 467800 Done. | loss1: 0.0220 | loss_class: 0.0215 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:29:23] 	Iter 467900 Done. | loss1: 0.3654 | loss_class: 0.3648 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:30:15] 	Iter 468000 Done. | loss1: 0.0402 | loss_class: 0.0395 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:31:07] 	Iter 468100 Done. | loss1: 1.9565 | loss_class: 1.9557 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:31:58] 	Iter 468200 Done. | loss1: 0.0448 | loss_class: 0.0442 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:32:50] 	Iter 468300 Done. | loss1: 0.2946 | loss_class: 0.2941 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:33:42] 	Iter 468400 Done. | loss1: 1.8523 | loss_class: 1.8517 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:34:33] 	Iter 468500 Done. | loss1: 0.1662 | loss_class: 0.1654 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:35:25] 	Iter 468600 Done. | loss1: 0.2226 | loss_class: 0.2220 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:36:17] 	Iter 468700 Done. | loss1: 0.3695 | loss_class: 0.3691 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:37:09] 	Iter 468800 Done. | loss1: 0.7032 | loss_class: 0.7026 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:38:01] 	Iter 468900 Done. | loss1: 0.2241 | loss_class: 0.2235 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:38:52] 	Iter 469000 Done. | loss1: 0.1555 | loss_class: 0.1550 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:39:44] 	Iter 469100 Done. | loss1: 0.0766 | loss_class: 0.0759 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|04:40:36] 	Iter 469200 Done. | loss1: 0.2142 | loss_class: 0.2137 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:41:27] 	Iter 469300 Done. | loss1: 0.1078 | loss_class: 0.1073 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:42:18] 	Iter 469400 Done. | loss1: 0.3595 | loss_class: 0.3588 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:43:10] 	Iter 469500 Done. | loss1: 0.0320 | loss_class: 0.0314 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:44:02] 	Iter 469600 Done. | loss1: 1.2545 | loss_class: 1.2539 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:44:54] 	Iter 469700 Done. | loss1: 0.2240 | loss_class: 0.2236 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:45:46] 	Iter 469800 Done. | loss1: 0.7131 | loss_class: 0.7125 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:46:37] 	Iter 469900 Done. | loss1: 0.0410 | loss_class: 0.0403 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:47:29] 	Iter 470000 Done. | loss1: 0.0854 | loss_class: 0.0847 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|04:48:21] 	Iter 470100 Done. | loss1: 0.8967 | loss_class: 0.8961 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:49:12] 	Iter 470200 Done. | loss1: 0.0577 | loss_class: 0.0571 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:50:04] 	Iter 470300 Done. | loss1: 0.2024 | loss_class: 0.2018 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:50:56] 	Iter 470400 Done. | loss1: 0.2494 | loss_class: 0.2489 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:51:47] 	Iter 470500 Done. | loss1: 0.0147 | loss_class: 0.0140 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|04:52:39] 	Iter 470600 Done. | loss1: 0.5728 | loss_class: 0.5723 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:53:31] 	Iter 470700 Done. | loss1: 0.6714 | loss_class: 0.6710 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|04:54:23] 	Iter 470800 Done. | loss1: 1.1055 | loss_class: 1.1049 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:55:15] 	Iter 470900 Done. | loss1: 0.5846 | loss_class: 0.5840 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:56:07] 	Iter 471000 Done. | loss1: 0.8599 | loss_class: 0.8593 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:56:24] 	mean_loss1: 0.4661031803334815
[06.25.21|04:56:24] 	mean_loss_class: 0.4654570171769345
[06.25.21|04:56:24] 	mean_loss_recon: 0.0006461632297274134
[06.25.21|04:56:24] Time consumption:
[06.25.21|04:56:24] Done.
[06.25.21|04:56:24] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch56_model1.pt.
[06.25.21|04:56:24] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch56_model2.pt.
[06.25.21|04:56:24] Training epoch: 57
[06.25.21|04:56:59] 	Iter 471100 Done. | loss1: 0.8817 | loss_class: 0.8810 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|04:57:51] 	Iter 471200 Done. | loss1: 0.8588 | loss_class: 0.8582 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|04:58:42] 	Iter 471300 Done. | loss1: 0.0222 | loss_class: 0.0214 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|04:59:34] 	Iter 471400 Done. | loss1: 0.7264 | loss_class: 0.7258 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:00:26] 	Iter 471500 Done. | loss1: 0.0812 | loss_class: 0.0805 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:01:17] 	Iter 471600 Done. | loss1: 0.3136 | loss_class: 0.3128 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|05:02:09] 	Iter 471700 Done. | loss1: 0.2428 | loss_class: 0.2420 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:03:01] 	Iter 471800 Done. | loss1: 1.0876 | loss_class: 1.0870 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:03:52] 	Iter 471900 Done. | loss1: 0.0364 | loss_class: 0.0360 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|05:04:44] 	Iter 472000 Done. | loss1: 0.3310 | loss_class: 0.3301 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|05:05:36] 	Iter 472100 Done. | loss1: 0.0421 | loss_class: 0.0412 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|05:06:27] 	Iter 472200 Done. | loss1: 0.9250 | loss_class: 0.9244 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|05:07:19] 	Iter 472300 Done. | loss1: 0.9367 | loss_class: 0.9360 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:08:10] 	Iter 472400 Done. | loss1: 0.6980 | loss_class: 0.6973 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:09:01] 	Iter 472500 Done. | loss1: 0.7729 | loss_class: 0.7723 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:09:53] 	Iter 472600 Done. | loss1: 0.0648 | loss_class: 0.0639 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|05:10:45] 	Iter 472700 Done. | loss1: 0.1826 | loss_class: 0.1818 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:11:37] 	Iter 472800 Done. | loss1: 0.1542 | loss_class: 0.1535 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:12:28] 	Iter 472900 Done. | loss1: 0.0140 | loss_class: 0.0133 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:13:19] 	Iter 473000 Done. | loss1: 0.9857 | loss_class: 0.9850 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:14:11] 	Iter 473100 Done. | loss1: 0.0108 | loss_class: 0.0101 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:15:03] 	Iter 473200 Done. | loss1: 0.2566 | loss_class: 0.2559 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:15:55] 	Iter 473300 Done. | loss1: 0.5283 | loss_class: 0.5277 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:16:47] 	Iter 473400 Done. | loss1: 0.9697 | loss_class: 0.9690 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:17:39] 	Iter 473500 Done. | loss1: 0.0153 | loss_class: 0.0147 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:18:30] 	Iter 473600 Done. | loss1: 0.1071 | loss_class: 0.1066 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|05:19:21] 	Iter 473700 Done. | loss1: 0.0678 | loss_class: 0.0670 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|05:20:13] 	Iter 473800 Done. | loss1: 0.1356 | loss_class: 0.1351 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|05:21:05] 	Iter 473900 Done. | loss1: 0.0453 | loss_class: 0.0445 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|05:21:57] 	Iter 474000 Done. | loss1: 0.2919 | loss_class: 0.2913 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:22:49] 	Iter 474100 Done. | loss1: 0.0184 | loss_class: 0.0179 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|05:23:40] 	Iter 474200 Done. | loss1: 0.9675 | loss_class: 0.9669 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:24:32] 	Iter 474300 Done. | loss1: 0.1493 | loss_class: 0.1488 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|05:25:24] 	Iter 474400 Done. | loss1: 0.3013 | loss_class: 0.3006 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:26:16] 	Iter 474500 Done. | loss1: 0.1330 | loss_class: 0.1322 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|05:27:07] 	Iter 474600 Done. | loss1: 0.2404 | loss_class: 0.2396 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|05:27:58] 	Iter 474700 Done. | loss1: 1.1788 | loss_class: 1.1781 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:28:50] 	Iter 474800 Done. | loss1: 0.9504 | loss_class: 0.9497 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:29:42] 	Iter 474900 Done. | loss1: 0.1355 | loss_class: 0.1347 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|05:30:34] 	Iter 475000 Done. | loss1: 0.2663 | loss_class: 0.2655 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:31:25] 	Iter 475100 Done. | loss1: 0.1273 | loss_class: 0.1265 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|05:32:16] 	Iter 475200 Done. | loss1: 0.0553 | loss_class: 0.0546 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:33:08] 	Iter 475300 Done. | loss1: 0.3715 | loss_class: 0.3710 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|05:34:00] 	Iter 475400 Done. | loss1: 0.1181 | loss_class: 0.1171 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|05:34:52] 	Iter 475500 Done. | loss1: 1.0252 | loss_class: 1.0245 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:35:43] 	Iter 475600 Done. | loss1: 0.0317 | loss_class: 0.0310 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:36:35] 	Iter 475700 Done. | loss1: 1.4675 | loss_class: 1.4667 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|05:37:27] 	Iter 475800 Done. | loss1: 0.4067 | loss_class: 0.4061 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:38:18] 	Iter 475900 Done. | loss1: 1.0346 | loss_class: 1.0339 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:39:10] 	Iter 476000 Done. | loss1: 0.8492 | loss_class: 0.8487 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:40:02] 	Iter 476100 Done. | loss1: 1.0162 | loss_class: 1.0155 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:40:53] 	Iter 476200 Done. | loss1: 1.8614 | loss_class: 1.8608 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:41:45] 	Iter 476300 Done. | loss1: 0.9749 | loss_class: 0.9743 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:42:37] 	Iter 476400 Done. | loss1: 0.3478 | loss_class: 0.3471 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:43:29] 	Iter 476500 Done. | loss1: 1.3426 | loss_class: 1.3421 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|05:44:21] 	Iter 476600 Done. | loss1: 0.0775 | loss_class: 0.0769 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:45:13] 	Iter 476700 Done. | loss1: 0.6074 | loss_class: 0.6068 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:46:05] 	Iter 476800 Done. | loss1: 0.4007 | loss_class: 0.4002 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|05:46:56] 	Iter 476900 Done. | loss1: 0.0492 | loss_class: 0.0487 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:47:49] 	Iter 477000 Done. | loss1: 0.2327 | loss_class: 0.2320 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:48:40] 	Iter 477100 Done. | loss1: 0.2209 | loss_class: 0.2203 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:49:33] 	Iter 477200 Done. | loss1: 0.5502 | loss_class: 0.5494 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|05:50:24] 	Iter 477300 Done. | loss1: 0.2777 | loss_class: 0.2771 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:51:16] 	Iter 477400 Done. | loss1: 0.0239 | loss_class: 0.0235 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|05:52:07] 	Iter 477500 Done. | loss1: 0.1051 | loss_class: 0.1043 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|05:52:59] 	Iter 477600 Done. | loss1: 0.0051 | loss_class: 0.0044 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:53:51] 	Iter 477700 Done. | loss1: 0.7639 | loss_class: 0.7631 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|05:54:42] 	Iter 477800 Done. | loss1: 0.7228 | loss_class: 0.7223 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|05:55:34] 	Iter 477900 Done. | loss1: 0.4076 | loss_class: 0.4069 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|05:56:26] 	Iter 478000 Done. | loss1: 0.0125 | loss_class: 0.0119 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:57:18] 	Iter 478100 Done. | loss1: 0.0527 | loss_class: 0.0521 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|05:58:10] 	Iter 478200 Done. | loss1: 0.5355 | loss_class: 0.5350 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|05:59:02] 	Iter 478300 Done. | loss1: 0.4941 | loss_class: 0.4936 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|05:59:53] 	Iter 478400 Done. | loss1: 0.3816 | loss_class: 0.3811 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|06:00:45] 	Iter 478500 Done. | loss1: 0.0198 | loss_class: 0.0192 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:01:37] 	Iter 478600 Done. | loss1: 0.6215 | loss_class: 0.6209 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:02:29] 	Iter 478700 Done. | loss1: 0.0296 | loss_class: 0.0291 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|06:03:21] 	Iter 478800 Done. | loss1: 0.6794 | loss_class: 0.6788 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:04:13] 	Iter 478900 Done. | loss1: 0.6159 | loss_class: 0.6153 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:05:04] 	Iter 479000 Done. | loss1: 0.8203 | loss_class: 0.8197 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:05:56] 	Iter 479100 Done. | loss1: 0.0971 | loss_class: 0.0965 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:06:49] 	Iter 479200 Done. | loss1: 0.4177 | loss_class: 0.4170 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|06:07:42] 	Iter 479300 Done. | loss1: 0.6403 | loss_class: 0.6397 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|06:08:34] 	Iter 479400 Done. | loss1: 0.9254 | loss_class: 0.9249 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|06:09:25] 	Iter 479500 Done. | loss1: 0.1543 | loss_class: 0.1538 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:10:16] 	Iter 479600 Done. | loss1: 0.3426 | loss_class: 0.3420 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:11:08] 	Iter 479700 Done. | loss1: 0.0319 | loss_class: 0.0313 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:12:01] 	Iter 479800 Done. | loss1: 0.0384 | loss_class: 0.0376 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|06:12:52] 	Iter 479900 Done. | loss1: 0.1320 | loss_class: 0.1316 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|06:13:43] 	Iter 480000 Done. | loss1: 0.0145 | loss_class: 0.0139 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:14:36] 	Iter 480100 Done. | loss1: 0.0423 | loss_class: 0.0417 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:15:29] 	Iter 480200 Done. | loss1: 1.0035 | loss_class: 1.0029 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:16:21] 	Iter 480300 Done. | loss1: 0.4721 | loss_class: 0.4715 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:17:13] 	Iter 480400 Done. | loss1: 0.9235 | loss_class: 0.9229 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:18:04] 	Iter 480500 Done. | loss1: 0.0043 | loss_class: 0.0037 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|06:18:56] 	Iter 480600 Done. | loss1: 0.0849 | loss_class: 0.0844 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|06:19:48] 	Iter 480700 Done. | loss1: 0.0775 | loss_class: 0.0769 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:20:40] 	Iter 480800 Done. | loss1: 0.1980 | loss_class: 0.1974 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|06:21:32] 	Iter 480900 Done. | loss1: 0.0689 | loss_class: 0.0683 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:22:23] 	Iter 481000 Done. | loss1: 0.2640 | loss_class: 0.2632 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|06:22:52] 	mean_loss1: 0.4648236941437453
[06.25.21|06:22:52] 	mean_loss_class: 0.464177330282267
[06.25.21|06:22:52] 	mean_loss_recon: 0.0006463636751664902
[06.25.21|06:22:52] Time consumption:
[06.25.21|06:22:52] Done.
[06.25.21|06:22:53] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch57_model1.pt.
[06.25.21|06:22:53] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch57_model2.pt.
[06.25.21|06:22:53] Training epoch: 58
[06.25.21|06:23:16] 	Iter 481100 Done. | loss1: 1.7055 | loss_class: 1.7050 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|06:24:05] 	Iter 481200 Done. | loss1: 0.1389 | loss_class: 0.1381 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|06:24:56] 	Iter 481300 Done. | loss1: 0.1499 | loss_class: 0.1492 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|06:25:47] 	Iter 481400 Done. | loss1: 0.7065 | loss_class: 0.7060 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:26:41] 	Iter 481500 Done. | loss1: 1.1320 | loss_class: 1.1313 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:27:32] 	Iter 481600 Done. | loss1: 1.1387 | loss_class: 1.1379 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|06:28:24] 	Iter 481700 Done. | loss1: 0.0661 | loss_class: 0.0652 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|06:29:15] 	Iter 481800 Done. | loss1: 1.4655 | loss_class: 1.4647 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|06:30:06] 	Iter 481900 Done. | loss1: 0.0775 | loss_class: 0.0768 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|06:30:59] 	Iter 482000 Done. | loss1: 0.2759 | loss_class: 0.2753 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:31:52] 	Iter 482100 Done. | loss1: 0.0403 | loss_class: 0.0396 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|06:32:42] 	Iter 482200 Done. | loss1: 0.0319 | loss_class: 0.0313 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:33:32] 	Iter 482300 Done. | loss1: 0.0518 | loss_class: 0.0512 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:34:22] 	Iter 482400 Done. | loss1: 0.0117 | loss_class: 0.0105 | loss_recon: 0.0012 | lr: 0.010000
[06.25.21|06:35:12] 	Iter 482500 Done. | loss1: 0.7963 | loss_class: 0.7958 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:36:02] 	Iter 482600 Done. | loss1: 0.3804 | loss_class: 0.3794 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|06:36:52] 	Iter 482700 Done. | loss1: 0.8328 | loss_class: 0.8321 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|06:37:44] 	Iter 482800 Done. | loss1: 0.8301 | loss_class: 0.8295 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:38:35] 	Iter 482900 Done. | loss1: 0.7071 | loss_class: 0.7065 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:39:26] 	Iter 483000 Done. | loss1: 0.0534 | loss_class: 0.0528 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:40:16] 	Iter 483100 Done. | loss1: 0.0580 | loss_class: 0.0573 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|06:41:07] 	Iter 483200 Done. | loss1: 0.8357 | loss_class: 0.8351 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:41:57] 	Iter 483300 Done. | loss1: 1.2306 | loss_class: 1.2300 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|06:42:48] 	Iter 483400 Done. | loss1: 0.4243 | loss_class: 0.4237 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:43:39] 	Iter 483500 Done. | loss1: 0.3018 | loss_class: 0.3011 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|06:44:30] 	Iter 483600 Done. | loss1: 0.8236 | loss_class: 0.8231 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:45:20] 	Iter 483700 Done. | loss1: 0.6107 | loss_class: 0.6100 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:46:11] 	Iter 483800 Done. | loss1: 0.2947 | loss_class: 0.2941 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:47:02] 	Iter 483900 Done. | loss1: 0.5730 | loss_class: 0.5725 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|06:47:52] 	Iter 484000 Done. | loss1: 0.4131 | loss_class: 0.4124 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|06:48:42] 	Iter 484100 Done. | loss1: 0.0097 | loss_class: 0.0092 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|06:49:32] 	Iter 484200 Done. | loss1: 0.1887 | loss_class: 0.1879 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|06:50:22] 	Iter 484300 Done. | loss1: 0.7506 | loss_class: 0.7499 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|06:51:13] 	Iter 484400 Done. | loss1: 0.0403 | loss_class: 0.0398 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|06:52:03] 	Iter 484500 Done. | loss1: 0.4225 | loss_class: 0.4217 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|06:52:54] 	Iter 484600 Done. | loss1: 0.5488 | loss_class: 0.5482 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|06:53:44] 	Iter 484700 Done. | loss1: 0.2524 | loss_class: 0.2516 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|06:54:36] 	Iter 484800 Done. | loss1: 0.1864 | loss_class: 0.1858 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:55:26] 	Iter 484900 Done. | loss1: 0.5172 | loss_class: 0.5166 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|06:56:16] 	Iter 485000 Done. | loss1: 0.6441 | loss_class: 0.6433 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|06:57:07] 	Iter 485100 Done. | loss1: 0.4703 | loss_class: 0.4698 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|06:57:57] 	Iter 485200 Done. | loss1: 0.0097 | loss_class: 0.0089 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|06:58:47] 	Iter 485300 Done. | loss1: 0.0035 | loss_class: 0.0031 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|06:59:37] 	Iter 485400 Done. | loss1: 0.0775 | loss_class: 0.0769 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|07:00:27] 	Iter 485500 Done. | loss1: 0.2336 | loss_class: 0.2327 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|07:01:18] 	Iter 485600 Done. | loss1: 0.7251 | loss_class: 0.7244 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:02:08] 	Iter 485700 Done. | loss1: 0.1567 | loss_class: 0.1560 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:02:58] 	Iter 485800 Done. | loss1: 0.0026 | loss_class: 0.0019 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:03:49] 	Iter 485900 Done. | loss1: 0.5827 | loss_class: 0.5821 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:04:40] 	Iter 486000 Done. | loss1: 0.6347 | loss_class: 0.6342 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|07:05:31] 	Iter 486100 Done. | loss1: 0.3275 | loss_class: 0.3268 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:06:21] 	Iter 486200 Done. | loss1: 0.6218 | loss_class: 0.6211 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:07:11] 	Iter 486300 Done. | loss1: 0.0198 | loss_class: 0.0193 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:08:01] 	Iter 486400 Done. | loss1: 0.4386 | loss_class: 0.4382 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|07:08:52] 	Iter 486500 Done. | loss1: 0.3596 | loss_class: 0.3589 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:09:42] 	Iter 486600 Done. | loss1: 0.1471 | loss_class: 0.1465 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:10:34] 	Iter 486700 Done. | loss1: 0.6607 | loss_class: 0.6599 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|07:11:27] 	Iter 486800 Done. | loss1: 0.8515 | loss_class: 0.8508 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:12:17] 	Iter 486900 Done. | loss1: 0.3270 | loss_class: 0.3263 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:13:08] 	Iter 487000 Done. | loss1: 0.2294 | loss_class: 0.2288 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:13:58] 	Iter 487100 Done. | loss1: 0.7055 | loss_class: 0.7048 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:14:49] 	Iter 487200 Done. | loss1: 0.0462 | loss_class: 0.0456 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:15:40] 	Iter 487300 Done. | loss1: 0.3656 | loss_class: 0.3650 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:16:30] 	Iter 487400 Done. | loss1: 0.3405 | loss_class: 0.3400 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|07:17:21] 	Iter 487500 Done. | loss1: 1.3308 | loss_class: 1.3300 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|07:18:11] 	Iter 487600 Done. | loss1: 0.1942 | loss_class: 0.1937 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|07:19:01] 	Iter 487700 Done. | loss1: 0.7101 | loss_class: 0.7095 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:19:52] 	Iter 487800 Done. | loss1: 0.0612 | loss_class: 0.0603 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|07:20:43] 	Iter 487900 Done. | loss1: 0.0820 | loss_class: 0.0814 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|07:21:33] 	Iter 488000 Done. | loss1: 0.5895 | loss_class: 0.5887 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|07:22:24] 	Iter 488100 Done. | loss1: 0.4233 | loss_class: 0.4224 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|07:23:14] 	Iter 488200 Done. | loss1: 0.1015 | loss_class: 0.1010 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:24:05] 	Iter 488300 Done. | loss1: 0.6705 | loss_class: 0.6697 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|07:24:56] 	Iter 488400 Done. | loss1: 0.3423 | loss_class: 0.3416 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:25:47] 	Iter 488500 Done. | loss1: 0.0156 | loss_class: 0.0149 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:26:37] 	Iter 488600 Done. | loss1: 1.7254 | loss_class: 1.7248 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:27:28] 	Iter 488700 Done. | loss1: 0.0739 | loss_class: 0.0732 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:28:18] 	Iter 488800 Done. | loss1: 1.4556 | loss_class: 1.4551 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|07:29:10] 	Iter 488900 Done. | loss1: 0.2189 | loss_class: 0.2182 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:30:02] 	Iter 489000 Done. | loss1: 0.1437 | loss_class: 0.1431 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:30:52] 	Iter 489100 Done. | loss1: 0.1988 | loss_class: 0.1981 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:31:42] 	Iter 489200 Done. | loss1: 0.7134 | loss_class: 0.7128 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:32:33] 	Iter 489300 Done. | loss1: 0.6974 | loss_class: 0.6969 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:33:23] 	Iter 489400 Done. | loss1: 0.4361 | loss_class: 0.4355 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:34:13] 	Iter 489500 Done. | loss1: 0.3638 | loss_class: 0.3633 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:35:03] 	Iter 489600 Done. | loss1: 1.8795 | loss_class: 1.8787 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|07:35:54] 	Iter 489700 Done. | loss1: 0.3554 | loss_class: 0.3548 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:36:46] 	Iter 489800 Done. | loss1: 0.3172 | loss_class: 0.3165 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:37:38] 	Iter 489900 Done. | loss1: 1.3135 | loss_class: 1.3127 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:38:28] 	Iter 490000 Done. | loss1: 1.0700 | loss_class: 1.0696 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|07:39:18] 	Iter 490100 Done. | loss1: 1.5811 | loss_class: 1.5805 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:40:08] 	Iter 490200 Done. | loss1: 0.0496 | loss_class: 0.0491 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|07:40:58] 	Iter 490300 Done. | loss1: 0.1366 | loss_class: 0.1360 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:41:48] 	Iter 490400 Done. | loss1: 0.7344 | loss_class: 0.7338 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|07:42:38] 	Iter 490500 Done. | loss1: 1.8963 | loss_class: 1.8956 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:43:29] 	Iter 490600 Done. | loss1: 0.2264 | loss_class: 0.2258 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:44:19] 	Iter 490700 Done. | loss1: 0.5245 | loss_class: 0.5237 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|07:45:10] 	Iter 490800 Done. | loss1: 0.8480 | loss_class: 0.8474 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:46:01] 	Iter 490900 Done. | loss1: 0.2856 | loss_class: 0.2848 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:46:51] 	Iter 491000 Done. | loss1: 0.0408 | loss_class: 0.0400 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|07:47:30] 	mean_loss1: 0.4641083828182406
[06.25.21|07:47:30] 	mean_loss_class: 0.4634622401162966
[06.25.21|07:47:30] 	mean_loss_recon: 0.0006461428377403396
[06.25.21|07:47:30] Time consumption:
[06.25.21|07:47:30] Done.
[06.25.21|07:47:30] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch58_model1.pt.
[06.25.21|07:47:30] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch58_model2.pt.
[06.25.21|07:47:30] Training epoch: 59
[06.25.21|07:47:42] 	Iter 491100 Done. | loss1: 1.0725 | loss_class: 1.0721 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|07:48:33] 	Iter 491200 Done. | loss1: 0.7847 | loss_class: 0.7839 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|07:49:25] 	Iter 491300 Done. | loss1: 0.0538 | loss_class: 0.0533 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|07:50:18] 	Iter 491400 Done. | loss1: 0.1814 | loss_class: 0.1807 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:51:08] 	Iter 491500 Done. | loss1: 0.5150 | loss_class: 0.5144 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:51:59] 	Iter 491600 Done. | loss1: 1.1816 | loss_class: 1.1809 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|07:52:51] 	Iter 491700 Done. | loss1: 1.5372 | loss_class: 1.5365 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:53:42] 	Iter 491800 Done. | loss1: 0.4540 | loss_class: 0.4533 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:54:32] 	Iter 491900 Done. | loss1: 0.0160 | loss_class: 0.0153 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:55:24] 	Iter 492000 Done. | loss1: 1.0401 | loss_class: 1.0396 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|07:56:14] 	Iter 492100 Done. | loss1: 0.1700 | loss_class: 0.1695 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|07:57:05] 	Iter 492200 Done. | loss1: 0.1115 | loss_class: 0.1108 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:57:55] 	Iter 492300 Done. | loss1: 0.1420 | loss_class: 0.1412 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:58:46] 	Iter 492400 Done. | loss1: 0.0021 | loss_class: 0.0014 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|07:59:36] 	Iter 492500 Done. | loss1: 0.8560 | loss_class: 0.8552 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|08:00:26] 	Iter 492600 Done. | loss1: 0.0608 | loss_class: 0.0601 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:01:17] 	Iter 492700 Done. | loss1: 0.2562 | loss_class: 0.2555 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:02:07] 	Iter 492800 Done. | loss1: 0.3760 | loss_class: 0.3754 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:02:58] 	Iter 492900 Done. | loss1: 0.1014 | loss_class: 0.1007 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|08:03:48] 	Iter 493000 Done. | loss1: 0.4892 | loss_class: 0.4886 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:04:38] 	Iter 493100 Done. | loss1: 0.2200 | loss_class: 0.2193 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:05:28] 	Iter 493200 Done. | loss1: 0.5089 | loss_class: 0.5083 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:06:18] 	Iter 493300 Done. | loss1: 0.1136 | loss_class: 0.1129 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:07:08] 	Iter 493400 Done. | loss1: 0.7156 | loss_class: 0.7150 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:07:59] 	Iter 493500 Done. | loss1: 0.5580 | loss_class: 0.5572 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|08:08:49] 	Iter 493600 Done. | loss1: 0.0070 | loss_class: 0.0065 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|08:09:40] 	Iter 493700 Done. | loss1: 0.3245 | loss_class: 0.3239 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:10:31] 	Iter 493800 Done. | loss1: 0.6729 | loss_class: 0.6721 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|08:11:23] 	Iter 493900 Done. | loss1: 0.0728 | loss_class: 0.0722 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:12:15] 	Iter 494000 Done. | loss1: 0.0134 | loss_class: 0.0127 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:13:05] 	Iter 494100 Done. | loss1: 0.2196 | loss_class: 0.2188 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:13:55] 	Iter 494200 Done. | loss1: 0.1709 | loss_class: 0.1702 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:14:46] 	Iter 494300 Done. | loss1: 1.2081 | loss_class: 1.2074 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:15:36] 	Iter 494400 Done. | loss1: 1.0212 | loss_class: 1.0204 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|08:16:27] 	Iter 494500 Done. | loss1: 0.9427 | loss_class: 0.9422 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:17:21] 	Iter 494600 Done. | loss1: 1.0758 | loss_class: 1.0752 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:18:17] 	Iter 494700 Done. | loss1: 0.2200 | loss_class: 0.2194 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|08:19:12] 	Iter 494800 Done. | loss1: 0.3169 | loss_class: 0.3161 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|08:20:03] 	Iter 494900 Done. | loss1: 0.1812 | loss_class: 0.1805 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|08:20:53] 	Iter 495000 Done. | loss1: 0.3245 | loss_class: 0.3237 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|08:21:43] 	Iter 495100 Done. | loss1: 0.4045 | loss_class: 0.4039 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:22:33] 	Iter 495200 Done. | loss1: 0.0162 | loss_class: 0.0156 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:23:23] 	Iter 495300 Done. | loss1: 0.7182 | loss_class: 0.7174 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|08:24:13] 	Iter 495400 Done. | loss1: 0.0221 | loss_class: 0.0215 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:25:04] 	Iter 495500 Done. | loss1: 0.4942 | loss_class: 0.4936 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:25:54] 	Iter 495600 Done. | loss1: 0.3804 | loss_class: 0.3797 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:26:44] 	Iter 495700 Done. | loss1: 0.1574 | loss_class: 0.1567 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:27:35] 	Iter 495800 Done. | loss1: 0.3768 | loss_class: 0.3764 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|08:28:25] 	Iter 495900 Done. | loss1: 0.5611 | loss_class: 0.5605 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:29:16] 	Iter 496000 Done. | loss1: 0.0424 | loss_class: 0.0418 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:30:07] 	Iter 496100 Done. | loss1: 0.0350 | loss_class: 0.0343 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:30:59] 	Iter 496200 Done. | loss1: 0.5252 | loss_class: 0.5246 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:31:50] 	Iter 496300 Done. | loss1: 0.0075 | loss_class: 0.0066 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|08:32:40] 	Iter 496400 Done. | loss1: 1.3870 | loss_class: 1.3863 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:33:31] 	Iter 496500 Done. | loss1: 0.1734 | loss_class: 0.1728 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:34:21] 	Iter 496600 Done. | loss1: 0.4736 | loss_class: 0.4729 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:35:11] 	Iter 496700 Done. | loss1: 0.1550 | loss_class: 0.1546 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|08:36:01] 	Iter 496800 Done. | loss1: 0.4144 | loss_class: 0.4137 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:36:51] 	Iter 496900 Done. | loss1: 1.5075 | loss_class: 1.5069 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:37:41] 	Iter 497000 Done. | loss1: 0.4595 | loss_class: 0.4588 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:38:31] 	Iter 497100 Done. | loss1: 0.1510 | loss_class: 0.1503 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:39:21] 	Iter 497200 Done. | loss1: 0.1601 | loss_class: 0.1595 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:40:15] 	Iter 497300 Done. | loss1: 0.1665 | loss_class: 0.1660 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|08:41:05] 	Iter 497400 Done. | loss1: 0.0815 | loss_class: 0.0809 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:41:56] 	Iter 497500 Done. | loss1: 0.6679 | loss_class: 0.6672 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:42:47] 	Iter 497600 Done. | loss1: 0.7606 | loss_class: 0.7601 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|08:43:40] 	Iter 497700 Done. | loss1: 0.0936 | loss_class: 0.0929 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|08:44:31] 	Iter 497800 Done. | loss1: 0.0581 | loss_class: 0.0575 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:45:21] 	Iter 497900 Done. | loss1: 0.5086 | loss_class: 0.5078 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|08:46:11] 	Iter 498000 Done. | loss1: 0.3278 | loss_class: 0.3273 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|08:47:02] 	Iter 498100 Done. | loss1: 0.4307 | loss_class: 0.4303 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|08:47:53] 	Iter 498200 Done. | loss1: 0.1951 | loss_class: 0.1944 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:48:43] 	Iter 498300 Done. | loss1: 0.0553 | loss_class: 0.0547 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:49:34] 	Iter 498400 Done. | loss1: 0.6630 | loss_class: 0.6625 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:50:24] 	Iter 498500 Done. | loss1: 0.0502 | loss_class: 0.0498 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|08:51:14] 	Iter 498600 Done. | loss1: 1.1277 | loss_class: 1.1272 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|08:52:04] 	Iter 498700 Done. | loss1: 1.1079 | loss_class: 1.1073 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:52:55] 	Iter 498800 Done. | loss1: 0.0408 | loss_class: 0.0401 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:53:45] 	Iter 498900 Done. | loss1: 0.1458 | loss_class: 0.1451 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:54:34] 	Iter 499000 Done. | loss1: 0.0697 | loss_class: 0.0690 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|08:55:25] 	Iter 499100 Done. | loss1: 1.0563 | loss_class: 1.0558 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|08:56:16] 	Iter 499200 Done. | loss1: 0.6535 | loss_class: 0.6529 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:57:07] 	Iter 499300 Done. | loss1: 0.1218 | loss_class: 0.1212 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:57:58] 	Iter 499400 Done. | loss1: 0.2696 | loss_class: 0.2691 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|08:58:48] 	Iter 499500 Done. | loss1: 0.7938 | loss_class: 0.7932 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|08:59:39] 	Iter 499600 Done. | loss1: 0.3587 | loss_class: 0.3579 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|09:00:30] 	Iter 499700 Done. | loss1: 0.2459 | loss_class: 0.2454 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|09:01:20] 	Iter 499800 Done. | loss1: 0.7512 | loss_class: 0.7504 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|09:02:11] 	Iter 499900 Done. | loss1: 0.1147 | loss_class: 0.1141 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:03:01] 	Iter 500000 Done. | loss1: 0.0273 | loss_class: 0.0266 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:03:51] 	Iter 500100 Done. | loss1: 0.2757 | loss_class: 0.2751 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|09:04:41] 	Iter 500200 Done. | loss1: 1.0442 | loss_class: 1.0437 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|09:05:31] 	Iter 500300 Done. | loss1: 0.1646 | loss_class: 0.1639 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:06:22] 	Iter 500400 Done. | loss1: 0.4282 | loss_class: 0.4276 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:07:12] 	Iter 500500 Done. | loss1: 0.7334 | loss_class: 0.7328 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:08:02] 	Iter 500600 Done. | loss1: 0.1779 | loss_class: 0.1774 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:08:52] 	Iter 500700 Done. | loss1: 0.0298 | loss_class: 0.0292 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:09:42] 	Iter 500800 Done. | loss1: 0.4417 | loss_class: 0.4411 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:10:34] 	Iter 500900 Done. | loss1: 0.0327 | loss_class: 0.0321 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:11:26] 	Iter 501000 Done. | loss1: 0.0188 | loss_class: 0.0182 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:12:16] 	mean_loss1: 0.46060864833747006
[06.25.21|09:12:16] 	mean_loss_class: 0.4599625054043754
[06.25.21|09:12:16] 	mean_loss_recon: 0.0006461429796559836
[06.25.21|09:12:16] Time consumption:
[06.25.21|09:12:16] Done.
[06.25.21|09:12:16] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch59_model1.pt.
[06.25.21|09:12:16] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch59_model2.pt.
[06.25.21|09:12:16] Eval epoch: 59
[06.25.21|09:18:48] 	mean_loss1: 0.7380817110917365
[06.25.21|09:18:48] 	mean_loss_class: 0.7377160363938919
[06.25.21|09:18:48] 	mean_loss_recon: 0.036567487494023734
[06.25.21|09:18:48] 

[06.25.21|09:18:48] 	Top1: 79.37%
[06.25.21|09:18:49] 

[06.25.21|09:18:49] 	Top5: 95.98%
[06.25.21|09:18:49] Done.
[06.25.21|09:18:49] Training epoch: 60
[06.25.21|09:18:49] 	Iter 501100 Done. | loss1: 0.0739 | loss_class: 0.0733 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:19:40] 	Iter 501200 Done. | loss1: 0.7568 | loss_class: 0.7561 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:20:31] 	Iter 501300 Done. | loss1: 0.5032 | loss_class: 0.5025 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:21:21] 	Iter 501400 Done. | loss1: 0.2509 | loss_class: 0.2501 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|09:22:11] 	Iter 501500 Done. | loss1: 0.0972 | loss_class: 0.0965 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:23:02] 	Iter 501600 Done. | loss1: 0.1252 | loss_class: 0.1245 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:23:57] 	Iter 501700 Done. | loss1: 0.6536 | loss_class: 0.6529 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:24:53] 	Iter 501800 Done. | loss1: 0.4158 | loss_class: 0.4151 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:25:47] 	Iter 501900 Done. | loss1: 0.1438 | loss_class: 0.1429 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|09:26:38] 	Iter 502000 Done. | loss1: 0.0155 | loss_class: 0.0149 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:27:29] 	Iter 502100 Done. | loss1: 0.1171 | loss_class: 0.1164 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:28:19] 	Iter 502200 Done. | loss1: 0.3955 | loss_class: 0.3948 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:29:10] 	Iter 502300 Done. | loss1: 1.2928 | loss_class: 1.2920 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|09:30:00] 	Iter 502400 Done. | loss1: 1.6037 | loss_class: 1.6031 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:30:51] 	Iter 502500 Done. | loss1: 1.3786 | loss_class: 1.3778 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|09:31:41] 	Iter 502600 Done. | loss1: 0.7958 | loss_class: 0.7953 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|09:32:31] 	Iter 502700 Done. | loss1: 0.2041 | loss_class: 0.2034 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:33:23] 	Iter 502800 Done. | loss1: 0.1692 | loss_class: 0.1684 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|09:34:13] 	Iter 502900 Done. | loss1: 0.5820 | loss_class: 0.5814 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:35:03] 	Iter 503000 Done. | loss1: 1.4128 | loss_class: 1.4121 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:35:54] 	Iter 503100 Done. | loss1: 0.2919 | loss_class: 0.2913 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:36:45] 	Iter 503200 Done. | loss1: 0.0068 | loss_class: 0.0060 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|09:37:35] 	Iter 503300 Done. | loss1: 0.8463 | loss_class: 0.8458 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:38:25] 	Iter 503400 Done. | loss1: 0.2857 | loss_class: 0.2847 | loss_recon: 0.0011 | lr: 0.010000
[06.25.21|09:39:16] 	Iter 503500 Done. | loss1: 0.0519 | loss_class: 0.0511 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|09:40:06] 	Iter 503600 Done. | loss1: 0.0146 | loss_class: 0.0140 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:40:57] 	Iter 503700 Done. | loss1: 0.1536 | loss_class: 0.1528 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|09:41:48] 	Iter 503800 Done. | loss1: 0.5399 | loss_class: 0.5391 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|09:42:38] 	Iter 503900 Done. | loss1: 0.0146 | loss_class: 0.0140 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:43:28] 	Iter 504000 Done. | loss1: 0.0408 | loss_class: 0.0399 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|09:44:18] 	Iter 504100 Done. | loss1: 0.4389 | loss_class: 0.4382 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:45:08] 	Iter 504200 Done. | loss1: 0.1163 | loss_class: 0.1154 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|09:46:03] 	Iter 504300 Done. | loss1: 0.0420 | loss_class: 0.0414 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:46:53] 	Iter 504400 Done. | loss1: 0.3902 | loss_class: 0.3896 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:47:45] 	Iter 504500 Done. | loss1: 0.8286 | loss_class: 0.8279 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:48:35] 	Iter 504600 Done. | loss1: 0.6241 | loss_class: 0.6235 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:49:25] 	Iter 504700 Done. | loss1: 0.0232 | loss_class: 0.0226 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|09:50:16] 	Iter 504800 Done. | loss1: 1.5343 | loss_class: 1.5334 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|09:51:07] 	Iter 504900 Done. | loss1: 0.0842 | loss_class: 0.0834 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|09:51:58] 	Iter 505000 Done. | loss1: 1.6590 | loss_class: 1.6584 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:52:50] 	Iter 505100 Done. | loss1: 1.1604 | loss_class: 1.1599 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|09:53:41] 	Iter 505200 Done. | loss1: 2.4948 | loss_class: 2.4942 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:54:36] 	Iter 505300 Done. | loss1: 0.1825 | loss_class: 0.1820 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|09:55:29] 	Iter 505400 Done. | loss1: 0.0361 | loss_class: 0.0355 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:56:22] 	Iter 505500 Done. | loss1: 0.1149 | loss_class: 0.1144 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|09:57:12] 	Iter 505600 Done. | loss1: 0.6182 | loss_class: 0.6175 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|09:58:03] 	Iter 505700 Done. | loss1: 0.0699 | loss_class: 0.0692 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|09:58:54] 	Iter 505800 Done. | loss1: 0.1474 | loss_class: 0.1469 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|09:59:44] 	Iter 505900 Done. | loss1: 0.0145 | loss_class: 0.0135 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|10:00:34] 	Iter 506000 Done. | loss1: 0.5820 | loss_class: 0.5813 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|10:01:25] 	Iter 506100 Done. | loss1: 0.7993 | loss_class: 0.7986 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|10:02:15] 	Iter 506200 Done. | loss1: 0.7738 | loss_class: 0.7732 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:03:06] 	Iter 506300 Done. | loss1: 0.4415 | loss_class: 0.4409 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:03:56] 	Iter 506400 Done. | loss1: 0.1596 | loss_class: 0.1591 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:04:47] 	Iter 506500 Done. | loss1: 1.5100 | loss_class: 1.5095 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:05:38] 	Iter 506600 Done. | loss1: 0.0134 | loss_class: 0.0128 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|10:06:28] 	Iter 506700 Done. | loss1: 0.0568 | loss_class: 0.0559 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|10:07:19] 	Iter 506800 Done. | loss1: 0.6833 | loss_class: 0.6827 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:08:09] 	Iter 506900 Done. | loss1: 0.8073 | loss_class: 0.8066 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|10:08:59] 	Iter 507000 Done. | loss1: 0.0546 | loss_class: 0.0540 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:09:49] 	Iter 507100 Done. | loss1: 0.0037 | loss_class: 0.0029 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|10:10:40] 	Iter 507200 Done. | loss1: 1.4520 | loss_class: 1.4515 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:11:30] 	Iter 507300 Done. | loss1: 1.1459 | loss_class: 1.1451 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|10:12:20] 	Iter 507400 Done. | loss1: 0.5554 | loss_class: 0.5549 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:13:10] 	Iter 507500 Done. | loss1: 0.8430 | loss_class: 0.8424 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|10:14:00] 	Iter 507600 Done. | loss1: 0.0480 | loss_class: 0.0473 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|10:14:50] 	Iter 507700 Done. | loss1: 0.4045 | loss_class: 0.4037 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|10:15:40] 	Iter 507800 Done. | loss1: 0.0540 | loss_class: 0.0535 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:16:30] 	Iter 507900 Done. | loss1: 0.9279 | loss_class: 0.9273 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:17:24] 	Iter 508000 Done. | loss1: 0.0304 | loss_class: 0.0299 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:18:16] 	Iter 508100 Done. | loss1: 0.3663 | loss_class: 0.3658 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|10:19:07] 	Iter 508200 Done. | loss1: 0.6712 | loss_class: 0.6703 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|10:20:00] 	Iter 508300 Done. | loss1: 0.6811 | loss_class: 0.6805 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|10:20:55] 	Iter 508400 Done. | loss1: 0.1289 | loss_class: 0.1282 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:21:46] 	Iter 508500 Done. | loss1: 0.0360 | loss_class: 0.0354 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:22:36] 	Iter 508600 Done. | loss1: 0.0106 | loss_class: 0.0101 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:23:27] 	Iter 508700 Done. | loss1: 0.0283 | loss_class: 0.0275 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|10:24:17] 	Iter 508800 Done. | loss1: 0.3725 | loss_class: 0.3720 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:25:07] 	Iter 508900 Done. | loss1: 1.7947 | loss_class: 1.7942 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:25:57] 	Iter 509000 Done. | loss1: 0.6710 | loss_class: 0.6705 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:26:49] 	Iter 509100 Done. | loss1: 0.0229 | loss_class: 0.0224 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:27:40] 	Iter 509200 Done. | loss1: 0.0333 | loss_class: 0.0326 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|10:28:31] 	Iter 509300 Done. | loss1: 1.6358 | loss_class: 1.6349 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|10:29:24] 	Iter 509400 Done. | loss1: 0.0032 | loss_class: 0.0023 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|10:30:18] 	Iter 509500 Done. | loss1: 0.2356 | loss_class: 0.2348 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|10:31:08] 	Iter 509600 Done. | loss1: 1.0780 | loss_class: 1.0772 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|10:31:58] 	Iter 509700 Done. | loss1: 0.5269 | loss_class: 0.5263 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:32:49] 	Iter 509800 Done. | loss1: 0.2771 | loss_class: 0.2763 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|10:33:39] 	Iter 509900 Done. | loss1: 0.1128 | loss_class: 0.1122 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:34:29] 	Iter 510000 Done. | loss1: 0.0543 | loss_class: 0.0535 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|10:35:19] 	Iter 510100 Done. | loss1: 0.6951 | loss_class: 0.6942 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|10:36:09] 	Iter 510200 Done. | loss1: 0.0447 | loss_class: 0.0442 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:37:00] 	Iter 510300 Done. | loss1: 0.1063 | loss_class: 0.1056 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|10:37:50] 	Iter 510400 Done. | loss1: 0.3367 | loss_class: 0.3360 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:38:40] 	Iter 510500 Done. | loss1: 3.1366 | loss_class: 3.1359 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:39:31] 	Iter 510600 Done. | loss1: 0.4464 | loss_class: 0.4458 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:40:22] 	Iter 510700 Done. | loss1: 0.8157 | loss_class: 0.8149 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|10:41:12] 	Iter 510800 Done. | loss1: 0.0326 | loss_class: 0.0320 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:42:02] 	Iter 510900 Done. | loss1: 0.8892 | loss_class: 0.8886 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:42:55] 	Iter 511000 Done. | loss1: 0.2787 | loss_class: 0.2782 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:43:46] 	Iter 511100 Done. | loss1: 0.5899 | loss_class: 0.5893 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:43:57] 	mean_loss1: 0.45488854718999633
[06.25.21|10:43:57] 	mean_loss_class: 0.4542416814208202
[06.25.21|10:43:57] 	mean_loss_recon: 0.0006468660018541182
[06.25.21|10:43:57] Time consumption:
[06.25.21|10:43:57] Done.
[06.25.21|10:43:57] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch60_model1.pt.
[06.25.21|10:43:57] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch60_model2.pt.
[06.25.21|10:43:57] Training epoch: 61
[06.25.21|10:44:37] 	Iter 511200 Done. | loss1: 0.2153 | loss_class: 0.2147 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:45:29] 	Iter 511300 Done. | loss1: 1.1809 | loss_class: 1.1804 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:46:20] 	Iter 511400 Done. | loss1: 0.4809 | loss_class: 0.4801 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|10:47:11] 	Iter 511500 Done. | loss1: 0.4435 | loss_class: 0.4430 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:48:02] 	Iter 511600 Done. | loss1: 0.5405 | loss_class: 0.5395 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|10:48:55] 	Iter 511700 Done. | loss1: 0.1283 | loss_class: 0.1276 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|10:49:46] 	Iter 511800 Done. | loss1: 0.1716 | loss_class: 0.1710 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:50:37] 	Iter 511900 Done. | loss1: 0.6810 | loss_class: 0.6805 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:51:28] 	Iter 512000 Done. | loss1: 1.1599 | loss_class: 1.1594 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:52:18] 	Iter 512100 Done. | loss1: 0.8028 | loss_class: 0.8022 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:53:09] 	Iter 512200 Done. | loss1: 0.0512 | loss_class: 0.0505 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|10:54:00] 	Iter 512300 Done. | loss1: 0.0969 | loss_class: 0.0963 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:54:51] 	Iter 512400 Done. | loss1: 1.5484 | loss_class: 1.5479 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:55:42] 	Iter 512500 Done. | loss1: 0.2855 | loss_class: 0.2848 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|10:56:32] 	Iter 512600 Done. | loss1: 0.0374 | loss_class: 0.0369 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:57:23] 	Iter 512700 Done. | loss1: 1.0710 | loss_class: 1.0705 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|10:58:14] 	Iter 512800 Done. | loss1: 0.2608 | loss_class: 0.2602 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|10:59:04] 	Iter 512900 Done. | loss1: 0.8174 | loss_class: 0.8164 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|10:59:55] 	Iter 513000 Done. | loss1: 0.8847 | loss_class: 0.8839 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|11:00:46] 	Iter 513100 Done. | loss1: 0.9409 | loss_class: 0.9404 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|11:01:37] 	Iter 513200 Done. | loss1: 0.0215 | loss_class: 0.0208 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:02:28] 	Iter 513300 Done. | loss1: 1.1588 | loss_class: 1.1582 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:03:18] 	Iter 513400 Done. | loss1: 0.0173 | loss_class: 0.0167 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:04:09] 	Iter 513500 Done. | loss1: 0.0613 | loss_class: 0.0591 | loss_recon: 0.0022 | lr: 0.010000
[06.25.21|11:05:00] 	Iter 513600 Done. | loss1: 0.2189 | loss_class: 0.2183 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:05:50] 	Iter 513700 Done. | loss1: 0.2676 | loss_class: 0.2670 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:06:42] 	Iter 513800 Done. | loss1: 0.5150 | loss_class: 0.5144 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:07:33] 	Iter 513900 Done. | loss1: 0.7508 | loss_class: 0.7501 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:08:24] 	Iter 514000 Done. | loss1: 0.0372 | loss_class: 0.0365 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|11:09:15] 	Iter 514100 Done. | loss1: 0.0777 | loss_class: 0.0770 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:10:06] 	Iter 514200 Done. | loss1: 0.0398 | loss_class: 0.0392 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:10:57] 	Iter 514300 Done. | loss1: 0.0466 | loss_class: 0.0460 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:11:49] 	Iter 514400 Done. | loss1: 0.2387 | loss_class: 0.2383 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|11:12:43] 	Iter 514500 Done. | loss1: 1.3566 | loss_class: 1.3560 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|11:13:33] 	Iter 514600 Done. | loss1: 0.2729 | loss_class: 0.2724 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|11:14:24] 	Iter 514700 Done. | loss1: 0.1534 | loss_class: 0.1529 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|11:15:14] 	Iter 514800 Done. | loss1: 0.0114 | loss_class: 0.0107 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:16:07] 	Iter 514900 Done. | loss1: 1.7495 | loss_class: 1.7490 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:16:58] 	Iter 515000 Done. | loss1: 0.2742 | loss_class: 0.2735 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:17:48] 	Iter 515100 Done. | loss1: 0.0649 | loss_class: 0.0642 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:18:40] 	Iter 515200 Done. | loss1: 0.1184 | loss_class: 0.1176 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|11:19:32] 	Iter 515300 Done. | loss1: 0.6954 | loss_class: 0.6948 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:20:23] 	Iter 515400 Done. | loss1: 0.3683 | loss_class: 0.3675 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|11:21:13] 	Iter 515500 Done. | loss1: 1.6958 | loss_class: 1.6953 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|11:22:04] 	Iter 515600 Done. | loss1: 0.2063 | loss_class: 0.2055 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|11:22:54] 	Iter 515700 Done. | loss1: 1.2551 | loss_class: 1.2545 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:23:44] 	Iter 515800 Done. | loss1: 2.4950 | loss_class: 2.4943 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:24:34] 	Iter 515900 Done. | loss1: 2.2252 | loss_class: 2.2244 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|11:25:25] 	Iter 516000 Done. | loss1: 0.0213 | loss_class: 0.0206 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:26:15] 	Iter 516100 Done. | loss1: 0.0085 | loss_class: 0.0079 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:27:05] 	Iter 516200 Done. | loss1: 0.8562 | loss_class: 0.8555 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:27:55] 	Iter 516300 Done. | loss1: 0.2441 | loss_class: 0.2435 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:28:46] 	Iter 516400 Done. | loss1: 0.3976 | loss_class: 0.3968 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|11:29:36] 	Iter 516500 Done. | loss1: 0.1429 | loss_class: 0.1422 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:30:27] 	Iter 516600 Done. | loss1: 0.0348 | loss_class: 0.0340 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|11:31:17] 	Iter 516700 Done. | loss1: 0.4697 | loss_class: 0.4688 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|11:32:06] 	Iter 516800 Done. | loss1: 0.6606 | loss_class: 0.6599 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|11:32:56] 	Iter 516900 Done. | loss1: 1.1248 | loss_class: 1.1242 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:33:46] 	Iter 517000 Done. | loss1: 0.2480 | loss_class: 0.2475 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|11:34:36] 	Iter 517100 Done. | loss1: 0.6931 | loss_class: 0.6924 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:35:27] 	Iter 517200 Done. | loss1: 0.1241 | loss_class: 0.1235 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:36:18] 	Iter 517300 Done. | loss1: 1.4701 | loss_class: 1.4696 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|11:37:08] 	Iter 517400 Done. | loss1: 0.4928 | loss_class: 0.4922 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:37:59] 	Iter 517500 Done. | loss1: 0.0130 | loss_class: 0.0122 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|11:38:49] 	Iter 517600 Done. | loss1: 0.9330 | loss_class: 0.9324 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:39:39] 	Iter 517700 Done. | loss1: 0.0598 | loss_class: 0.0594 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|11:40:30] 	Iter 517800 Done. | loss1: 0.6164 | loss_class: 0.6155 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|11:41:21] 	Iter 517900 Done. | loss1: 0.8118 | loss_class: 0.8111 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:42:12] 	Iter 518000 Done. | loss1: 0.2017 | loss_class: 0.2011 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:43:02] 	Iter 518100 Done. | loss1: 1.4087 | loss_class: 1.4082 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:43:52] 	Iter 518200 Done. | loss1: 0.0156 | loss_class: 0.0149 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:44:45] 	Iter 518300 Done. | loss1: 0.5970 | loss_class: 0.5964 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:45:36] 	Iter 518400 Done. | loss1: 1.0591 | loss_class: 1.0584 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:46:26] 	Iter 518500 Done. | loss1: 0.0852 | loss_class: 0.0846 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|11:47:16] 	Iter 518600 Done. | loss1: 0.5533 | loss_class: 0.5526 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|11:48:07] 	Iter 518700 Done. | loss1: 2.0047 | loss_class: 2.0040 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:48:58] 	Iter 518800 Done. | loss1: 0.2697 | loss_class: 0.2691 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:49:48] 	Iter 518900 Done. | loss1: 1.5249 | loss_class: 1.5243 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:50:39] 	Iter 519000 Done. | loss1: 1.4110 | loss_class: 1.4103 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:51:29] 	Iter 519100 Done. | loss1: 0.3528 | loss_class: 0.3522 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:52:19] 	Iter 519200 Done. | loss1: 0.2950 | loss_class: 0.2945 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|11:53:09] 	Iter 519300 Done. | loss1: 0.0475 | loss_class: 0.0468 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:53:59] 	Iter 519400 Done. | loss1: 0.5208 | loss_class: 0.5202 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:54:49] 	Iter 519500 Done. | loss1: 0.6818 | loss_class: 0.6811 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:55:38] 	Iter 519600 Done. | loss1: 1.3177 | loss_class: 1.3168 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|11:56:28] 	Iter 519700 Done. | loss1: 0.9250 | loss_class: 0.9245 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|11:57:18] 	Iter 519800 Done. | loss1: 2.1288 | loss_class: 2.1282 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|11:58:08] 	Iter 519900 Done. | loss1: 2.0341 | loss_class: 2.0334 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|11:58:58] 	Iter 520000 Done. | loss1: 0.1117 | loss_class: 0.1110 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|11:59:48] 	Iter 520100 Done. | loss1: 0.7625 | loss_class: 0.7619 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:00:39] 	Iter 520200 Done. | loss1: 0.3458 | loss_class: 0.3451 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:01:30] 	Iter 520300 Done. | loss1: 0.0574 | loss_class: 0.0569 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:02:21] 	Iter 520400 Done. | loss1: 0.3338 | loss_class: 0.3331 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:03:11] 	Iter 520500 Done. | loss1: 0.0334 | loss_class: 0.0326 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|12:04:01] 	Iter 520600 Done. | loss1: 0.0215 | loss_class: 0.0209 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:04:51] 	Iter 520700 Done. | loss1: 0.8826 | loss_class: 0.8820 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:05:41] 	Iter 520800 Done. | loss1: 0.0143 | loss_class: 0.0138 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:06:32] 	Iter 520900 Done. | loss1: 0.2258 | loss_class: 0.2251 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:07:23] 	Iter 521000 Done. | loss1: 0.3477 | loss_class: 0.3470 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:08:14] 	Iter 521100 Done. | loss1: 0.8496 | loss_class: 0.8491 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:08:36] 	mean_loss1: 0.4493020638156392
[06.25.21|12:08:36] 	mean_loss_class: 0.44865592546047367
[06.25.21|12:08:36] 	mean_loss_recon: 0.0006461383755361754
[06.25.21|12:08:36] Time consumption:
[06.25.21|12:08:36] Done.
[06.25.21|12:08:36] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch61_model1.pt.
[06.25.21|12:08:36] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch61_model2.pt.
[06.25.21|12:08:36] Training epoch: 62
[06.25.21|12:09:05] 	Iter 521200 Done. | loss1: 0.4325 | loss_class: 0.4319 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:09:56] 	Iter 521300 Done. | loss1: 0.6424 | loss_class: 0.6416 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:10:46] 	Iter 521400 Done. | loss1: 0.1988 | loss_class: 0.1983 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:11:37] 	Iter 521500 Done. | loss1: 1.8979 | loss_class: 1.8973 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:12:28] 	Iter 521600 Done. | loss1: 0.2478 | loss_class: 0.2471 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:13:18] 	Iter 521700 Done. | loss1: 0.0147 | loss_class: 0.0137 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|12:14:09] 	Iter 521800 Done. | loss1: 0.1080 | loss_class: 0.1073 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:14:59] 	Iter 521900 Done. | loss1: 0.0696 | loss_class: 0.0690 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:15:49] 	Iter 522000 Done. | loss1: 1.1113 | loss_class: 1.1107 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:16:39] 	Iter 522100 Done. | loss1: 1.2207 | loss_class: 1.2202 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:17:30] 	Iter 522200 Done. | loss1: 0.0487 | loss_class: 0.0480 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|12:18:20] 	Iter 522300 Done. | loss1: 0.6372 | loss_class: 0.6365 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:19:11] 	Iter 522400 Done. | loss1: 0.3926 | loss_class: 0.3921 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:20:03] 	Iter 522500 Done. | loss1: 0.9425 | loss_class: 0.9418 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:20:54] 	Iter 522600 Done. | loss1: 0.2037 | loss_class: 0.2030 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:21:44] 	Iter 522700 Done. | loss1: 0.0189 | loss_class: 0.0180 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|12:22:35] 	Iter 522800 Done. | loss1: 0.0598 | loss_class: 0.0590 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:23:26] 	Iter 522900 Done. | loss1: 0.3017 | loss_class: 0.3011 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:24:16] 	Iter 523000 Done. | loss1: 0.0689 | loss_class: 0.0683 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:25:06] 	Iter 523100 Done. | loss1: 0.0862 | loss_class: 0.0857 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:25:57] 	Iter 523200 Done. | loss1: 0.9395 | loss_class: 0.9389 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:26:47] 	Iter 523300 Done. | loss1: 0.2470 | loss_class: 0.2465 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:27:38] 	Iter 523400 Done. | loss1: 0.9736 | loss_class: 0.9729 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|12:28:28] 	Iter 523500 Done. | loss1: 0.6171 | loss_class: 0.6164 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:29:19] 	Iter 523600 Done. | loss1: 0.1029 | loss_class: 0.1022 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|12:30:13] 	Iter 523700 Done. | loss1: 0.8535 | loss_class: 0.8528 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:31:03] 	Iter 523800 Done. | loss1: 0.2292 | loss_class: 0.2286 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:31:54] 	Iter 523900 Done. | loss1: 1.1978 | loss_class: 1.1970 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|12:32:44] 	Iter 524000 Done. | loss1: 0.3639 | loss_class: 0.3632 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:33:35] 	Iter 524100 Done. | loss1: 0.2297 | loss_class: 0.2291 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:34:25] 	Iter 524200 Done. | loss1: 0.1595 | loss_class: 0.1588 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:35:15] 	Iter 524300 Done. | loss1: 0.3344 | loss_class: 0.3338 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:36:05] 	Iter 524400 Done. | loss1: 0.1718 | loss_class: 0.1709 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|12:36:55] 	Iter 524500 Done. | loss1: 0.1494 | loss_class: 0.1487 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:37:45] 	Iter 524600 Done. | loss1: 0.1423 | loss_class: 0.1416 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:38:35] 	Iter 524700 Done. | loss1: 1.0627 | loss_class: 1.0622 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:39:25] 	Iter 524800 Done. | loss1: 0.3873 | loss_class: 0.3866 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:40:15] 	Iter 524900 Done. | loss1: 0.6453 | loss_class: 0.6446 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:41:05] 	Iter 525000 Done. | loss1: 0.7150 | loss_class: 0.7141 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|12:41:56] 	Iter 525100 Done. | loss1: 0.1800 | loss_class: 0.1795 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:42:46] 	Iter 525200 Done. | loss1: 0.4778 | loss_class: 0.4772 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:43:36] 	Iter 525300 Done. | loss1: 0.2522 | loss_class: 0.2514 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:44:27] 	Iter 525400 Done. | loss1: 0.0092 | loss_class: 0.0087 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:45:22] 	Iter 525500 Done. | loss1: 0.0510 | loss_class: 0.0505 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:46:18] 	Iter 525600 Done. | loss1: 0.0149 | loss_class: 0.0144 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:47:08] 	Iter 525700 Done. | loss1: 0.6588 | loss_class: 0.6581 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:48:01] 	Iter 525800 Done. | loss1: 0.0854 | loss_class: 0.0849 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:48:54] 	Iter 525900 Done. | loss1: 0.3367 | loss_class: 0.3359 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|12:49:45] 	Iter 526000 Done. | loss1: 0.1123 | loss_class: 0.1116 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|12:50:36] 	Iter 526100 Done. | loss1: 0.1410 | loss_class: 0.1402 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|12:51:27] 	Iter 526200 Done. | loss1: 0.6666 | loss_class: 0.6660 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:52:19] 	Iter 526300 Done. | loss1: 0.0309 | loss_class: 0.0303 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:53:10] 	Iter 526400 Done. | loss1: 0.4024 | loss_class: 0.4016 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|12:54:00] 	Iter 526500 Done. | loss1: 0.0082 | loss_class: 0.0077 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:54:51] 	Iter 526600 Done. | loss1: 0.0769 | loss_class: 0.0764 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:55:42] 	Iter 526700 Done. | loss1: 0.0112 | loss_class: 0.0106 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:56:35] 	Iter 526800 Done. | loss1: 0.8397 | loss_class: 0.8390 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|12:57:26] 	Iter 526900 Done. | loss1: 1.4742 | loss_class: 1.4734 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|12:58:17] 	Iter 527000 Done. | loss1: 0.0591 | loss_class: 0.0586 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|12:59:09] 	Iter 527100 Done. | loss1: 1.2604 | loss_class: 1.2600 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|13:00:00] 	Iter 527200 Done. | loss1: 0.1930 | loss_class: 0.1921 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|13:00:51] 	Iter 527300 Done. | loss1: 1.8948 | loss_class: 1.8941 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:01:42] 	Iter 527400 Done. | loss1: 0.0497 | loss_class: 0.0491 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:02:33] 	Iter 527500 Done. | loss1: 0.0535 | loss_class: 0.0530 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|13:03:23] 	Iter 527600 Done. | loss1: 0.1655 | loss_class: 0.1646 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|13:04:14] 	Iter 527700 Done. | loss1: 0.4448 | loss_class: 0.4441 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:05:04] 	Iter 527800 Done. | loss1: 0.9698 | loss_class: 0.9692 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|13:05:54] 	Iter 527900 Done. | loss1: 0.2431 | loss_class: 0.2426 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|13:06:45] 	Iter 528000 Done. | loss1: 0.4291 | loss_class: 0.4286 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|13:07:36] 	Iter 528100 Done. | loss1: 0.1347 | loss_class: 0.1343 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|13:08:27] 	Iter 528200 Done. | loss1: 0.2600 | loss_class: 0.2594 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|13:09:19] 	Iter 528300 Done. | loss1: 0.0510 | loss_class: 0.0504 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:10:13] 	Iter 528400 Done. | loss1: 0.1145 | loss_class: 0.1139 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:11:07] 	Iter 528500 Done. | loss1: 0.4832 | loss_class: 0.4828 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|13:11:58] 	Iter 528600 Done. | loss1: 0.2649 | loss_class: 0.2643 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:12:49] 	Iter 528700 Done. | loss1: 0.0510 | loss_class: 0.0503 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|13:13:42] 	Iter 528800 Done. | loss1: 0.3710 | loss_class: 0.3705 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|13:14:32] 	Iter 528900 Done. | loss1: 0.3356 | loss_class: 0.3348 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|13:15:21] 	Iter 529000 Done. | loss1: 0.0052 | loss_class: 0.0043 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|13:16:12] 	Iter 529100 Done. | loss1: 0.4443 | loss_class: 0.4436 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:17:02] 	Iter 529200 Done. | loss1: 0.0868 | loss_class: 0.0862 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|13:17:53] 	Iter 529300 Done. | loss1: 0.3865 | loss_class: 0.3856 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|13:18:46] 	Iter 529400 Done. | loss1: 0.6402 | loss_class: 0.6396 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:19:36] 	Iter 529500 Done. | loss1: 0.2796 | loss_class: 0.2789 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:20:28] 	Iter 529600 Done. | loss1: 2.0368 | loss_class: 2.0363 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:21:18] 	Iter 529700 Done. | loss1: 0.4857 | loss_class: 0.4851 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:22:08] 	Iter 529800 Done. | loss1: 0.2892 | loss_class: 0.2886 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:22:58] 	Iter 529900 Done. | loss1: 0.1020 | loss_class: 0.1013 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:23:48] 	Iter 530000 Done. | loss1: 0.6040 | loss_class: 0.6034 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:24:39] 	Iter 530100 Done. | loss1: 0.1369 | loss_class: 0.1363 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:25:29] 	Iter 530200 Done. | loss1: 0.9289 | loss_class: 0.9282 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:26:20] 	Iter 530300 Done. | loss1: 0.0053 | loss_class: 0.0048 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|13:27:10] 	Iter 530400 Done. | loss1: 0.4199 | loss_class: 0.4192 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:28:01] 	Iter 530500 Done. | loss1: 0.2036 | loss_class: 0.2028 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|13:28:51] 	Iter 530600 Done. | loss1: 0.5687 | loss_class: 0.5683 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|13:29:42] 	Iter 530700 Done. | loss1: 1.4817 | loss_class: 1.4809 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:30:32] 	Iter 530800 Done. | loss1: 0.1386 | loss_class: 0.1378 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|13:31:23] 	Iter 530900 Done. | loss1: 0.0219 | loss_class: 0.0210 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|13:32:13] 	Iter 531000 Done. | loss1: 0.0689 | loss_class: 0.0684 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:33:08] 	Iter 531100 Done. | loss1: 0.2326 | loss_class: 0.2322 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|13:33:41] 	mean_loss1: 0.4503295498108742
[06.25.21|13:33:41] 	mean_loss_class: 0.4496829918763483
[06.25.21|13:33:41] 	mean_loss_recon: 0.0006465579439166467
[06.25.21|13:33:41] Time consumption:
[06.25.21|13:33:41] Done.
[06.25.21|13:33:41] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch62_model1.pt.
[06.25.21|13:33:41] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch62_model2.pt.
[06.25.21|13:33:41] Training epoch: 63
[06.25.21|13:33:59] 	Iter 531200 Done. | loss1: 0.0114 | loss_class: 0.0106 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|13:34:50] 	Iter 531300 Done. | loss1: 0.4130 | loss_class: 0.4124 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:35:41] 	Iter 531400 Done. | loss1: 1.1927 | loss_class: 1.1920 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:36:32] 	Iter 531500 Done. | loss1: 0.5587 | loss_class: 0.5581 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:37:23] 	Iter 531600 Done. | loss1: 0.2730 | loss_class: 0.2725 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:38:15] 	Iter 531700 Done. | loss1: 0.2860 | loss_class: 0.2856 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|13:39:05] 	Iter 531800 Done. | loss1: 0.4520 | loss_class: 0.4513 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|13:39:57] 	Iter 531900 Done. | loss1: 0.7615 | loss_class: 0.7609 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|13:40:47] 	Iter 532000 Done. | loss1: 0.2480 | loss_class: 0.2474 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|13:41:37] 	Iter 532100 Done. | loss1: 0.8019 | loss_class: 0.8013 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:42:27] 	Iter 532200 Done. | loss1: 0.0295 | loss_class: 0.0289 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:43:18] 	Iter 532300 Done. | loss1: 0.3090 | loss_class: 0.3084 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:44:08] 	Iter 532400 Done. | loss1: 0.0101 | loss_class: 0.0095 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:44:58] 	Iter 532500 Done. | loss1: 0.1786 | loss_class: 0.1780 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:45:48] 	Iter 532600 Done. | loss1: 0.1203 | loss_class: 0.1194 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|13:46:38] 	Iter 532700 Done. | loss1: 0.3703 | loss_class: 0.3695 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:47:29] 	Iter 532800 Done. | loss1: 0.1612 | loss_class: 0.1603 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|13:48:20] 	Iter 532900 Done. | loss1: 0.2236 | loss_class: 0.2232 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|13:49:10] 	Iter 533000 Done. | loss1: 0.0433 | loss_class: 0.0428 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|13:50:00] 	Iter 533100 Done. | loss1: 0.6383 | loss_class: 0.6377 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:50:50] 	Iter 533200 Done. | loss1: 0.1098 | loss_class: 0.1091 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:51:40] 	Iter 533300 Done. | loss1: 0.0524 | loss_class: 0.0518 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:52:30] 	Iter 533400 Done. | loss1: 0.8463 | loss_class: 0.8457 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:53:20] 	Iter 533500 Done. | loss1: 0.0378 | loss_class: 0.0370 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|13:54:10] 	Iter 533600 Done. | loss1: 0.0808 | loss_class: 0.0801 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:55:01] 	Iter 533700 Done. | loss1: 0.0522 | loss_class: 0.0516 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:55:51] 	Iter 533800 Done. | loss1: 0.0215 | loss_class: 0.0209 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|13:56:42] 	Iter 533900 Done. | loss1: 0.3330 | loss_class: 0.3323 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:57:32] 	Iter 534000 Done. | loss1: 0.0562 | loss_class: 0.0555 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|13:58:22] 	Iter 534100 Done. | loss1: 0.5763 | loss_class: 0.5753 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|13:59:12] 	Iter 534200 Done. | loss1: 0.0446 | loss_class: 0.0439 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:00:02] 	Iter 534300 Done. | loss1: 0.0976 | loss_class: 0.0972 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|14:00:54] 	Iter 534400 Done. | loss1: 0.0657 | loss_class: 0.0649 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:01:46] 	Iter 534500 Done. | loss1: 0.0766 | loss_class: 0.0761 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:02:36] 	Iter 534600 Done. | loss1: 0.0509 | loss_class: 0.0504 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:03:26] 	Iter 534700 Done. | loss1: 0.0625 | loss_class: 0.0620 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:04:16] 	Iter 534800 Done. | loss1: 0.4649 | loss_class: 0.4644 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:05:06] 	Iter 534900 Done. | loss1: 1.7390 | loss_class: 1.7383 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|14:05:57] 	Iter 535000 Done. | loss1: 0.8507 | loss_class: 0.8501 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:06:47] 	Iter 535100 Done. | loss1: 0.8664 | loss_class: 0.8658 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:07:38] 	Iter 535200 Done. | loss1: 0.0631 | loss_class: 0.0625 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:08:29] 	Iter 535300 Done. | loss1: 0.3521 | loss_class: 0.3515 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:09:19] 	Iter 535400 Done. | loss1: 0.0517 | loss_class: 0.0512 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|14:10:10] 	Iter 535500 Done. | loss1: 2.0431 | loss_class: 2.0425 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:11:00] 	Iter 535600 Done. | loss1: 0.3124 | loss_class: 0.3118 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:11:50] 	Iter 535700 Done. | loss1: 1.1543 | loss_class: 1.1538 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:12:45] 	Iter 535800 Done. | loss1: 0.2602 | loss_class: 0.2596 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:13:36] 	Iter 535900 Done. | loss1: 0.4188 | loss_class: 0.4181 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:14:27] 	Iter 536000 Done. | loss1: 0.1230 | loss_class: 0.1225 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:15:17] 	Iter 536100 Done. | loss1: 0.0096 | loss_class: 0.0089 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:16:07] 	Iter 536200 Done. | loss1: 0.3040 | loss_class: 0.3033 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|14:16:57] 	Iter 536300 Done. | loss1: 0.0284 | loss_class: 0.0276 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:17:46] 	Iter 536400 Done. | loss1: 0.1380 | loss_class: 0.1374 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:18:36] 	Iter 536500 Done. | loss1: 0.7774 | loss_class: 0.7769 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:19:26] 	Iter 536600 Done. | loss1: 1.1353 | loss_class: 1.1344 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:20:16] 	Iter 536700 Done. | loss1: 0.3002 | loss_class: 0.2997 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:21:06] 	Iter 536800 Done. | loss1: 0.3037 | loss_class: 0.3031 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|14:21:56] 	Iter 536900 Done. | loss1: 0.6772 | loss_class: 0.6767 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:22:46] 	Iter 537000 Done. | loss1: 0.3358 | loss_class: 0.3350 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:23:35] 	Iter 537100 Done. | loss1: 0.1378 | loss_class: 0.1371 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|14:24:25] 	Iter 537200 Done. | loss1: 1.3979 | loss_class: 1.3973 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:25:15] 	Iter 537300 Done. | loss1: 0.1010 | loss_class: 0.1003 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|14:26:05] 	Iter 537400 Done. | loss1: 0.4315 | loss_class: 0.4307 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:26:55] 	Iter 537500 Done. | loss1: 0.1959 | loss_class: 0.1952 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|14:27:46] 	Iter 537600 Done. | loss1: 1.3396 | loss_class: 1.3390 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:28:36] 	Iter 537700 Done. | loss1: 0.0623 | loss_class: 0.0617 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|14:29:26] 	Iter 537800 Done. | loss1: 0.2100 | loss_class: 0.2094 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:30:18] 	Iter 537900 Done. | loss1: 0.1067 | loss_class: 0.1059 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:31:09] 	Iter 538000 Done. | loss1: 0.1275 | loss_class: 0.1269 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|14:32:00] 	Iter 538100 Done. | loss1: 0.4180 | loss_class: 0.4172 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:32:53] 	Iter 538200 Done. | loss1: 0.1355 | loss_class: 0.1350 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:33:43] 	Iter 538300 Done. | loss1: 0.0874 | loss_class: 0.0868 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|14:34:34] 	Iter 538400 Done. | loss1: 0.0249 | loss_class: 0.0241 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:35:26] 	Iter 538500 Done. | loss1: 0.3648 | loss_class: 0.3639 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:36:16] 	Iter 538600 Done. | loss1: 1.3771 | loss_class: 1.3765 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:37:09] 	Iter 538700 Done. | loss1: 1.8672 | loss_class: 1.8665 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|14:37:59] 	Iter 538800 Done. | loss1: 0.1979 | loss_class: 0.1972 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|14:38:50] 	Iter 538900 Done. | loss1: 0.3731 | loss_class: 0.3722 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|14:39:40] 	Iter 539000 Done. | loss1: 0.2813 | loss_class: 0.2804 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|14:40:31] 	Iter 539100 Done. | loss1: 0.8443 | loss_class: 0.8438 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|14:41:21] 	Iter 539200 Done. | loss1: 0.1221 | loss_class: 0.1215 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:42:12] 	Iter 539300 Done. | loss1: 0.1373 | loss_class: 0.1367 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:43:03] 	Iter 539400 Done. | loss1: 0.1648 | loss_class: 0.1638 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|14:43:54] 	Iter 539500 Done. | loss1: 0.0141 | loss_class: 0.0135 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:44:49] 	Iter 539600 Done. | loss1: 0.2608 | loss_class: 0.2603 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:45:39] 	Iter 539700 Done. | loss1: 0.4372 | loss_class: 0.4367 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|14:46:30] 	Iter 539800 Done. | loss1: 0.0577 | loss_class: 0.0571 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:47:20] 	Iter 539900 Done. | loss1: 0.0719 | loss_class: 0.0710 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|14:48:11] 	Iter 540000 Done. | loss1: 1.3332 | loss_class: 1.3321 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|14:49:01] 	Iter 540100 Done. | loss1: 0.5723 | loss_class: 0.5716 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|14:49:51] 	Iter 540200 Done. | loss1: 0.6501 | loss_class: 0.6493 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:50:43] 	Iter 540300 Done. | loss1: 0.0111 | loss_class: 0.0105 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:51:33] 	Iter 540400 Done. | loss1: 1.2644 | loss_class: 1.2638 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:52:24] 	Iter 540500 Done. | loss1: 0.7390 | loss_class: 0.7384 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:53:14] 	Iter 540600 Done. | loss1: 0.4862 | loss_class: 0.4856 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:54:04] 	Iter 540700 Done. | loss1: 1.4403 | loss_class: 1.4395 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:54:54] 	Iter 540800 Done. | loss1: 0.2495 | loss_class: 0.2490 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:55:44] 	Iter 540900 Done. | loss1: 0.7975 | loss_class: 0.7969 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:56:34] 	Iter 541000 Done. | loss1: 0.0194 | loss_class: 0.0186 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|14:57:24] 	Iter 541100 Done. | loss1: 0.0856 | loss_class: 0.0849 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|14:58:11] 	mean_loss1: 0.4398509199621792
[06.25.21|14:58:11] 	mean_loss_class: 0.4392040669958542
[06.25.21|14:58:11] 	mean_loss_recon: 0.000646852968597788
[06.25.21|14:58:11] Time consumption:
[06.25.21|14:58:11] Done.
[06.25.21|14:58:11] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch63_model1.pt.
[06.25.21|14:58:11] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch63_model2.pt.
[06.25.21|14:58:11] Training epoch: 64
[06.25.21|14:58:18] 	Iter 541200 Done. | loss1: 1.8357 | loss_class: 1.8348 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|14:59:08] 	Iter 541300 Done. | loss1: 0.4403 | loss_class: 0.4393 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|15:00:02] 	Iter 541400 Done. | loss1: 0.2483 | loss_class: 0.2478 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|15:00:52] 	Iter 541500 Done. | loss1: 0.7541 | loss_class: 0.7535 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:01:41] 	Iter 541600 Done. | loss1: 0.5907 | loss_class: 0.5901 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:02:31] 	Iter 541700 Done. | loss1: 0.0415 | loss_class: 0.0409 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:03:22] 	Iter 541800 Done. | loss1: 0.0697 | loss_class: 0.0691 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:04:12] 	Iter 541900 Done. | loss1: 0.2652 | loss_class: 0.2647 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|15:05:03] 	Iter 542000 Done. | loss1: 0.0258 | loss_class: 0.0252 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:05:53] 	Iter 542100 Done. | loss1: 0.8738 | loss_class: 0.8732 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:06:44] 	Iter 542200 Done. | loss1: 0.0267 | loss_class: 0.0261 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:07:35] 	Iter 542300 Done. | loss1: 0.4097 | loss_class: 0.4091 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:08:25] 	Iter 542400 Done. | loss1: 0.5054 | loss_class: 0.5047 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:09:16] 	Iter 542500 Done. | loss1: 0.3661 | loss_class: 0.3655 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:10:07] 	Iter 542600 Done. | loss1: 0.0289 | loss_class: 0.0284 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:10:59] 	Iter 542700 Done. | loss1: 0.8722 | loss_class: 0.8712 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|15:11:49] 	Iter 542800 Done. | loss1: 0.1203 | loss_class: 0.1197 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:12:38] 	Iter 542900 Done. | loss1: 1.9784 | loss_class: 1.9777 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|15:13:29] 	Iter 543000 Done. | loss1: 0.4975 | loss_class: 0.4970 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|15:14:20] 	Iter 543100 Done. | loss1: 0.5532 | loss_class: 0.5526 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:15:09] 	Iter 543200 Done. | loss1: 0.0389 | loss_class: 0.0382 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:16:00] 	Iter 543300 Done. | loss1: 0.5884 | loss_class: 0.5877 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:16:53] 	Iter 543400 Done. | loss1: 0.4698 | loss_class: 0.4691 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:17:43] 	Iter 543500 Done. | loss1: 0.8125 | loss_class: 0.8119 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:18:34] 	Iter 543600 Done. | loss1: 0.9845 | loss_class: 0.9838 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|15:19:24] 	Iter 543700 Done. | loss1: 0.3848 | loss_class: 0.3842 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:20:14] 	Iter 543800 Done. | loss1: 0.2440 | loss_class: 0.2434 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:21:04] 	Iter 543900 Done. | loss1: 0.0049 | loss_class: 0.0043 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:21:54] 	Iter 544000 Done. | loss1: 0.2228 | loss_class: 0.2222 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:22:48] 	Iter 544100 Done. | loss1: 0.0913 | loss_class: 0.0905 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|15:23:39] 	Iter 544200 Done. | loss1: 0.0450 | loss_class: 0.0446 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|15:24:29] 	Iter 544300 Done. | loss1: 0.0067 | loss_class: 0.0059 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|15:25:20] 	Iter 544400 Done. | loss1: 0.4211 | loss_class: 0.4202 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|15:26:10] 	Iter 544500 Done. | loss1: 0.1194 | loss_class: 0.1186 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:27:03] 	Iter 544600 Done. | loss1: 1.3812 | loss_class: 1.3806 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|15:27:54] 	Iter 544700 Done. | loss1: 0.5469 | loss_class: 0.5462 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:28:46] 	Iter 544800 Done. | loss1: 0.1324 | loss_class: 0.1317 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:29:37] 	Iter 544900 Done. | loss1: 0.9100 | loss_class: 0.9092 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|15:30:29] 	Iter 545000 Done. | loss1: 0.1294 | loss_class: 0.1288 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:31:20] 	Iter 545100 Done. | loss1: 0.0288 | loss_class: 0.0281 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:32:12] 	Iter 545200 Done. | loss1: 0.1550 | loss_class: 0.1543 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:33:02] 	Iter 545300 Done. | loss1: 2.9455 | loss_class: 2.9449 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:33:53] 	Iter 545400 Done. | loss1: 0.1386 | loss_class: 0.1379 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:34:44] 	Iter 545500 Done. | loss1: 2.6101 | loss_class: 2.6095 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:35:34] 	Iter 545600 Done. | loss1: 0.4985 | loss_class: 0.4978 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:36:25] 	Iter 545700 Done. | loss1: 0.0259 | loss_class: 0.0252 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:37:15] 	Iter 545800 Done. | loss1: 0.4484 | loss_class: 0.4477 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:38:06] 	Iter 545900 Done. | loss1: 1.3444 | loss_class: 1.3439 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|15:38:57] 	Iter 546000 Done. | loss1: 0.7704 | loss_class: 0.7698 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|15:39:48] 	Iter 546100 Done. | loss1: 0.0096 | loss_class: 0.0089 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:40:38] 	Iter 546200 Done. | loss1: 0.0970 | loss_class: 0.0962 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|15:41:29] 	Iter 546300 Done. | loss1: 1.1661 | loss_class: 1.1656 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|15:42:20] 	Iter 546400 Done. | loss1: 0.6884 | loss_class: 0.6877 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:43:11] 	Iter 546500 Done. | loss1: 0.5106 | loss_class: 0.5099 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:44:02] 	Iter 546600 Done. | loss1: 0.5283 | loss_class: 0.5277 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:44:53] 	Iter 546700 Done. | loss1: 1.0813 | loss_class: 1.0806 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:45:45] 	Iter 546800 Done. | loss1: 0.3514 | loss_class: 0.3510 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|15:46:36] 	Iter 546900 Done. | loss1: 0.0349 | loss_class: 0.0343 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:47:28] 	Iter 547000 Done. | loss1: 0.5360 | loss_class: 0.5355 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:48:19] 	Iter 547100 Done. | loss1: 0.2846 | loss_class: 0.2840 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:49:11] 	Iter 547200 Done. | loss1: 0.8093 | loss_class: 0.8087 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:50:03] 	Iter 547300 Done. | loss1: 0.1335 | loss_class: 0.1330 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|15:50:54] 	Iter 547400 Done. | loss1: 0.8349 | loss_class: 0.8343 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:51:47] 	Iter 547500 Done. | loss1: 0.5059 | loss_class: 0.5051 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|15:52:41] 	Iter 547600 Done. | loss1: 1.3605 | loss_class: 1.3600 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|15:53:33] 	Iter 547700 Done. | loss1: 0.2769 | loss_class: 0.2762 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:54:24] 	Iter 547800 Done. | loss1: 0.8913 | loss_class: 0.8907 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:55:15] 	Iter 547900 Done. | loss1: 0.4159 | loss_class: 0.4154 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|15:56:06] 	Iter 548000 Done. | loss1: 0.3391 | loss_class: 0.3384 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:56:58] 	Iter 548100 Done. | loss1: 0.2134 | loss_class: 0.2129 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|15:57:49] 	Iter 548200 Done. | loss1: 0.6615 | loss_class: 0.6608 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|15:58:41] 	Iter 548300 Done. | loss1: 0.2877 | loss_class: 0.2871 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|15:59:33] 	Iter 548400 Done. | loss1: 2.1172 | loss_class: 2.1165 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|16:00:24] 	Iter 548500 Done. | loss1: 0.1949 | loss_class: 0.1942 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|16:01:15] 	Iter 548600 Done. | loss1: 0.9319 | loss_class: 0.9312 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|16:02:06] 	Iter 548700 Done. | loss1: 0.0072 | loss_class: 0.0066 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|16:02:58] 	Iter 548800 Done. | loss1: 1.7501 | loss_class: 1.7496 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|16:03:48] 	Iter 548900 Done. | loss1: 1.4702 | loss_class: 1.4695 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|16:04:42] 	Iter 549000 Done. | loss1: 0.0538 | loss_class: 0.0533 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|16:05:32] 	Iter 549100 Done. | loss1: 0.7459 | loss_class: 0.7451 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|16:06:23] 	Iter 549200 Done. | loss1: 0.3803 | loss_class: 0.3798 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:07:14] 	Iter 549300 Done. | loss1: 0.3277 | loss_class: 0.3269 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|16:08:06] 	Iter 549400 Done. | loss1: 0.3587 | loss_class: 0.3579 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|16:08:57] 	Iter 549500 Done. | loss1: 0.0779 | loss_class: 0.0773 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|16:09:49] 	Iter 549600 Done. | loss1: 0.3634 | loss_class: 0.3628 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:10:40] 	Iter 549700 Done. | loss1: 1.0097 | loss_class: 1.0091 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:11:30] 	Iter 549800 Done. | loss1: 1.0370 | loss_class: 1.0360 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|16:12:21] 	Iter 549900 Done. | loss1: 0.2283 | loss_class: 0.2274 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|16:13:12] 	Iter 550000 Done. | loss1: 0.5343 | loss_class: 0.5338 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|16:14:05] 	Iter 550100 Done. | loss1: 0.4070 | loss_class: 0.4060 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|16:14:58] 	Iter 550200 Done. | loss1: 0.9868 | loss_class: 0.9862 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:15:49] 	Iter 550300 Done. | loss1: 0.5346 | loss_class: 0.5339 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:16:40] 	Iter 550400 Done. | loss1: 0.1039 | loss_class: 0.1033 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|16:17:30] 	Iter 550500 Done. | loss1: 0.0660 | loss_class: 0.0655 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|16:18:21] 	Iter 550600 Done. | loss1: 0.1050 | loss_class: 0.1044 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:19:10] 	Iter 550700 Done. | loss1: 0.1687 | loss_class: 0.1682 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|16:20:00] 	Iter 550800 Done. | loss1: 0.1562 | loss_class: 0.1554 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|16:20:49] 	Iter 550900 Done. | loss1: 0.0863 | loss_class: 0.0857 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:21:39] 	Iter 551000 Done. | loss1: 0.4994 | loss_class: 0.4988 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|16:22:29] 	Iter 551100 Done. | loss1: 0.1242 | loss_class: 0.1236 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|16:23:19] 	Iter 551200 Done. | loss1: 0.4767 | loss_class: 0.4761 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:23:23] 	mean_loss1: 0.4336258419902746
[06.25.21|16:23:23] 	mean_loss_class: 0.4329799176515159
[06.25.21|16:23:23] 	mean_loss_recon: 0.0006459242624039809
[06.25.21|16:23:23] Time consumption:
[06.25.21|16:23:23] Done.
[06.25.21|16:23:24] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch64_model1.pt.
[06.25.21|16:23:24] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch64_model2.pt.
[06.25.21|16:23:24] Eval epoch: 64
[06.25.21|16:30:01] 	mean_loss1: 0.7182163236475037
[06.25.21|16:30:01] 	mean_loss_class: 0.7178361443080759
[06.25.21|16:30:01] 	mean_loss_recon: 0.03801808145272639
[06.25.21|16:30:01] 

[06.25.21|16:30:01] 	Top1: 80.37%
[06.25.21|16:30:01] 

[06.25.21|16:30:01] 	Top5: 96.13%
[06.25.21|16:30:01] Done.
[06.25.21|16:30:01] Training epoch: 65
[06.25.21|16:30:48] 	Iter 551300 Done. | loss1: 0.3156 | loss_class: 0.3151 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|16:31:39] 	Iter 551400 Done. | loss1: 0.0535 | loss_class: 0.0529 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:32:31] 	Iter 551500 Done. | loss1: 1.1987 | loss_class: 1.1983 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|16:33:22] 	Iter 551600 Done. | loss1: 0.2870 | loss_class: 0.2866 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|16:34:14] 	Iter 551700 Done. | loss1: 0.1853 | loss_class: 0.1848 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|16:35:05] 	Iter 551800 Done. | loss1: 0.5953 | loss_class: 0.5947 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:35:55] 	Iter 551900 Done. | loss1: 0.0676 | loss_class: 0.0670 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|16:36:46] 	Iter 552000 Done. | loss1: 0.0652 | loss_class: 0.0646 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:37:36] 	Iter 552100 Done. | loss1: 0.0502 | loss_class: 0.0494 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|16:38:28] 	Iter 552200 Done. | loss1: 0.2929 | loss_class: 0.2923 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:39:20] 	Iter 552300 Done. | loss1: 0.1840 | loss_class: 0.1830 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|16:40:11] 	Iter 552400 Done. | loss1: 0.0750 | loss_class: 0.0744 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|16:41:04] 	Iter 552500 Done. | loss1: 0.4034 | loss_class: 0.4027 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|16:41:58] 	Iter 552600 Done. | loss1: 0.0725 | loss_class: 0.0716 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|16:42:51] 	Iter 552700 Done. | loss1: 0.2963 | loss_class: 0.2955 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|16:43:43] 	Iter 552800 Done. | loss1: 0.8263 | loss_class: 0.8254 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|16:44:36] 	Iter 552900 Done. | loss1: 0.3195 | loss_class: 0.3190 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:45:28] 	Iter 553000 Done. | loss1: 0.8920 | loss_class: 0.8914 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:46:19] 	Iter 553100 Done. | loss1: 0.0601 | loss_class: 0.0595 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:47:11] 	Iter 553200 Done. | loss1: 0.4655 | loss_class: 0.4647 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|16:48:03] 	Iter 553300 Done. | loss1: 0.0837 | loss_class: 0.0830 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|16:48:54] 	Iter 553400 Done. | loss1: 0.2079 | loss_class: 0.2073 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:49:46] 	Iter 553500 Done. | loss1: 0.3043 | loss_class: 0.3036 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|16:50:38] 	Iter 553600 Done. | loss1: 0.3634 | loss_class: 0.3626 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|16:51:29] 	Iter 553700 Done. | loss1: 0.0092 | loss_class: 0.0085 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|16:52:21] 	Iter 553800 Done. | loss1: 0.6767 | loss_class: 0.6760 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:53:13] 	Iter 553900 Done. | loss1: 0.4367 | loss_class: 0.4358 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|16:54:04] 	Iter 554000 Done. | loss1: 0.2444 | loss_class: 0.2437 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|16:54:55] 	Iter 554100 Done. | loss1: 0.3730 | loss_class: 0.3723 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|16:55:46] 	Iter 554200 Done. | loss1: 0.0361 | loss_class: 0.0354 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|16:56:38] 	Iter 554300 Done. | loss1: 1.2159 | loss_class: 1.2152 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:57:30] 	Iter 554400 Done. | loss1: 0.1301 | loss_class: 0.1296 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:58:22] 	Iter 554500 Done. | loss1: 0.2899 | loss_class: 0.2894 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|16:59:15] 	Iter 554600 Done. | loss1: 0.0967 | loss_class: 0.0960 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|17:00:07] 	Iter 554700 Done. | loss1: 1.1917 | loss_class: 1.1912 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:00:58] 	Iter 554800 Done. | loss1: 0.0371 | loss_class: 0.0365 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:01:50] 	Iter 554900 Done. | loss1: 0.0372 | loss_class: 0.0366 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:02:42] 	Iter 555000 Done. | loss1: 0.2440 | loss_class: 0.2431 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|17:03:34] 	Iter 555100 Done. | loss1: 1.2793 | loss_class: 1.2786 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|17:04:26] 	Iter 555200 Done. | loss1: 0.3754 | loss_class: 0.3748 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:05:17] 	Iter 555300 Done. | loss1: 0.0679 | loss_class: 0.0671 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|17:06:09] 	Iter 555400 Done. | loss1: 1.1752 | loss_class: 1.1746 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:07:01] 	Iter 555500 Done. | loss1: 1.1974 | loss_class: 1.1966 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|17:07:53] 	Iter 555600 Done. | loss1: 0.3470 | loss_class: 0.3464 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|17:08:45] 	Iter 555700 Done. | loss1: 0.3459 | loss_class: 0.3452 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|17:09:37] 	Iter 555800 Done. | loss1: 0.6514 | loss_class: 0.6508 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:10:28] 	Iter 555900 Done. | loss1: 0.1614 | loss_class: 0.1608 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:11:19] 	Iter 556000 Done. | loss1: 0.0498 | loss_class: 0.0492 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:12:11] 	Iter 556100 Done. | loss1: 0.1174 | loss_class: 0.1164 | loss_recon: 0.0011 | lr: 0.010000
[06.25.21|17:13:02] 	Iter 556200 Done. | loss1: 0.0263 | loss_class: 0.0256 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:13:54] 	Iter 556300 Done. | loss1: 0.7661 | loss_class: 0.7653 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|17:14:46] 	Iter 556400 Done. | loss1: 0.1802 | loss_class: 0.1794 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|17:15:37] 	Iter 556500 Done. | loss1: 1.0430 | loss_class: 1.0425 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|17:16:29] 	Iter 556600 Done. | loss1: 0.2452 | loss_class: 0.2444 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|17:17:21] 	Iter 556700 Done. | loss1: 0.1478 | loss_class: 0.1473 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:18:12] 	Iter 556800 Done. | loss1: 0.1846 | loss_class: 0.1841 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:19:04] 	Iter 556900 Done. | loss1: 0.5031 | loss_class: 0.5026 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:19:56] 	Iter 557000 Done. | loss1: 0.0184 | loss_class: 0.0177 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|17:20:47] 	Iter 557100 Done. | loss1: 0.6280 | loss_class: 0.6274 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:21:39] 	Iter 557200 Done. | loss1: 0.2924 | loss_class: 0.2917 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|17:22:30] 	Iter 557300 Done. | loss1: 0.1766 | loss_class: 0.1760 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|17:23:21] 	Iter 557400 Done. | loss1: 0.1915 | loss_class: 0.1909 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|17:24:12] 	Iter 557500 Done. | loss1: 0.0376 | loss_class: 0.0371 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:25:04] 	Iter 557600 Done. | loss1: 0.1363 | loss_class: 0.1355 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|17:25:55] 	Iter 557700 Done. | loss1: 0.6443 | loss_class: 0.6438 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:26:47] 	Iter 557800 Done. | loss1: 0.0052 | loss_class: 0.0047 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:27:38] 	Iter 557900 Done. | loss1: 1.3273 | loss_class: 1.3268 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:28:29] 	Iter 558000 Done. | loss1: 0.0226 | loss_class: 0.0219 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|17:29:21] 	Iter 558100 Done. | loss1: 0.7004 | loss_class: 0.6997 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|17:30:12] 	Iter 558200 Done. | loss1: 0.2216 | loss_class: 0.2210 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:31:04] 	Iter 558300 Done. | loss1: 1.4560 | loss_class: 1.4555 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|17:31:55] 	Iter 558400 Done. | loss1: 0.1119 | loss_class: 0.1111 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|17:32:47] 	Iter 558500 Done. | loss1: 0.2195 | loss_class: 0.2190 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:33:39] 	Iter 558600 Done. | loss1: 0.1072 | loss_class: 0.1067 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:34:30] 	Iter 558700 Done. | loss1: 0.5321 | loss_class: 0.5316 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:35:22] 	Iter 558800 Done. | loss1: 0.5627 | loss_class: 0.5622 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:36:13] 	Iter 558900 Done. | loss1: 0.4224 | loss_class: 0.4219 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:37:04] 	Iter 559000 Done. | loss1: 0.0197 | loss_class: 0.0188 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|17:37:56] 	Iter 559100 Done. | loss1: 0.3697 | loss_class: 0.3692 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:38:47] 	Iter 559200 Done. | loss1: 0.3183 | loss_class: 0.3171 | loss_recon: 0.0012 | lr: 0.010000
[06.25.21|17:39:39] 	Iter 559300 Done. | loss1: 0.3664 | loss_class: 0.3660 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:40:30] 	Iter 559400 Done. | loss1: 0.0951 | loss_class: 0.0945 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:41:21] 	Iter 559500 Done. | loss1: 0.4380 | loss_class: 0.4374 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:42:12] 	Iter 559600 Done. | loss1: 0.1434 | loss_class: 0.1426 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|17:43:03] 	Iter 559700 Done. | loss1: 0.6732 | loss_class: 0.6726 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:43:55] 	Iter 559800 Done. | loss1: 0.0362 | loss_class: 0.0357 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:44:47] 	Iter 559900 Done. | loss1: 0.6556 | loss_class: 0.6550 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:45:38] 	Iter 560000 Done. | loss1: 0.1241 | loss_class: 0.1234 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:46:30] 	Iter 560100 Done. | loss1: 0.3188 | loss_class: 0.3181 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|17:47:22] 	Iter 560200 Done. | loss1: 1.5198 | loss_class: 1.5194 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|17:48:13] 	Iter 560300 Done. | loss1: 0.0421 | loss_class: 0.0417 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|17:49:05] 	Iter 560400 Done. | loss1: 0.0395 | loss_class: 0.0391 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|17:49:56] 	Iter 560500 Done. | loss1: 0.0495 | loss_class: 0.0489 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:50:48] 	Iter 560600 Done. | loss1: 0.1668 | loss_class: 0.1663 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:51:39] 	Iter 560700 Done. | loss1: 0.3695 | loss_class: 0.3690 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:52:30] 	Iter 560800 Done. | loss1: 0.0086 | loss_class: 0.0079 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|17:53:22] 	Iter 560900 Done. | loss1: 0.9043 | loss_class: 0.9037 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|17:54:13] 	Iter 561000 Done. | loss1: 0.1262 | loss_class: 0.1256 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:55:05] 	Iter 561100 Done. | loss1: 1.3221 | loss_class: 1.3215 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:55:56] 	Iter 561200 Done. | loss1: 0.0371 | loss_class: 0.0366 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:56:13] 	mean_loss1: 0.42698500717772675
[06.25.21|17:56:13] 	mean_loss_class: 0.42633832881227346
[06.25.21|17:56:13] 	mean_loss_recon: 0.0006466782433944497
[06.25.21|17:56:13] Time consumption:
[06.25.21|17:56:13] Done.
[06.25.21|17:56:13] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch65_model1.pt.
[06.25.21|17:56:13] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch65_model2.pt.
[06.25.21|17:56:13] Training epoch: 66
[06.25.21|17:56:49] 	Iter 561300 Done. | loss1: 0.8434 | loss_class: 0.8428 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:57:40] 	Iter 561400 Done. | loss1: 0.1417 | loss_class: 0.1410 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|17:58:31] 	Iter 561500 Done. | loss1: 0.4139 | loss_class: 0.4134 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|17:59:22] 	Iter 561600 Done. | loss1: 1.4195 | loss_class: 1.4190 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|18:00:13] 	Iter 561700 Done. | loss1: 0.7050 | loss_class: 0.7043 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:01:04] 	Iter 561800 Done. | loss1: 0.1181 | loss_class: 0.1175 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:01:55] 	Iter 561900 Done. | loss1: 0.0567 | loss_class: 0.0560 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:02:46] 	Iter 562000 Done. | loss1: 0.2677 | loss_class: 0.2671 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:03:37] 	Iter 562100 Done. | loss1: 0.0588 | loss_class: 0.0581 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:04:28] 	Iter 562200 Done. | loss1: 1.1479 | loss_class: 1.1473 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:05:19] 	Iter 562300 Done. | loss1: 0.0409 | loss_class: 0.0402 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:06:11] 	Iter 562400 Done. | loss1: 0.3628 | loss_class: 0.3623 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:07:02] 	Iter 562500 Done. | loss1: 0.8116 | loss_class: 0.8107 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|18:07:53] 	Iter 562600 Done. | loss1: 0.0035 | loss_class: 0.0030 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|18:08:44] 	Iter 562700 Done. | loss1: 0.1221 | loss_class: 0.1214 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:09:35] 	Iter 562800 Done. | loss1: 0.0179 | loss_class: 0.0173 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:10:26] 	Iter 562900 Done. | loss1: 0.2467 | loss_class: 0.2459 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|18:11:18] 	Iter 563000 Done. | loss1: 0.5390 | loss_class: 0.5382 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|18:12:09] 	Iter 563100 Done. | loss1: 0.1989 | loss_class: 0.1983 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:13:00] 	Iter 563200 Done. | loss1: 0.0611 | loss_class: 0.0605 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:13:51] 	Iter 563300 Done. | loss1: 0.3395 | loss_class: 0.3389 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:14:43] 	Iter 563400 Done. | loss1: 1.0031 | loss_class: 1.0027 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|18:15:34] 	Iter 563500 Done. | loss1: 0.5835 | loss_class: 0.5829 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:16:24] 	Iter 563600 Done. | loss1: 0.0806 | loss_class: 0.0799 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:17:16] 	Iter 563700 Done. | loss1: 0.0328 | loss_class: 0.0321 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:18:06] 	Iter 563800 Done. | loss1: 0.3947 | loss_class: 0.3940 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|18:18:58] 	Iter 563900 Done. | loss1: 0.1345 | loss_class: 0.1336 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|18:19:49] 	Iter 564000 Done. | loss1: 0.1315 | loss_class: 0.1304 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|18:20:41] 	Iter 564100 Done. | loss1: 0.3075 | loss_class: 0.3068 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:21:32] 	Iter 564200 Done. | loss1: 0.0130 | loss_class: 0.0124 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:22:23] 	Iter 564300 Done. | loss1: 0.0172 | loss_class: 0.0163 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|18:23:15] 	Iter 564400 Done. | loss1: 0.0684 | loss_class: 0.0677 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:24:06] 	Iter 564500 Done. | loss1: 0.1756 | loss_class: 0.1750 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:24:57] 	Iter 564600 Done. | loss1: 0.0692 | loss_class: 0.0687 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|18:25:48] 	Iter 564700 Done. | loss1: 0.0443 | loss_class: 0.0435 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:26:41] 	Iter 564800 Done. | loss1: 0.3935 | loss_class: 0.3927 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|18:27:32] 	Iter 564900 Done. | loss1: 1.3995 | loss_class: 1.3990 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:28:26] 	Iter 565000 Done. | loss1: 0.0039 | loss_class: 0.0033 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:29:17] 	Iter 565100 Done. | loss1: 0.1909 | loss_class: 0.1902 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:30:08] 	Iter 565200 Done. | loss1: 0.2838 | loss_class: 0.2829 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|18:31:00] 	Iter 565300 Done. | loss1: 0.2704 | loss_class: 0.2698 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:31:51] 	Iter 565400 Done. | loss1: 0.6585 | loss_class: 0.6579 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:32:42] 	Iter 565500 Done. | loss1: 0.0474 | loss_class: 0.0467 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:33:34] 	Iter 565600 Done. | loss1: 0.3102 | loss_class: 0.3093 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|18:34:25] 	Iter 565700 Done. | loss1: 0.8360 | loss_class: 0.8356 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|18:35:16] 	Iter 565800 Done. | loss1: 0.4487 | loss_class: 0.4482 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|18:36:07] 	Iter 565900 Done. | loss1: 0.1583 | loss_class: 0.1577 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|18:36:58] 	Iter 566000 Done. | loss1: 1.0986 | loss_class: 1.0980 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:37:49] 	Iter 566100 Done. | loss1: 0.6668 | loss_class: 0.6661 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:38:40] 	Iter 566200 Done. | loss1: 0.2091 | loss_class: 0.2084 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|18:39:31] 	Iter 566300 Done. | loss1: 1.0733 | loss_class: 1.0727 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:40:22] 	Iter 566400 Done. | loss1: 0.1863 | loss_class: 0.1857 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:41:13] 	Iter 566500 Done. | loss1: 0.0617 | loss_class: 0.0609 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|18:42:04] 	Iter 566600 Done. | loss1: 0.5360 | loss_class: 0.5355 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|18:42:54] 	Iter 566700 Done. | loss1: 0.1088 | loss_class: 0.1083 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:43:45] 	Iter 566800 Done. | loss1: 0.6555 | loss_class: 0.6548 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|18:44:37] 	Iter 566900 Done. | loss1: 0.0291 | loss_class: 0.0284 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:45:28] 	Iter 567000 Done. | loss1: 0.7183 | loss_class: 0.7177 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:46:19] 	Iter 567100 Done. | loss1: 0.5335 | loss_class: 0.5328 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:47:10] 	Iter 567200 Done. | loss1: 1.1724 | loss_class: 1.1718 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:48:01] 	Iter 567300 Done. | loss1: 0.0765 | loss_class: 0.0756 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|18:48:52] 	Iter 567400 Done. | loss1: 0.1621 | loss_class: 0.1617 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|18:49:43] 	Iter 567500 Done. | loss1: 0.9204 | loss_class: 0.9198 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:50:34] 	Iter 567600 Done. | loss1: 0.9855 | loss_class: 0.9849 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:51:25] 	Iter 567700 Done. | loss1: 0.1001 | loss_class: 0.0996 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|18:52:17] 	Iter 567800 Done. | loss1: 1.3455 | loss_class: 1.3446 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|18:53:08] 	Iter 567900 Done. | loss1: 0.0611 | loss_class: 0.0606 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:53:59] 	Iter 568000 Done. | loss1: 0.8146 | loss_class: 0.8142 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|18:54:49] 	Iter 568100 Done. | loss1: 0.2366 | loss_class: 0.2361 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|18:55:39] 	Iter 568200 Done. | loss1: 1.1010 | loss_class: 1.1004 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:56:30] 	Iter 568300 Done. | loss1: 0.0392 | loss_class: 0.0385 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:57:20] 	Iter 568400 Done. | loss1: 0.1602 | loss_class: 0.1594 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|18:58:10] 	Iter 568500 Done. | loss1: 0.2064 | loss_class: 0.2056 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|18:59:01] 	Iter 568600 Done. | loss1: 0.0106 | loss_class: 0.0099 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|18:59:51] 	Iter 568700 Done. | loss1: 1.7828 | loss_class: 1.7821 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|19:00:42] 	Iter 568800 Done. | loss1: 0.0977 | loss_class: 0.0973 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|19:01:33] 	Iter 568900 Done. | loss1: 0.0163 | loss_class: 0.0158 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|19:02:24] 	Iter 569000 Done. | loss1: 0.2508 | loss_class: 0.2501 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:03:15] 	Iter 569100 Done. | loss1: 0.0616 | loss_class: 0.0609 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:04:06] 	Iter 569200 Done. | loss1: 0.0159 | loss_class: 0.0152 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:04:56] 	Iter 569300 Done. | loss1: 2.3010 | loss_class: 2.3003 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:05:47] 	Iter 569400 Done. | loss1: 0.3624 | loss_class: 0.3616 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|19:06:38] 	Iter 569500 Done. | loss1: 0.8733 | loss_class: 0.8726 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:07:29] 	Iter 569600 Done. | loss1: 0.0576 | loss_class: 0.0569 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:08:20] 	Iter 569700 Done. | loss1: 0.2116 | loss_class: 0.2110 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:09:11] 	Iter 569800 Done. | loss1: 0.5902 | loss_class: 0.5896 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:10:02] 	Iter 569900 Done. | loss1: 0.0310 | loss_class: 0.0302 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|19:10:53] 	Iter 570000 Done. | loss1: 0.1531 | loss_class: 0.1525 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:11:44] 	Iter 570100 Done. | loss1: 1.9739 | loss_class: 1.9734 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|19:12:35] 	Iter 570200 Done. | loss1: 0.0868 | loss_class: 0.0861 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:13:26] 	Iter 570300 Done. | loss1: 0.0130 | loss_class: 0.0122 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|19:14:17] 	Iter 570400 Done. | loss1: 0.3218 | loss_class: 0.3211 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:15:09] 	Iter 570500 Done. | loss1: 0.2346 | loss_class: 0.2340 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|19:16:00] 	Iter 570600 Done. | loss1: 1.2054 | loss_class: 1.2047 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:16:51] 	Iter 570700 Done. | loss1: 1.1613 | loss_class: 1.1608 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|19:17:42] 	Iter 570800 Done. | loss1: 0.0501 | loss_class: 0.0492 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|19:18:33] 	Iter 570900 Done. | loss1: 0.1301 | loss_class: 0.1295 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:19:25] 	Iter 571000 Done. | loss1: 0.2027 | loss_class: 0.2021 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|19:20:16] 	Iter 571100 Done. | loss1: 0.0105 | loss_class: 0.0099 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:21:07] 	Iter 571200 Done. | loss1: 0.1721 | loss_class: 0.1710 | loss_recon: 0.0011 | lr: 0.010000
[06.25.21|19:21:35] 	mean_loss1: 0.4260865951649973
[06.25.21|19:21:35] 	mean_loss_class: 0.42544153996519796
[06.25.21|19:21:35] 	mean_loss_recon: 0.0006450551933536652
[06.25.21|19:21:35] Time consumption:
[06.25.21|19:21:35] Done.
[06.25.21|19:21:35] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch66_model1.pt.
[06.25.21|19:21:35] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch66_model2.pt.
[06.25.21|19:21:35] Training epoch: 67
[06.25.21|19:21:59] 	Iter 571300 Done. | loss1: 0.1084 | loss_class: 0.1077 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|19:22:51] 	Iter 571400 Done. | loss1: 0.3613 | loss_class: 0.3606 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:23:42] 	Iter 571500 Done. | loss1: 0.3802 | loss_class: 0.3795 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:24:34] 	Iter 571600 Done. | loss1: 0.0358 | loss_class: 0.0352 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:25:25] 	Iter 571700 Done. | loss1: 0.0829 | loss_class: 0.0823 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:26:16] 	Iter 571800 Done. | loss1: 0.0950 | loss_class: 0.0943 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:27:08] 	Iter 571900 Done. | loss1: 0.2859 | loss_class: 0.2851 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|19:27:59] 	Iter 572000 Done. | loss1: 0.0139 | loss_class: 0.0132 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:28:50] 	Iter 572100 Done. | loss1: 0.6252 | loss_class: 0.6247 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|19:29:42] 	Iter 572200 Done. | loss1: 0.5620 | loss_class: 0.5613 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:30:33] 	Iter 572300 Done. | loss1: 0.1652 | loss_class: 0.1644 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|19:31:24] 	Iter 572400 Done. | loss1: 0.6872 | loss_class: 0.6866 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:32:15] 	Iter 572500 Done. | loss1: 0.8661 | loss_class: 0.8656 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|19:33:07] 	Iter 572600 Done. | loss1: 0.1512 | loss_class: 0.1505 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:33:58] 	Iter 572700 Done. | loss1: 1.9287 | loss_class: 1.9281 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:34:49] 	Iter 572800 Done. | loss1: 0.0037 | loss_class: 0.0029 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|19:35:40] 	Iter 572900 Done. | loss1: 0.4619 | loss_class: 0.4613 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:36:32] 	Iter 573000 Done. | loss1: 0.4571 | loss_class: 0.4567 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|19:37:23] 	Iter 573100 Done. | loss1: 1.2950 | loss_class: 1.2945 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:38:14] 	Iter 573200 Done. | loss1: 0.4826 | loss_class: 0.4819 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:39:06] 	Iter 573300 Done. | loss1: 0.0109 | loss_class: 0.0103 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:39:58] 	Iter 573400 Done. | loss1: 0.1235 | loss_class: 0.1229 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:40:48] 	Iter 573500 Done. | loss1: 0.0239 | loss_class: 0.0232 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:41:39] 	Iter 573600 Done. | loss1: 0.8379 | loss_class: 0.8373 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:42:29] 	Iter 573700 Done. | loss1: 0.0944 | loss_class: 0.0938 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:43:20] 	Iter 573800 Done. | loss1: 0.6299 | loss_class: 0.6292 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:44:11] 	Iter 573900 Done. | loss1: 0.0828 | loss_class: 0.0823 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|19:45:05] 	Iter 574000 Done. | loss1: 0.0194 | loss_class: 0.0188 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:45:58] 	Iter 574100 Done. | loss1: 0.4740 | loss_class: 0.4735 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|19:46:50] 	Iter 574200 Done. | loss1: 0.5150 | loss_class: 0.5142 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|19:47:42] 	Iter 574300 Done. | loss1: 0.2317 | loss_class: 0.2312 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|19:48:35] 	Iter 574400 Done. | loss1: 0.4195 | loss_class: 0.4187 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|19:49:28] 	Iter 574500 Done. | loss1: 1.3952 | loss_class: 1.3948 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|19:50:20] 	Iter 574600 Done. | loss1: 0.0999 | loss_class: 0.0992 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:51:13] 	Iter 574700 Done. | loss1: 1.0485 | loss_class: 1.0480 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:52:06] 	Iter 574800 Done. | loss1: 0.6660 | loss_class: 0.6649 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|19:52:58] 	Iter 574900 Done. | loss1: 1.9289 | loss_class: 1.9284 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:53:49] 	Iter 575000 Done. | loss1: 0.0859 | loss_class: 0.0851 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|19:54:41] 	Iter 575100 Done. | loss1: 1.8388 | loss_class: 1.8383 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|19:55:33] 	Iter 575200 Done. | loss1: 0.0261 | loss_class: 0.0254 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|19:56:25] 	Iter 575300 Done. | loss1: 0.2844 | loss_class: 0.2838 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:57:17] 	Iter 575400 Done. | loss1: 0.0507 | loss_class: 0.0501 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:58:09] 	Iter 575500 Done. | loss1: 2.1338 | loss_class: 2.1334 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|19:59:01] 	Iter 575600 Done. | loss1: 0.5921 | loss_class: 0.5915 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|19:59:52] 	Iter 575700 Done. | loss1: 1.1815 | loss_class: 1.1808 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:00:44] 	Iter 575800 Done. | loss1: 0.0059 | loss_class: 0.0052 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:01:35] 	Iter 575900 Done. | loss1: 0.1979 | loss_class: 0.1973 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:02:27] 	Iter 576000 Done. | loss1: 0.0758 | loss_class: 0.0752 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:03:20] 	Iter 576100 Done. | loss1: 1.2280 | loss_class: 1.2275 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:04:12] 	Iter 576200 Done. | loss1: 0.7058 | loss_class: 0.7051 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:05:04] 	Iter 576300 Done. | loss1: 0.0984 | loss_class: 0.0978 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:05:56] 	Iter 576400 Done. | loss1: 0.9892 | loss_class: 0.9886 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:06:50] 	Iter 576500 Done. | loss1: 0.2460 | loss_class: 0.2455 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:07:44] 	Iter 576600 Done. | loss1: 0.6944 | loss_class: 0.6939 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:08:35] 	Iter 576700 Done. | loss1: 0.1566 | loss_class: 0.1557 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|20:09:27] 	Iter 576800 Done. | loss1: 0.7123 | loss_class: 0.7116 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:10:20] 	Iter 576900 Done. | loss1: 0.2847 | loss_class: 0.2839 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|20:11:14] 	Iter 577000 Done. | loss1: 0.8711 | loss_class: 0.8703 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|20:12:06] 	Iter 577100 Done. | loss1: 1.0447 | loss_class: 1.0441 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:12:57] 	Iter 577200 Done. | loss1: 0.2400 | loss_class: 0.2392 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|20:13:49] 	Iter 577300 Done. | loss1: 0.2010 | loss_class: 0.2005 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|20:14:41] 	Iter 577400 Done. | loss1: 0.0383 | loss_class: 0.0376 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:15:34] 	Iter 577500 Done. | loss1: 0.8246 | loss_class: 0.8240 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:16:25] 	Iter 577600 Done. | loss1: 0.1868 | loss_class: 0.1860 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|20:17:17] 	Iter 577700 Done. | loss1: 0.1562 | loss_class: 0.1556 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:18:08] 	Iter 577800 Done. | loss1: 0.0106 | loss_class: 0.0099 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:18:59] 	Iter 577900 Done. | loss1: 0.0629 | loss_class: 0.0622 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:19:50] 	Iter 578000 Done. | loss1: 1.0540 | loss_class: 1.0534 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:20:41] 	Iter 578100 Done. | loss1: 0.0173 | loss_class: 0.0168 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:21:33] 	Iter 578200 Done. | loss1: 0.4258 | loss_class: 0.4252 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:22:24] 	Iter 578300 Done. | loss1: 0.0506 | loss_class: 0.0500 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:23:17] 	Iter 578400 Done. | loss1: 0.0061 | loss_class: 0.0055 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:24:10] 	Iter 578500 Done. | loss1: 1.2428 | loss_class: 1.2420 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|20:25:04] 	Iter 578600 Done. | loss1: 0.3125 | loss_class: 0.3116 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|20:25:57] 	Iter 578700 Done. | loss1: 0.5187 | loss_class: 0.5182 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|20:26:51] 	Iter 578800 Done. | loss1: 0.1712 | loss_class: 0.1704 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:27:43] 	Iter 578900 Done. | loss1: 0.5106 | loss_class: 0.5100 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:28:34] 	Iter 579000 Done. | loss1: 0.6670 | loss_class: 0.6662 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|20:29:25] 	Iter 579100 Done. | loss1: 0.1171 | loss_class: 0.1163 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:30:17] 	Iter 579200 Done. | loss1: 0.9049 | loss_class: 0.9044 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|20:31:09] 	Iter 579300 Done. | loss1: 0.1041 | loss_class: 0.1035 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:32:01] 	Iter 579400 Done. | loss1: 1.6262 | loss_class: 1.6255 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:32:53] 	Iter 579500 Done. | loss1: 0.8282 | loss_class: 0.8275 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|20:33:44] 	Iter 579600 Done. | loss1: 0.4567 | loss_class: 0.4562 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|20:34:36] 	Iter 579700 Done. | loss1: 0.3864 | loss_class: 0.3855 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|20:35:28] 	Iter 579800 Done. | loss1: 0.0135 | loss_class: 0.0127 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|20:36:19] 	Iter 579900 Done. | loss1: 0.0200 | loss_class: 0.0192 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|20:37:10] 	Iter 580000 Done. | loss1: 0.0174 | loss_class: 0.0167 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:38:01] 	Iter 580100 Done. | loss1: 0.0682 | loss_class: 0.0676 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:38:52] 	Iter 580200 Done. | loss1: 0.4829 | loss_class: 0.4821 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|20:39:43] 	Iter 580300 Done. | loss1: 0.0680 | loss_class: 0.0675 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|20:40:37] 	Iter 580400 Done. | loss1: 0.9246 | loss_class: 0.9241 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|20:41:31] 	Iter 580500 Done. | loss1: 0.3189 | loss_class: 0.3182 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:42:22] 	Iter 580600 Done. | loss1: 0.0116 | loss_class: 0.0109 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:43:12] 	Iter 580700 Done. | loss1: 0.3653 | loss_class: 0.3647 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|20:44:03] 	Iter 580800 Done. | loss1: 0.0897 | loss_class: 0.0888 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|20:44:54] 	Iter 580900 Done. | loss1: 0.0394 | loss_class: 0.0387 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:45:46] 	Iter 581000 Done. | loss1: 1.0802 | loss_class: 1.0797 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|20:46:39] 	Iter 581100 Done. | loss1: 0.1316 | loss_class: 0.1311 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|20:47:30] 	Iter 581200 Done. | loss1: 0.3194 | loss_class: 0.3188 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:48:10] 	mean_loss1: 0.4191278598350276
[06.25.21|20:48:10] 	mean_loss_class: 0.41848006428426254
[06.25.21|20:48:10] 	mean_loss_recon: 0.0006477955241660275
[06.25.21|20:48:10] Time consumption:
[06.25.21|20:48:10] Done.
[06.25.21|20:48:10] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch67_model1.pt.
[06.25.21|20:48:10] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch67_model2.pt.
[06.25.21|20:48:10] Training epoch: 68
[06.25.21|20:48:23] 	Iter 581300 Done. | loss1: 0.5653 | loss_class: 0.5646 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:49:14] 	Iter 581400 Done. | loss1: 0.6350 | loss_class: 0.6345 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:50:04] 	Iter 581500 Done. | loss1: 0.2434 | loss_class: 0.2427 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:50:55] 	Iter 581600 Done. | loss1: 2.4965 | loss_class: 2.4958 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:51:45] 	Iter 581700 Done. | loss1: 0.7536 | loss_class: 0.7530 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:52:35] 	Iter 581800 Done. | loss1: 1.2795 | loss_class: 1.2790 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|20:53:26] 	Iter 581900 Done. | loss1: 0.0852 | loss_class: 0.0845 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:54:16] 	Iter 582000 Done. | loss1: 1.4314 | loss_class: 1.4307 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:55:08] 	Iter 582100 Done. | loss1: 0.3916 | loss_class: 0.3909 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:55:58] 	Iter 582200 Done. | loss1: 0.0156 | loss_class: 0.0149 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:56:54] 	Iter 582300 Done. | loss1: 0.7190 | loss_class: 0.7183 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|20:57:46] 	Iter 582400 Done. | loss1: 0.0133 | loss_class: 0.0125 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|20:58:37] 	Iter 582500 Done. | loss1: 0.5789 | loss_class: 0.5778 | loss_recon: 0.0011 | lr: 0.010000
[06.25.21|20:59:27] 	Iter 582600 Done. | loss1: 0.6785 | loss_class: 0.6780 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|21:00:18] 	Iter 582700 Done. | loss1: 0.0302 | loss_class: 0.0295 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:01:09] 	Iter 582800 Done. | loss1: 0.1608 | loss_class: 0.1600 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|21:02:03] 	Iter 582900 Done. | loss1: 0.0571 | loss_class: 0.0564 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:02:54] 	Iter 583000 Done. | loss1: 1.5841 | loss_class: 1.5835 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:03:46] 	Iter 583100 Done. | loss1: 0.0107 | loss_class: 0.0101 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:04:37] 	Iter 583200 Done. | loss1: 0.0705 | loss_class: 0.0699 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:05:29] 	Iter 583300 Done. | loss1: 0.2603 | loss_class: 0.2597 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:06:21] 	Iter 583400 Done. | loss1: 0.4117 | loss_class: 0.4111 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|21:07:14] 	Iter 583500 Done. | loss1: 0.0133 | loss_class: 0.0126 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:08:06] 	Iter 583600 Done. | loss1: 0.1641 | loss_class: 0.1635 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:08:57] 	Iter 583700 Done. | loss1: 0.1147 | loss_class: 0.1141 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|21:09:48] 	Iter 583800 Done. | loss1: 0.0019 | loss_class: 0.0013 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:10:43] 	Iter 583900 Done. | loss1: 1.1897 | loss_class: 1.1892 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|21:11:35] 	Iter 584000 Done. | loss1: 0.4289 | loss_class: 0.4283 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:12:31] 	Iter 584100 Done. | loss1: 0.2656 | loss_class: 0.2650 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:13:21] 	Iter 584200 Done. | loss1: 0.0742 | loss_class: 0.0736 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:14:12] 	Iter 584300 Done. | loss1: 0.1730 | loss_class: 0.1723 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:15:03] 	Iter 584400 Done. | loss1: 0.2068 | loss_class: 0.2061 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:15:53] 	Iter 584500 Done. | loss1: 0.2774 | loss_class: 0.2766 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|21:16:44] 	Iter 584600 Done. | loss1: 0.1578 | loss_class: 0.1572 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:17:35] 	Iter 584700 Done. | loss1: 1.3422 | loss_class: 1.3415 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:18:25] 	Iter 584800 Done. | loss1: 0.9007 | loss_class: 0.9001 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:19:16] 	Iter 584900 Done. | loss1: 0.4829 | loss_class: 0.4823 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:20:07] 	Iter 585000 Done. | loss1: 0.0652 | loss_class: 0.0645 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:20:58] 	Iter 585100 Done. | loss1: 0.0254 | loss_class: 0.0249 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|21:21:48] 	Iter 585200 Done. | loss1: 0.0253 | loss_class: 0.0245 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|21:22:39] 	Iter 585300 Done. | loss1: 0.4297 | loss_class: 0.4290 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:23:31] 	Iter 585400 Done. | loss1: 0.0606 | loss_class: 0.0600 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|21:24:22] 	Iter 585500 Done. | loss1: 0.0268 | loss_class: 0.0262 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:25:13] 	Iter 585600 Done. | loss1: 0.9386 | loss_class: 0.9378 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|21:26:06] 	Iter 585700 Done. | loss1: 0.0567 | loss_class: 0.0559 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:26:57] 	Iter 585800 Done. | loss1: 0.2257 | loss_class: 0.2250 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:27:50] 	Iter 585900 Done. | loss1: 0.7888 | loss_class: 0.7883 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|21:28:45] 	Iter 586000 Done. | loss1: 0.2936 | loss_class: 0.2930 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:29:36] 	Iter 586100 Done. | loss1: 0.0580 | loss_class: 0.0571 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|21:30:31] 	Iter 586200 Done. | loss1: 0.5082 | loss_class: 0.5077 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|21:31:21] 	Iter 586300 Done. | loss1: 0.1844 | loss_class: 0.1836 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|21:32:11] 	Iter 586400 Done. | loss1: 0.4376 | loss_class: 0.4369 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:33:02] 	Iter 586500 Done. | loss1: 0.6326 | loss_class: 0.6318 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|21:33:54] 	Iter 586600 Done. | loss1: 0.0842 | loss_class: 0.0837 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|21:34:45] 	Iter 586700 Done. | loss1: 0.4265 | loss_class: 0.4259 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:35:35] 	Iter 586800 Done. | loss1: 0.0078 | loss_class: 0.0072 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:36:26] 	Iter 586900 Done. | loss1: 0.0176 | loss_class: 0.0168 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|21:37:19] 	Iter 587000 Done. | loss1: 0.1641 | loss_class: 0.1634 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:38:14] 	Iter 587100 Done. | loss1: 0.2133 | loss_class: 0.2127 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:39:05] 	Iter 587200 Done. | loss1: 0.4181 | loss_class: 0.4176 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|21:39:55] 	Iter 587300 Done. | loss1: 0.9730 | loss_class: 0.9724 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:40:45] 	Iter 587400 Done. | loss1: 1.3626 | loss_class: 1.3621 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:41:37] 	Iter 587500 Done. | loss1: 0.5146 | loss_class: 0.5140 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:42:28] 	Iter 587600 Done. | loss1: 0.5348 | loss_class: 0.5343 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:43:19] 	Iter 587700 Done. | loss1: 0.0322 | loss_class: 0.0316 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:44:10] 	Iter 587800 Done. | loss1: 0.0301 | loss_class: 0.0295 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:45:01] 	Iter 587900 Done. | loss1: 0.0502 | loss_class: 0.0497 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|21:45:52] 	Iter 588000 Done. | loss1: 0.1049 | loss_class: 0.1042 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:46:42] 	Iter 588100 Done. | loss1: 0.1848 | loss_class: 0.1843 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:47:34] 	Iter 588200 Done. | loss1: 0.1025 | loss_class: 0.1018 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:48:25] 	Iter 588300 Done. | loss1: 0.0249 | loss_class: 0.0244 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|21:49:16] 	Iter 588400 Done. | loss1: 0.0370 | loss_class: 0.0364 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|21:50:07] 	Iter 588500 Done. | loss1: 0.0327 | loss_class: 0.0321 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:50:59] 	Iter 588600 Done. | loss1: 0.9359 | loss_class: 0.9353 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:51:51] 	Iter 588700 Done. | loss1: 0.0675 | loss_class: 0.0668 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:52:42] 	Iter 588800 Done. | loss1: 0.0095 | loss_class: 0.0087 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|21:53:36] 	Iter 588900 Done. | loss1: 0.6741 | loss_class: 0.6734 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:54:28] 	Iter 589000 Done. | loss1: 0.4682 | loss_class: 0.4676 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:55:18] 	Iter 589100 Done. | loss1: 0.2487 | loss_class: 0.2479 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|21:56:12] 	Iter 589200 Done. | loss1: 0.6337 | loss_class: 0.6330 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:57:06] 	Iter 589300 Done. | loss1: 0.0062 | loss_class: 0.0056 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:57:56] 	Iter 589400 Done. | loss1: 0.4205 | loss_class: 0.4199 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|21:58:47] 	Iter 589500 Done. | loss1: 0.9680 | loss_class: 0.9673 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|21:59:38] 	Iter 589600 Done. | loss1: 0.0987 | loss_class: 0.0981 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:00:28] 	Iter 589700 Done. | loss1: 0.1254 | loss_class: 0.1247 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:01:19] 	Iter 589800 Done. | loss1: 0.0519 | loss_class: 0.0513 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:02:09] 	Iter 589900 Done. | loss1: 0.1743 | loss_class: 0.1736 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:03:02] 	Iter 590000 Done. | loss1: 0.3639 | loss_class: 0.3631 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|22:03:57] 	Iter 590100 Done. | loss1: 0.6033 | loss_class: 0.6026 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:04:48] 	Iter 590200 Done. | loss1: 0.3487 | loss_class: 0.3480 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:05:39] 	Iter 590300 Done. | loss1: 2.2736 | loss_class: 2.2731 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:06:29] 	Iter 590400 Done. | loss1: 0.5095 | loss_class: 0.5090 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:07:19] 	Iter 590500 Done. | loss1: 0.4115 | loss_class: 0.4108 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:08:10] 	Iter 590600 Done. | loss1: 0.4101 | loss_class: 0.4094 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:09:02] 	Iter 590700 Done. | loss1: 2.2171 | loss_class: 2.2166 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:09:53] 	Iter 590800 Done. | loss1: 0.1228 | loss_class: 0.1221 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:10:44] 	Iter 590900 Done. | loss1: 0.2224 | loss_class: 0.2218 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:11:34] 	Iter 591000 Done. | loss1: 0.0530 | loss_class: 0.0524 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:12:24] 	Iter 591100 Done. | loss1: 0.3890 | loss_class: 0.3883 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:13:15] 	Iter 591200 Done. | loss1: 0.1849 | loss_class: 0.1843 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:14:04] 	mean_loss1: 0.4136622372347444
[06.25.21|22:14:04] 	mean_loss_class: 0.4130154111518268
[06.25.21|22:14:04] 	mean_loss_recon: 0.0006468259128573722
[06.25.21|22:14:04] Time consumption:
[06.25.21|22:14:04] Done.
[06.25.21|22:14:04] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch68_model1.pt.
[06.25.21|22:14:04] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch68_model2.pt.
[06.25.21|22:14:04] Training epoch: 69
[06.25.21|22:14:06] 	Iter 591300 Done. | loss1: 0.9450 | loss_class: 0.9443 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:14:57] 	Iter 591400 Done. | loss1: 0.0901 | loss_class: 0.0895 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:15:52] 	Iter 591500 Done. | loss1: 0.0490 | loss_class: 0.0482 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|22:16:46] 	Iter 591600 Done. | loss1: 0.1111 | loss_class: 0.1104 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:17:37] 	Iter 591700 Done. | loss1: 0.0009 | loss_class: 0.0004 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:18:27] 	Iter 591800 Done. | loss1: 0.0202 | loss_class: 0.0197 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:19:19] 	Iter 591900 Done. | loss1: 0.9390 | loss_class: 0.9383 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:20:09] 	Iter 592000 Done. | loss1: 0.0240 | loss_class: 0.0233 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:21:04] 	Iter 592100 Done. | loss1: 0.3813 | loss_class: 0.3807 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:21:55] 	Iter 592200 Done. | loss1: 1.0547 | loss_class: 1.0541 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:22:48] 	Iter 592300 Done. | loss1: 0.2494 | loss_class: 0.2487 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:23:43] 	Iter 592400 Done. | loss1: 0.0654 | loss_class: 0.0646 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|22:24:36] 	Iter 592500 Done. | loss1: 0.6696 | loss_class: 0.6689 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:25:26] 	Iter 592600 Done. | loss1: 0.1001 | loss_class: 0.0997 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|22:26:16] 	Iter 592700 Done. | loss1: 0.5294 | loss_class: 0.5288 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:27:06] 	Iter 592800 Done. | loss1: 0.2583 | loss_class: 0.2576 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:27:56] 	Iter 592900 Done. | loss1: 0.3930 | loss_class: 0.3923 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:28:47] 	Iter 593000 Done. | loss1: 0.0120 | loss_class: 0.0113 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:29:37] 	Iter 593100 Done. | loss1: 0.0581 | loss_class: 0.0575 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:30:28] 	Iter 593200 Done. | loss1: 0.0022 | loss_class: 0.0017 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:31:18] 	Iter 593300 Done. | loss1: 0.3655 | loss_class: 0.3649 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:32:09] 	Iter 593400 Done. | loss1: 0.0855 | loss_class: 0.0848 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:32:59] 	Iter 593500 Done. | loss1: 0.0654 | loss_class: 0.0647 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|22:33:50] 	Iter 593600 Done. | loss1: 0.6056 | loss_class: 0.6050 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:34:40] 	Iter 593700 Done. | loss1: 0.0121 | loss_class: 0.0115 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:35:31] 	Iter 593800 Done. | loss1: 0.1263 | loss_class: 0.1256 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:36:22] 	Iter 593900 Done. | loss1: 0.2394 | loss_class: 0.2387 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|22:37:12] 	Iter 594000 Done. | loss1: 0.3028 | loss_class: 0.3019 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|22:38:04] 	Iter 594100 Done. | loss1: 3.0654 | loss_class: 3.0648 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:38:55] 	Iter 594200 Done. | loss1: 0.1365 | loss_class: 0.1358 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:39:47] 	Iter 594300 Done. | loss1: 0.0392 | loss_class: 0.0387 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:40:38] 	Iter 594400 Done. | loss1: 0.3106 | loss_class: 0.3101 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:41:30] 	Iter 594500 Done. | loss1: 0.0200 | loss_class: 0.0190 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|22:42:21] 	Iter 594600 Done. | loss1: 0.6691 | loss_class: 0.6686 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:43:12] 	Iter 594700 Done. | loss1: 0.6753 | loss_class: 0.6747 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:44:08] 	Iter 594800 Done. | loss1: 0.0522 | loss_class: 0.0517 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:44:59] 	Iter 594900 Done. | loss1: 0.1874 | loss_class: 0.1868 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:45:50] 	Iter 595000 Done. | loss1: 0.6131 | loss_class: 0.6123 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|22:46:41] 	Iter 595100 Done. | loss1: 0.4790 | loss_class: 0.4782 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|22:47:33] 	Iter 595200 Done. | loss1: 0.0437 | loss_class: 0.0431 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:48:24] 	Iter 595300 Done. | loss1: 0.0286 | loss_class: 0.0278 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:49:14] 	Iter 595400 Done. | loss1: 0.3180 | loss_class: 0.3172 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|22:50:05] 	Iter 595500 Done. | loss1: 0.2450 | loss_class: 0.2443 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:50:55] 	Iter 595600 Done. | loss1: 0.0069 | loss_class: 0.0063 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:51:46] 	Iter 595700 Done. | loss1: 0.0895 | loss_class: 0.0887 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|22:52:37] 	Iter 595800 Done. | loss1: 1.7148 | loss_class: 1.7143 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:53:27] 	Iter 595900 Done. | loss1: 0.1414 | loss_class: 0.1408 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:54:18] 	Iter 596000 Done. | loss1: 0.0173 | loss_class: 0.0165 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|22:55:11] 	Iter 596100 Done. | loss1: 0.2748 | loss_class: 0.2744 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:56:07] 	Iter 596200 Done. | loss1: 0.2709 | loss_class: 0.2701 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|22:56:59] 	Iter 596300 Done. | loss1: 0.4888 | loss_class: 0.4882 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:57:49] 	Iter 596400 Done. | loss1: 0.3406 | loss_class: 0.3401 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|22:58:39] 	Iter 596500 Done. | loss1: 0.0318 | loss_class: 0.0312 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|22:59:29] 	Iter 596600 Done. | loss1: 1.0326 | loss_class: 1.0321 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:00:20] 	Iter 596700 Done. | loss1: 0.1498 | loss_class: 0.1492 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|23:01:10] 	Iter 596800 Done. | loss1: 0.0303 | loss_class: 0.0299 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|23:02:01] 	Iter 596900 Done. | loss1: 0.1816 | loss_class: 0.1810 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:02:53] 	Iter 597000 Done. | loss1: 0.2591 | loss_class: 0.2582 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|23:03:42] 	Iter 597100 Done. | loss1: 0.5371 | loss_class: 0.5364 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|23:04:35] 	Iter 597200 Done. | loss1: 0.1183 | loss_class: 0.1175 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|23:05:30] 	Iter 597300 Done. | loss1: 0.7679 | loss_class: 0.7672 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|23:06:26] 	Iter 597400 Done. | loss1: 1.0202 | loss_class: 1.0196 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:07:18] 	Iter 597500 Done. | loss1: 0.0964 | loss_class: 0.0958 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:08:08] 	Iter 597600 Done. | loss1: 0.8650 | loss_class: 0.8644 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|23:08:59] 	Iter 597700 Done. | loss1: 0.0407 | loss_class: 0.0398 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|23:09:49] 	Iter 597800 Done. | loss1: 0.2784 | loss_class: 0.2776 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|23:10:40] 	Iter 597900 Done. | loss1: 0.3540 | loss_class: 0.3531 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|23:11:31] 	Iter 598000 Done. | loss1: 0.0603 | loss_class: 0.0597 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:12:21] 	Iter 598100 Done. | loss1: 0.1412 | loss_class: 0.1405 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|23:13:12] 	Iter 598200 Done. | loss1: 0.0653 | loss_class: 0.0648 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|23:14:07] 	Iter 598300 Done. | loss1: 0.0034 | loss_class: 0.0026 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|23:15:02] 	Iter 598400 Done. | loss1: 1.0154 | loss_class: 1.0148 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|23:15:57] 	Iter 598500 Done. | loss1: 0.2615 | loss_class: 0.2610 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:16:49] 	Iter 598600 Done. | loss1: 0.8498 | loss_class: 0.8492 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|23:17:40] 	Iter 598700 Done. | loss1: 1.0007 | loss_class: 1.0000 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|23:18:31] 	Iter 598800 Done. | loss1: 0.3032 | loss_class: 0.3027 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|23:19:21] 	Iter 598900 Done. | loss1: 0.0144 | loss_class: 0.0135 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|23:20:12] 	Iter 599000 Done. | loss1: 0.1094 | loss_class: 0.1087 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|23:21:02] 	Iter 599100 Done. | loss1: 1.9819 | loss_class: 1.9813 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:21:53] 	Iter 599200 Done. | loss1: 0.0511 | loss_class: 0.0504 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|23:22:43] 	Iter 599300 Done. | loss1: 0.0970 | loss_class: 0.0965 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|23:23:34] 	Iter 599400 Done. | loss1: 0.9887 | loss_class: 0.9881 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:24:24] 	Iter 599500 Done. | loss1: 2.1904 | loss_class: 2.1898 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:25:15] 	Iter 599600 Done. | loss1: 0.5363 | loss_class: 0.5354 | loss_recon: 0.0009 | lr: 0.010000
[06.25.21|23:26:05] 	Iter 599700 Done. | loss1: 1.3174 | loss_class: 1.3164 | loss_recon: 0.0010 | lr: 0.010000
[06.25.21|23:26:56] 	Iter 599800 Done. | loss1: 0.3958 | loss_class: 0.3953 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:27:52] 	Iter 599900 Done. | loss1: 0.2286 | loss_class: 0.2279 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|23:28:47] 	Iter 600000 Done. | loss1: 0.0143 | loss_class: 0.0137 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|23:29:40] 	Iter 600100 Done. | loss1: 0.6066 | loss_class: 0.6062 | loss_recon: 0.0004 | lr: 0.010000
[06.25.21|23:30:31] 	Iter 600200 Done. | loss1: 0.0154 | loss_class: 0.0148 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:31:21] 	Iter 600300 Done. | loss1: 0.7758 | loss_class: 0.7751 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|23:32:11] 	Iter 600400 Done. | loss1: 0.5564 | loss_class: 0.5557 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:33:02] 	Iter 600500 Done. | loss1: 0.4002 | loss_class: 0.3995 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:33:52] 	Iter 600600 Done. | loss1: 0.8237 | loss_class: 0.8232 | loss_recon: 0.0005 | lr: 0.010000
[06.25.21|23:34:43] 	Iter 600700 Done. | loss1: 0.2098 | loss_class: 0.2092 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:35:33] 	Iter 600800 Done. | loss1: 0.1688 | loss_class: 0.1682 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:36:24] 	Iter 600900 Done. | loss1: 0.0389 | loss_class: 0.0382 | loss_recon: 0.0007 | lr: 0.010000
[06.25.21|23:37:15] 	Iter 601000 Done. | loss1: 0.3063 | loss_class: 0.3056 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|23:38:05] 	Iter 601100 Done. | loss1: 0.0173 | loss_class: 0.0166 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|23:38:55] 	Iter 601200 Done. | loss1: 0.0988 | loss_class: 0.0980 | loss_recon: 0.0008 | lr: 0.010000
[06.25.21|23:39:46] 	Iter 601300 Done. | loss1: 0.1153 | loss_class: 0.1148 | loss_recon: 0.0006 | lr: 0.010000
[06.25.21|23:39:56] 	mean_loss1: 0.41073745325696465
[06.25.21|23:39:56] 	mean_loss_class: 0.4100917996879577
[06.25.21|23:39:56] 	mean_loss_recon: 0.0006456537542348373
[06.25.21|23:39:56] Time consumption:
[06.25.21|23:39:56] Done.
[06.25.21|23:39:56] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch69_model1.pt.
[06.25.21|23:39:56] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch69_model2.pt.
[06.25.21|23:39:56] Eval epoch: 69
[06.25.21|23:46:26] 	mean_loss1: 0.7578024701468011
[06.25.21|23:46:26] 	mean_loss_class: 0.7574456396346637
[06.25.21|23:46:26] 	mean_loss_recon: 0.03568300625035005
[06.25.21|23:46:26] 

[06.25.21|23:46:26] 	Top1: 79.41%
[06.25.21|23:46:26] 

[06.25.21|23:46:26] 	Top5: 96.06%
[06.25.21|23:46:26] Done.
[06.25.21|23:46:26] Training epoch: 70
[06.25.21|23:47:08] 	Iter 601400 Done. | loss1: 0.7347 | loss_class: 0.7339 | loss_recon: 0.0007 | lr: 0.001000
[06.25.21|23:47:58] 	Iter 601500 Done. | loss1: 0.8601 | loss_class: 0.8596 | loss_recon: 0.0006 | lr: 0.001000
[06.25.21|23:48:49] 	Iter 601600 Done. | loss1: 0.0280 | loss_class: 0.0274 | loss_recon: 0.0006 | lr: 0.001000
[06.25.21|23:49:40] 	Iter 601700 Done. | loss1: 0.0241 | loss_class: 0.0233 | loss_recon: 0.0009 | lr: 0.001000
[06.25.21|23:50:30] 	Iter 601800 Done. | loss1: 0.0452 | loss_class: 0.0446 | loss_recon: 0.0005 | lr: 0.001000
[06.25.21|23:51:20] 	Iter 601900 Done. | loss1: 0.0691 | loss_class: 0.0684 | loss_recon: 0.0006 | lr: 0.001000
[06.25.21|23:52:10] 	Iter 602000 Done. | loss1: 0.0032 | loss_class: 0.0024 | loss_recon: 0.0008 | lr: 0.001000
[06.25.21|23:53:00] 	Iter 602100 Done. | loss1: 0.2018 | loss_class: 0.2010 | loss_recon: 0.0007 | lr: 0.001000
[06.25.21|23:53:51] 	Iter 602200 Done. | loss1: 0.0911 | loss_class: 0.0901 | loss_recon: 0.0010 | lr: 0.001000
[06.25.21|23:54:40] 	Iter 602300 Done. | loss1: 1.1935 | loss_class: 1.1928 | loss_recon: 0.0006 | lr: 0.001000
[06.25.21|23:55:31] 	Iter 602400 Done. | loss1: 0.2397 | loss_class: 0.2391 | loss_recon: 0.0005 | lr: 0.001000
[06.25.21|23:56:21] 	Iter 602500 Done. | loss1: 0.0836 | loss_class: 0.0829 | loss_recon: 0.0007 | lr: 0.001000
[06.25.21|23:57:12] 	Iter 602600 Done. | loss1: 0.0394 | loss_class: 0.0387 | loss_recon: 0.0006 | lr: 0.001000
[06.25.21|23:58:02] 	Iter 602700 Done. | loss1: 0.2564 | loss_class: 0.2557 | loss_recon: 0.0007 | lr: 0.001000
[06.25.21|23:58:52] 	Iter 602800 Done. | loss1: 0.0218 | loss_class: 0.0210 | loss_recon: 0.0008 | lr: 0.001000
[06.25.21|23:59:43] 	Iter 602900 Done. | loss1: 0.1238 | loss_class: 0.1232 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:00:35] 	Iter 603000 Done. | loss1: 0.1590 | loss_class: 0.1585 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:01:25] 	Iter 603100 Done. | loss1: 0.0291 | loss_class: 0.0285 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:02:16] 	Iter 603200 Done. | loss1: 0.2646 | loss_class: 0.2641 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:03:07] 	Iter 603300 Done. | loss1: 0.9795 | loss_class: 0.9789 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:03:59] 	Iter 603400 Done. | loss1: 0.0748 | loss_class: 0.0742 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:04:49] 	Iter 603500 Done. | loss1: 0.0766 | loss_class: 0.0760 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:05:40] 	Iter 603600 Done. | loss1: 0.6996 | loss_class: 0.6989 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:06:31] 	Iter 603700 Done. | loss1: 0.2849 | loss_class: 0.2843 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:07:21] 	Iter 603800 Done. | loss1: 0.0196 | loss_class: 0.0191 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|00:08:11] 	Iter 603900 Done. | loss1: 0.0960 | loss_class: 0.0952 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|00:09:02] 	Iter 604000 Done. | loss1: 0.1079 | loss_class: 0.1072 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:09:53] 	Iter 604100 Done. | loss1: 0.0280 | loss_class: 0.0275 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:10:44] 	Iter 604200 Done. | loss1: 0.7140 | loss_class: 0.7132 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|00:11:36] 	Iter 604300 Done. | loss1: 0.1029 | loss_class: 0.1021 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|00:12:26] 	Iter 604400 Done. | loss1: 0.4617 | loss_class: 0.4611 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:13:17] 	Iter 604500 Done. | loss1: 0.0408 | loss_class: 0.0403 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:14:08] 	Iter 604600 Done. | loss1: 0.5638 | loss_class: 0.5632 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:15:00] 	Iter 604700 Done. | loss1: 0.2210 | loss_class: 0.2205 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:15:51] 	Iter 604800 Done. | loss1: 0.2745 | loss_class: 0.2739 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:16:42] 	Iter 604900 Done. | loss1: 0.0193 | loss_class: 0.0185 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|00:17:33] 	Iter 605000 Done. | loss1: 0.0257 | loss_class: 0.0250 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:18:24] 	Iter 605100 Done. | loss1: 0.2031 | loss_class: 0.2024 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:19:14] 	Iter 605200 Done. | loss1: 0.0207 | loss_class: 0.0199 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|00:20:05] 	Iter 605300 Done. | loss1: 0.0494 | loss_class: 0.0487 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:20:55] 	Iter 605400 Done. | loss1: 1.0124 | loss_class: 1.0119 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:21:45] 	Iter 605500 Done. | loss1: 0.1247 | loss_class: 0.1241 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:22:36] 	Iter 605600 Done. | loss1: 0.0159 | loss_class: 0.0151 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|00:23:29] 	Iter 605700 Done. | loss1: 0.0182 | loss_class: 0.0174 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|00:24:21] 	Iter 605800 Done. | loss1: 0.0267 | loss_class: 0.0261 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:25:11] 	Iter 605900 Done. | loss1: 0.5370 | loss_class: 0.5362 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|00:26:02] 	Iter 606000 Done. | loss1: 0.1010 | loss_class: 0.1004 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:26:53] 	Iter 606100 Done. | loss1: 0.0403 | loss_class: 0.0398 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:27:44] 	Iter 606200 Done. | loss1: 0.6006 | loss_class: 0.5998 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:28:34] 	Iter 606300 Done. | loss1: 0.9023 | loss_class: 0.9016 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:29:26] 	Iter 606400 Done. | loss1: 0.0547 | loss_class: 0.0540 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:30:21] 	Iter 606500 Done. | loss1: 0.2025 | loss_class: 0.2021 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:31:11] 	Iter 606600 Done. | loss1: 0.0459 | loss_class: 0.0451 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:32:02] 	Iter 606700 Done. | loss1: 0.0088 | loss_class: 0.0082 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:32:52] 	Iter 606800 Done. | loss1: 0.0647 | loss_class: 0.0641 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:33:45] 	Iter 606900 Done. | loss1: 0.0137 | loss_class: 0.0129 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|00:34:41] 	Iter 607000 Done. | loss1: 0.0272 | loss_class: 0.0266 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:35:31] 	Iter 607100 Done. | loss1: 0.8248 | loss_class: 0.8241 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:36:22] 	Iter 607200 Done. | loss1: 0.0017 | loss_class: 0.0011 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:37:12] 	Iter 607300 Done. | loss1: 0.0032 | loss_class: 0.0026 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:38:03] 	Iter 607400 Done. | loss1: 0.0815 | loss_class: 0.0807 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|00:38:54] 	Iter 607500 Done. | loss1: 0.0073 | loss_class: 0.0067 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:39:44] 	Iter 607600 Done. | loss1: 0.0079 | loss_class: 0.0072 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:40:35] 	Iter 607700 Done. | loss1: 0.0115 | loss_class: 0.0110 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:41:26] 	Iter 607800 Done. | loss1: 0.0720 | loss_class: 0.0713 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:42:16] 	Iter 607900 Done. | loss1: 0.0568 | loss_class: 0.0561 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:43:07] 	Iter 608000 Done. | loss1: 0.0981 | loss_class: 0.0974 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:44:00] 	Iter 608100 Done. | loss1: 0.0635 | loss_class: 0.0629 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:44:55] 	Iter 608200 Done. | loss1: 1.9104 | loss_class: 1.9098 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:45:46] 	Iter 608300 Done. | loss1: 0.0713 | loss_class: 0.0708 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:46:36] 	Iter 608400 Done. | loss1: 0.1925 | loss_class: 0.1919 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:47:27] 	Iter 608500 Done. | loss1: 0.0030 | loss_class: 0.0024 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:48:17] 	Iter 608600 Done. | loss1: 0.0199 | loss_class: 0.0192 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:49:07] 	Iter 608700 Done. | loss1: 0.1434 | loss_class: 0.1426 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|00:49:58] 	Iter 608800 Done. | loss1: 0.0392 | loss_class: 0.0386 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:50:48] 	Iter 608900 Done. | loss1: 0.0168 | loss_class: 0.0160 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|00:51:39] 	Iter 609000 Done. | loss1: 0.0021 | loss_class: 0.0016 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:52:30] 	Iter 609100 Done. | loss1: 0.0212 | loss_class: 0.0206 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|00:53:21] 	Iter 609200 Done. | loss1: 0.0252 | loss_class: 0.0245 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:54:11] 	Iter 609300 Done. | loss1: 0.3449 | loss_class: 0.3441 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:55:01] 	Iter 609400 Done. | loss1: 0.3476 | loss_class: 0.3472 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:55:52] 	Iter 609500 Done. | loss1: 0.0487 | loss_class: 0.0479 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|00:56:42] 	Iter 609600 Done. | loss1: 0.1251 | loss_class: 0.1244 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|00:57:33] 	Iter 609700 Done. | loss1: 0.0470 | loss_class: 0.0462 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|00:58:24] 	Iter 609800 Done. | loss1: 0.0130 | loss_class: 0.0125 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|00:59:15] 	Iter 609900 Done. | loss1: 0.0552 | loss_class: 0.0545 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:00:05] 	Iter 610000 Done. | loss1: 0.0029 | loss_class: 0.0023 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:00:56] 	Iter 610100 Done. | loss1: 0.2764 | loss_class: 0.2757 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:01:46] 	Iter 610200 Done. | loss1: 1.4241 | loss_class: 1.4236 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|01:02:37] 	Iter 610300 Done. | loss1: 0.4746 | loss_class: 0.4737 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|01:03:27] 	Iter 610400 Done. | loss1: 0.0070 | loss_class: 0.0064 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:04:19] 	Iter 610500 Done. | loss1: 0.0665 | loss_class: 0.0659 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|01:05:10] 	Iter 610600 Done. | loss1: 0.0244 | loss_class: 0.0237 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:06:02] 	Iter 610700 Done. | loss1: 0.0558 | loss_class: 0.0551 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:06:57] 	Iter 610800 Done. | loss1: 0.1766 | loss_class: 0.1757 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|01:07:50] 	Iter 610900 Done. | loss1: 0.0175 | loss_class: 0.0169 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:08:41] 	Iter 611000 Done. | loss1: 0.0100 | loss_class: 0.0094 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:09:32] 	Iter 611100 Done. | loss1: 0.0537 | loss_class: 0.0529 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|01:10:22] 	Iter 611200 Done. | loss1: 0.0061 | loss_class: 0.0055 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:11:13] 	Iter 611300 Done. | loss1: 0.1625 | loss_class: 0.1619 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:11:34] 	mean_loss1: 0.22770292987033788
[06.26.21|01:11:34] 	mean_loss_class: 0.22705639427407776
[06.26.21|01:11:34] 	mean_loss_recon: 0.000646535572875607
[06.26.21|01:11:34] Time consumption:
[06.26.21|01:11:34] Done.
[06.26.21|01:11:34] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch70_model1.pt.
[06.26.21|01:11:34] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch70_model2.pt.
[06.26.21|01:11:34] Training epoch: 71
[06.26.21|01:12:04] 	Iter 611400 Done. | loss1: 0.0264 | loss_class: 0.0258 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:12:55] 	Iter 611500 Done. | loss1: 0.0032 | loss_class: 0.0026 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:13:46] 	Iter 611600 Done. | loss1: 0.0084 | loss_class: 0.0077 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:14:36] 	Iter 611700 Done. | loss1: 0.0748 | loss_class: 0.0740 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:15:28] 	Iter 611800 Done. | loss1: 0.1757 | loss_class: 0.1750 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:16:22] 	Iter 611900 Done. | loss1: 0.0016 | loss_class: 0.0008 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|01:17:14] 	Iter 612000 Done. | loss1: 0.2456 | loss_class: 0.2451 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:18:08] 	Iter 612100 Done. | loss1: 0.0008 | loss_class: 0.0003 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|01:18:58] 	Iter 612200 Done. | loss1: 0.0879 | loss_class: 0.0873 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:19:49] 	Iter 612300 Done. | loss1: 0.0333 | loss_class: 0.0326 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:20:40] 	Iter 612400 Done. | loss1: 0.6946 | loss_class: 0.6941 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|01:21:31] 	Iter 612500 Done. | loss1: 0.0200 | loss_class: 0.0195 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:22:21] 	Iter 612600 Done. | loss1: 0.3183 | loss_class: 0.3176 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:23:11] 	Iter 612700 Done. | loss1: 0.3626 | loss_class: 0.3621 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|01:24:01] 	Iter 612800 Done. | loss1: 0.0400 | loss_class: 0.0396 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|01:24:52] 	Iter 612900 Done. | loss1: 0.0747 | loss_class: 0.0740 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:25:42] 	Iter 613000 Done. | loss1: 0.0145 | loss_class: 0.0138 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|01:26:33] 	Iter 613100 Done. | loss1: 0.0948 | loss_class: 0.0942 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|01:27:24] 	Iter 613200 Done. | loss1: 0.0508 | loss_class: 0.0499 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|01:28:19] 	Iter 613300 Done. | loss1: 0.2375 | loss_class: 0.2370 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|01:29:15] 	Iter 613400 Done. | loss1: 0.0218 | loss_class: 0.0211 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:30:08] 	Iter 613500 Done. | loss1: 0.0730 | loss_class: 0.0723 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:30:59] 	Iter 613600 Done. | loss1: 0.0209 | loss_class: 0.0203 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|01:31:50] 	Iter 613700 Done. | loss1: 0.0157 | loss_class: 0.0149 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|01:32:41] 	Iter 613800 Done. | loss1: 0.0202 | loss_class: 0.0195 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:33:31] 	Iter 613900 Done. | loss1: 0.1158 | loss_class: 0.1151 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:34:21] 	Iter 614000 Done. | loss1: 0.0707 | loss_class: 0.0701 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:35:12] 	Iter 614100 Done. | loss1: 0.2086 | loss_class: 0.2074 | loss_recon: 0.0012 | lr: 0.001000
[06.26.21|01:36:02] 	Iter 614200 Done. | loss1: 0.0093 | loss_class: 0.0085 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|01:36:54] 	Iter 614300 Done. | loss1: 0.1423 | loss_class: 0.1417 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:37:46] 	Iter 614400 Done. | loss1: 1.2809 | loss_class: 1.2802 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:38:37] 	Iter 614500 Done. | loss1: 0.0560 | loss_class: 0.0555 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|01:39:32] 	Iter 614600 Done. | loss1: 0.0024 | loss_class: 0.0018 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:40:27] 	Iter 614700 Done. | loss1: 0.0058 | loss_class: 0.0053 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:41:17] 	Iter 614800 Done. | loss1: 0.1024 | loss_class: 0.1017 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:42:08] 	Iter 614900 Done. | loss1: 0.0322 | loss_class: 0.0315 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:42:59] 	Iter 615000 Done. | loss1: 0.0914 | loss_class: 0.0910 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|01:43:49] 	Iter 615100 Done. | loss1: 0.5446 | loss_class: 0.5440 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:44:39] 	Iter 615200 Done. | loss1: 0.0363 | loss_class: 0.0351 | loss_recon: 0.0011 | lr: 0.001000
[06.26.21|01:45:30] 	Iter 615300 Done. | loss1: 0.0120 | loss_class: 0.0114 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:46:21] 	Iter 615400 Done. | loss1: 0.0813 | loss_class: 0.0806 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|01:47:12] 	Iter 615500 Done. | loss1: 0.0080 | loss_class: 0.0070 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|01:48:03] 	Iter 615600 Done. | loss1: 0.0081 | loss_class: 0.0075 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:48:53] 	Iter 615700 Done. | loss1: 0.1404 | loss_class: 0.1399 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:49:44] 	Iter 615800 Done. | loss1: 0.0054 | loss_class: 0.0047 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:50:35] 	Iter 615900 Done. | loss1: 0.2171 | loss_class: 0.2165 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:51:26] 	Iter 616000 Done. | loss1: 0.0164 | loss_class: 0.0158 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:52:16] 	Iter 616100 Done. | loss1: 0.4788 | loss_class: 0.4783 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|01:53:07] 	Iter 616200 Done. | loss1: 0.2218 | loss_class: 0.2212 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:53:58] 	Iter 616300 Done. | loss1: 0.1949 | loss_class: 0.1943 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:54:53] 	Iter 616400 Done. | loss1: 0.4196 | loss_class: 0.4188 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|01:55:48] 	Iter 616500 Done. | loss1: 0.7047 | loss_class: 0.7041 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:56:39] 	Iter 616600 Done. | loss1: 0.1456 | loss_class: 0.1449 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|01:57:30] 	Iter 616700 Done. | loss1: 0.0226 | loss_class: 0.0220 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:58:21] 	Iter 616800 Done. | loss1: 0.0672 | loss_class: 0.0667 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|01:59:11] 	Iter 616900 Done. | loss1: 0.0260 | loss_class: 0.0254 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:00:02] 	Iter 617000 Done. | loss1: 0.0382 | loss_class: 0.0377 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:00:53] 	Iter 617100 Done. | loss1: 0.0143 | loss_class: 0.0138 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:01:44] 	Iter 617200 Done. | loss1: 0.0966 | loss_class: 0.0960 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:02:36] 	Iter 617300 Done. | loss1: 1.7443 | loss_class: 1.7437 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|02:03:27] 	Iter 617400 Done. | loss1: 0.0406 | loss_class: 0.0399 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:04:18] 	Iter 617500 Done. | loss1: 1.3926 | loss_class: 1.3918 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:05:09] 	Iter 617600 Done. | loss1: 0.0099 | loss_class: 0.0093 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:06:00] 	Iter 617700 Done. | loss1: 0.0647 | loss_class: 0.0641 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:06:50] 	Iter 617800 Done. | loss1: 0.2224 | loss_class: 0.2218 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:07:41] 	Iter 617900 Done. | loss1: 0.2109 | loss_class: 0.2103 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:08:32] 	Iter 618000 Done. | loss1: 0.0295 | loss_class: 0.0288 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|02:09:22] 	Iter 618100 Done. | loss1: 0.1688 | loss_class: 0.1681 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:10:13] 	Iter 618200 Done. | loss1: 0.2834 | loss_class: 0.2826 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:11:05] 	Iter 618300 Done. | loss1: 1.7475 | loss_class: 1.7467 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:11:57] 	Iter 618400 Done. | loss1: 0.2101 | loss_class: 0.2095 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:12:47] 	Iter 618500 Done. | loss1: 0.4729 | loss_class: 0.4724 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:13:37] 	Iter 618600 Done. | loss1: 0.5944 | loss_class: 0.5937 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|02:14:27] 	Iter 618700 Done. | loss1: 0.7191 | loss_class: 0.7185 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:15:18] 	Iter 618800 Done. | loss1: 0.2109 | loss_class: 0.2102 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:16:09] 	Iter 618900 Done. | loss1: 0.0085 | loss_class: 0.0079 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:16:59] 	Iter 619000 Done. | loss1: 0.0030 | loss_class: 0.0023 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:17:49] 	Iter 619100 Done. | loss1: 0.0077 | loss_class: 0.0069 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:18:40] 	Iter 619200 Done. | loss1: 0.5524 | loss_class: 0.5516 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|02:19:31] 	Iter 619300 Done. | loss1: 0.0585 | loss_class: 0.0580 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:20:21] 	Iter 619400 Done. | loss1: 0.1865 | loss_class: 0.1857 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|02:21:12] 	Iter 619500 Done. | loss1: 0.0045 | loss_class: 0.0038 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|02:22:03] 	Iter 619600 Done. | loss1: 0.0097 | loss_class: 0.0090 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|02:22:54] 	Iter 619700 Done. | loss1: 0.9452 | loss_class: 0.9447 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:23:45] 	Iter 619800 Done. | loss1: 0.2822 | loss_class: 0.2816 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:24:35] 	Iter 619900 Done. | loss1: 0.0105 | loss_class: 0.0097 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:25:26] 	Iter 620000 Done. | loss1: 0.0557 | loss_class: 0.0552 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:26:16] 	Iter 620100 Done. | loss1: 0.0385 | loss_class: 0.0379 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|02:27:07] 	Iter 620200 Done. | loss1: 0.1303 | loss_class: 0.1298 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:27:58] 	Iter 620300 Done. | loss1: 1.0800 | loss_class: 1.0794 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:28:48] 	Iter 620400 Done. | loss1: 0.0782 | loss_class: 0.0776 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:29:40] 	Iter 620500 Done. | loss1: 0.1880 | loss_class: 0.1873 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:30:34] 	Iter 620600 Done. | loss1: 0.0512 | loss_class: 0.0507 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:31:25] 	Iter 620700 Done. | loss1: 0.0418 | loss_class: 0.0411 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:32:16] 	Iter 620800 Done. | loss1: 0.2997 | loss_class: 0.2991 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:33:06] 	Iter 620900 Done. | loss1: 0.0312 | loss_class: 0.0306 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:33:57] 	Iter 621000 Done. | loss1: 1.8395 | loss_class: 1.8390 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:34:47] 	Iter 621100 Done. | loss1: 0.0170 | loss_class: 0.0164 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:35:37] 	Iter 621200 Done. | loss1: 0.0302 | loss_class: 0.0296 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:36:27] 	Iter 621300 Done. | loss1: 0.3129 | loss_class: 0.3123 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:36:59] 	mean_loss1: 0.18430348991262274
[06.26.21|02:36:59] 	mean_loss_class: 0.18365653589780057
[06.26.21|02:36:59] 	mean_loss_recon: 0.0006469540788997072
[06.26.21|02:36:59] Time consumption:
[06.26.21|02:36:59] Done.
[06.26.21|02:36:59] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch71_model1.pt.
[06.26.21|02:36:59] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch71_model2.pt.
[06.26.21|02:36:59] Training epoch: 72
[06.26.21|02:37:18] 	Iter 621400 Done. | loss1: 0.3122 | loss_class: 0.3116 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:38:12] 	Iter 621500 Done. | loss1: 0.1396 | loss_class: 0.1389 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:39:07] 	Iter 621600 Done. | loss1: 0.0017 | loss_class: 0.0011 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:40:03] 	Iter 621700 Done. | loss1: 0.0836 | loss_class: 0.0831 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:40:55] 	Iter 621800 Done. | loss1: 0.0033 | loss_class: 0.0024 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|02:41:46] 	Iter 621900 Done. | loss1: 0.0500 | loss_class: 0.0491 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|02:42:36] 	Iter 622000 Done. | loss1: 0.6612 | loss_class: 0.6604 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:43:27] 	Iter 622100 Done. | loss1: 0.0366 | loss_class: 0.0360 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:44:17] 	Iter 622200 Done. | loss1: 0.0046 | loss_class: 0.0039 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:45:08] 	Iter 622300 Done. | loss1: 0.1325 | loss_class: 0.1320 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:45:59] 	Iter 622400 Done. | loss1: 0.0016 | loss_class: 0.0008 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|02:46:50] 	Iter 622500 Done. | loss1: 0.2142 | loss_class: 0.2137 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:47:41] 	Iter 622600 Done. | loss1: 0.0265 | loss_class: 0.0257 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:48:32] 	Iter 622700 Done. | loss1: 0.0020 | loss_class: 0.0013 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:49:23] 	Iter 622800 Done. | loss1: 0.0661 | loss_class: 0.0653 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:50:14] 	Iter 622900 Done. | loss1: 0.5271 | loss_class: 0.5263 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|02:51:05] 	Iter 623000 Done. | loss1: 0.2083 | loss_class: 0.2077 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:51:57] 	Iter 623100 Done. | loss1: 0.1359 | loss_class: 0.1352 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|02:52:47] 	Iter 623200 Done. | loss1: 0.0061 | loss_class: 0.0056 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:53:38] 	Iter 623300 Done. | loss1: 2.0320 | loss_class: 2.0313 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:54:29] 	Iter 623400 Done. | loss1: 0.7382 | loss_class: 0.7376 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|02:55:20] 	Iter 623500 Done. | loss1: 0.0777 | loss_class: 0.0769 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|02:56:11] 	Iter 623600 Done. | loss1: 0.0166 | loss_class: 0.0160 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|02:57:02] 	Iter 623700 Done. | loss1: 0.3021 | loss_class: 0.3014 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|02:57:53] 	Iter 623800 Done. | loss1: 0.0943 | loss_class: 0.0938 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|02:58:45] 	Iter 623900 Done. | loss1: 0.1697 | loss_class: 0.1693 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|02:59:36] 	Iter 624000 Done. | loss1: 0.0131 | loss_class: 0.0125 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:00:30] 	Iter 624100 Done. | loss1: 0.0063 | loss_class: 0.0058 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:01:20] 	Iter 624200 Done. | loss1: 0.3179 | loss_class: 0.3174 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:02:12] 	Iter 624300 Done. | loss1: 0.0014 | loss_class: 0.0008 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:03:03] 	Iter 624400 Done. | loss1: 0.0332 | loss_class: 0.0326 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:03:56] 	Iter 624500 Done. | loss1: 0.2166 | loss_class: 0.2161 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:04:49] 	Iter 624600 Done. | loss1: 0.0575 | loss_class: 0.0568 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:05:40] 	Iter 624700 Done. | loss1: 0.1740 | loss_class: 0.1733 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:06:35] 	Iter 624800 Done. | loss1: 0.0182 | loss_class: 0.0177 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:07:31] 	Iter 624900 Done. | loss1: 0.0361 | loss_class: 0.0355 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:08:26] 	Iter 625000 Done. | loss1: 0.0792 | loss_class: 0.0784 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|03:09:16] 	Iter 625100 Done. | loss1: 0.0193 | loss_class: 0.0188 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:10:07] 	Iter 625200 Done. | loss1: 0.1189 | loss_class: 0.1184 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:10:58] 	Iter 625300 Done. | loss1: 0.0138 | loss_class: 0.0131 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:11:48] 	Iter 625400 Done. | loss1: 0.0672 | loss_class: 0.0666 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:12:39] 	Iter 625500 Done. | loss1: 0.0350 | loss_class: 0.0344 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:13:29] 	Iter 625600 Done. | loss1: 0.1002 | loss_class: 0.0996 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:14:20] 	Iter 625700 Done. | loss1: 0.2338 | loss_class: 0.2332 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:15:11] 	Iter 625800 Done. | loss1: 0.0012 | loss_class: 0.0006 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:16:01] 	Iter 625900 Done. | loss1: 0.0820 | loss_class: 0.0812 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|03:16:51] 	Iter 626000 Done. | loss1: 0.2330 | loss_class: 0.2324 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:17:42] 	Iter 626100 Done. | loss1: 0.7316 | loss_class: 0.7309 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:18:32] 	Iter 626200 Done. | loss1: 0.0042 | loss_class: 0.0035 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:19:23] 	Iter 626300 Done. | loss1: 0.0027 | loss_class: 0.0022 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:20:13] 	Iter 626400 Done. | loss1: 0.4010 | loss_class: 0.4002 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:21:03] 	Iter 626500 Done. | loss1: 0.1986 | loss_class: 0.1981 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:21:54] 	Iter 626600 Done. | loss1: 0.0908 | loss_class: 0.0903 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:22:44] 	Iter 626700 Done. | loss1: 0.1421 | loss_class: 0.1416 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:23:34] 	Iter 626800 Done. | loss1: 0.4275 | loss_class: 0.4268 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:24:26] 	Iter 626900 Done. | loss1: 0.1541 | loss_class: 0.1535 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:25:17] 	Iter 627000 Done. | loss1: 0.6669 | loss_class: 0.6664 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:26:08] 	Iter 627100 Done. | loss1: 0.1066 | loss_class: 0.1060 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:26:58] 	Iter 627200 Done. | loss1: 0.0710 | loss_class: 0.0703 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:27:49] 	Iter 627300 Done. | loss1: 0.3247 | loss_class: 0.3242 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:28:39] 	Iter 627400 Done. | loss1: 0.0802 | loss_class: 0.0795 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:29:29] 	Iter 627500 Done. | loss1: 0.1206 | loss_class: 0.1201 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:30:20] 	Iter 627600 Done. | loss1: 0.0751 | loss_class: 0.0745 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:31:10] 	Iter 627700 Done. | loss1: 0.0352 | loss_class: 0.0347 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:32:01] 	Iter 627800 Done. | loss1: 0.4509 | loss_class: 0.4504 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:32:51] 	Iter 627900 Done. | loss1: 0.2287 | loss_class: 0.2281 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:33:41] 	Iter 628000 Done. | loss1: 0.0423 | loss_class: 0.0416 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:34:33] 	Iter 628100 Done. | loss1: 0.0165 | loss_class: 0.0158 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:35:24] 	Iter 628200 Done. | loss1: 0.2249 | loss_class: 0.2244 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:36:16] 	Iter 628300 Done. | loss1: 0.0030 | loss_class: 0.0021 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|03:37:07] 	Iter 628400 Done. | loss1: 0.1709 | loss_class: 0.1704 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:37:57] 	Iter 628500 Done. | loss1: 0.0087 | loss_class: 0.0079 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|03:38:48] 	Iter 628600 Done. | loss1: 0.0088 | loss_class: 0.0083 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:39:39] 	Iter 628700 Done. | loss1: 0.0175 | loss_class: 0.0171 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:40:29] 	Iter 628800 Done. | loss1: 0.1066 | loss_class: 0.1060 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:41:24] 	Iter 628900 Done. | loss1: 0.0846 | loss_class: 0.0838 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:42:18] 	Iter 629000 Done. | loss1: 0.0236 | loss_class: 0.0230 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:43:08] 	Iter 629100 Done. | loss1: 0.0046 | loss_class: 0.0042 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|03:43:59] 	Iter 629200 Done. | loss1: 0.0355 | loss_class: 0.0349 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:44:52] 	Iter 629300 Done. | loss1: 0.5829 | loss_class: 0.5821 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|03:45:42] 	Iter 629400 Done. | loss1: 0.0834 | loss_class: 0.0830 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|03:46:35] 	Iter 629500 Done. | loss1: 0.0019 | loss_class: 0.0013 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:47:29] 	Iter 629600 Done. | loss1: 0.0883 | loss_class: 0.0873 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|03:48:19] 	Iter 629700 Done. | loss1: 0.0369 | loss_class: 0.0364 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:49:09] 	Iter 629800 Done. | loss1: 0.0187 | loss_class: 0.0182 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:49:59] 	Iter 629900 Done. | loss1: 0.0194 | loss_class: 0.0188 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:50:50] 	Iter 630000 Done. | loss1: 0.0052 | loss_class: 0.0044 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|03:51:41] 	Iter 630100 Done. | loss1: 0.3087 | loss_class: 0.3081 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:52:31] 	Iter 630200 Done. | loss1: 0.0576 | loss_class: 0.0570 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:53:21] 	Iter 630300 Done. | loss1: 0.6190 | loss_class: 0.6183 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:54:12] 	Iter 630400 Done. | loss1: 0.0108 | loss_class: 0.0102 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:55:03] 	Iter 630500 Done. | loss1: 0.1125 | loss_class: 0.1119 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|03:55:57] 	Iter 630600 Done. | loss1: 1.3756 | loss_class: 1.3751 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:56:53] 	Iter 630700 Done. | loss1: 0.0072 | loss_class: 0.0064 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:57:47] 	Iter 630800 Done. | loss1: 0.0270 | loss_class: 0.0265 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|03:58:38] 	Iter 630900 Done. | loss1: 0.4370 | loss_class: 0.4363 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|03:59:29] 	Iter 631000 Done. | loss1: 0.0429 | loss_class: 0.0423 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:00:21] 	Iter 631100 Done. | loss1: 0.3248 | loss_class: 0.3243 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:01:12] 	Iter 631200 Done. | loss1: 0.4321 | loss_class: 0.4316 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:02:03] 	Iter 631300 Done. | loss1: 0.0088 | loss_class: 0.0083 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:02:45] 	mean_loss1: 0.17027944635471123
[06.26.21|04:02:45] 	mean_loss_class: 0.16963491155431512
[06.26.21|04:02:45] 	mean_loss_recon: 0.000644534750996004
[06.26.21|04:02:45] Time consumption:
[06.26.21|04:02:45] Done.
[06.26.21|04:02:46] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch72_model1.pt.
[06.26.21|04:02:46] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch72_model2.pt.
[06.26.21|04:02:46] Training epoch: 73
[06.26.21|04:02:54] 	Iter 631400 Done. | loss1: 0.0742 | loss_class: 0.0736 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:03:45] 	Iter 631500 Done. | loss1: 0.0141 | loss_class: 0.0133 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|04:04:36] 	Iter 631600 Done. | loss1: 0.0534 | loss_class: 0.0529 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:05:26] 	Iter 631700 Done. | loss1: 0.0225 | loss_class: 0.0220 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:06:19] 	Iter 631800 Done. | loss1: 0.0170 | loss_class: 0.0164 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:07:14] 	Iter 631900 Done. | loss1: 0.0361 | loss_class: 0.0354 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:08:09] 	Iter 632000 Done. | loss1: 0.0698 | loss_class: 0.0694 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|04:09:03] 	Iter 632100 Done. | loss1: 0.0357 | loss_class: 0.0349 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|04:09:54] 	Iter 632200 Done. | loss1: 0.1116 | loss_class: 0.1108 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|04:10:44] 	Iter 632300 Done. | loss1: 0.0168 | loss_class: 0.0161 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:11:35] 	Iter 632400 Done. | loss1: 0.2682 | loss_class: 0.2674 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|04:12:26] 	Iter 632500 Done. | loss1: 0.0594 | loss_class: 0.0587 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:13:17] 	Iter 632600 Done. | loss1: 0.1014 | loss_class: 0.1007 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:14:08] 	Iter 632700 Done. | loss1: 0.1938 | loss_class: 0.1932 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:14:58] 	Iter 632800 Done. | loss1: 0.1812 | loss_class: 0.1804 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|04:15:49] 	Iter 632900 Done. | loss1: 0.2428 | loss_class: 0.2422 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:16:40] 	Iter 633000 Done. | loss1: 0.0030 | loss_class: 0.0025 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:17:30] 	Iter 633100 Done. | loss1: 0.0524 | loss_class: 0.0517 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:18:20] 	Iter 633200 Done. | loss1: 0.0583 | loss_class: 0.0577 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:19:10] 	Iter 633300 Done. | loss1: 0.0309 | loss_class: 0.0303 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:20:01] 	Iter 633400 Done. | loss1: 0.0775 | loss_class: 0.0770 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:20:53] 	Iter 633500 Done. | loss1: 0.2121 | loss_class: 0.2115 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:21:43] 	Iter 633600 Done. | loss1: 0.4993 | loss_class: 0.4987 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:22:36] 	Iter 633700 Done. | loss1: 0.0075 | loss_class: 0.0070 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:23:27] 	Iter 633800 Done. | loss1: 0.0703 | loss_class: 0.0696 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|04:24:19] 	Iter 633900 Done. | loss1: 0.0230 | loss_class: 0.0223 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:25:15] 	Iter 634000 Done. | loss1: 0.0234 | loss_class: 0.0227 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:26:08] 	Iter 634100 Done. | loss1: 0.0465 | loss_class: 0.0457 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|04:26:59] 	Iter 634200 Done. | loss1: 0.0167 | loss_class: 0.0160 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:27:50] 	Iter 634300 Done. | loss1: 0.1738 | loss_class: 0.1731 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:28:41] 	Iter 634400 Done. | loss1: 0.6285 | loss_class: 0.6279 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:29:36] 	Iter 634500 Done. | loss1: 0.0222 | loss_class: 0.0215 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:30:28] 	Iter 634600 Done. | loss1: 0.2634 | loss_class: 0.2627 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:31:21] 	Iter 634700 Done. | loss1: 0.0295 | loss_class: 0.0288 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:32:14] 	Iter 634800 Done. | loss1: 0.0060 | loss_class: 0.0054 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:33:04] 	Iter 634900 Done. | loss1: 0.0215 | loss_class: 0.0209 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:33:54] 	Iter 635000 Done. | loss1: 0.0248 | loss_class: 0.0242 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:34:45] 	Iter 635100 Done. | loss1: 0.0009 | loss_class: 0.0004 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:35:35] 	Iter 635200 Done. | loss1: 0.0026 | loss_class: 0.0019 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:36:26] 	Iter 635300 Done. | loss1: 0.3063 | loss_class: 0.3056 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:37:16] 	Iter 635400 Done. | loss1: 0.4395 | loss_class: 0.4389 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:38:07] 	Iter 635500 Done. | loss1: 0.5209 | loss_class: 0.5203 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:38:58] 	Iter 635600 Done. | loss1: 0.0192 | loss_class: 0.0185 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:39:49] 	Iter 635700 Done. | loss1: 0.5360 | loss_class: 0.5354 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:40:39] 	Iter 635800 Done. | loss1: 0.0074 | loss_class: 0.0067 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:41:30] 	Iter 635900 Done. | loss1: 0.0677 | loss_class: 0.0670 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|04:42:21] 	Iter 636000 Done. | loss1: 0.2584 | loss_class: 0.2579 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:43:11] 	Iter 636100 Done. | loss1: 0.0026 | loss_class: 0.0020 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:44:02] 	Iter 636200 Done. | loss1: 0.0772 | loss_class: 0.0767 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:44:58] 	Iter 636300 Done. | loss1: 0.0114 | loss_class: 0.0108 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:45:50] 	Iter 636400 Done. | loss1: 0.0198 | loss_class: 0.0190 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|04:46:41] 	Iter 636500 Done. | loss1: 0.5883 | loss_class: 0.5876 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:47:31] 	Iter 636600 Done. | loss1: 0.0680 | loss_class: 0.0672 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|04:48:21] 	Iter 636700 Done. | loss1: 0.0055 | loss_class: 0.0049 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:49:11] 	Iter 636800 Done. | loss1: 0.0286 | loss_class: 0.0279 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:50:02] 	Iter 636900 Done. | loss1: 0.0091 | loss_class: 0.0082 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|04:50:53] 	Iter 637000 Done. | loss1: 0.8554 | loss_class: 0.8549 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|04:51:43] 	Iter 637100 Done. | loss1: 0.0440 | loss_class: 0.0433 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:52:33] 	Iter 637200 Done. | loss1: 0.5505 | loss_class: 0.5499 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:53:23] 	Iter 637300 Done. | loss1: 0.0615 | loss_class: 0.0608 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:54:14] 	Iter 637400 Done. | loss1: 0.0485 | loss_class: 0.0479 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:55:04] 	Iter 637500 Done. | loss1: 0.1183 | loss_class: 0.1175 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|04:55:55] 	Iter 637600 Done. | loss1: 0.0544 | loss_class: 0.0538 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:56:46] 	Iter 637700 Done. | loss1: 0.1883 | loss_class: 0.1876 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|04:57:36] 	Iter 637800 Done. | loss1: 0.0367 | loss_class: 0.0363 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|04:58:27] 	Iter 637900 Done. | loss1: 0.3633 | loss_class: 0.3625 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|04:59:18] 	Iter 638000 Done. | loss1: 0.0076 | loss_class: 0.0071 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:00:09] 	Iter 638100 Done. | loss1: 0.1581 | loss_class: 0.1574 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:01:00] 	Iter 638200 Done. | loss1: 0.0771 | loss_class: 0.0764 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:01:50] 	Iter 638300 Done. | loss1: 0.0096 | loss_class: 0.0089 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:02:45] 	Iter 638400 Done. | loss1: 0.0024 | loss_class: 0.0018 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:03:36] 	Iter 638500 Done. | loss1: 0.0616 | loss_class: 0.0608 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:04:29] 	Iter 638600 Done. | loss1: 0.9352 | loss_class: 0.9345 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:05:20] 	Iter 638700 Done. | loss1: 0.0018 | loss_class: 0.0009 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|05:06:11] 	Iter 638800 Done. | loss1: 0.0775 | loss_class: 0.0768 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:07:02] 	Iter 638900 Done. | loss1: 0.0181 | loss_class: 0.0176 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:07:53] 	Iter 639000 Done. | loss1: 0.0434 | loss_class: 0.0428 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:08:45] 	Iter 639100 Done. | loss1: 0.0545 | loss_class: 0.0540 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:09:41] 	Iter 639200 Done. | loss1: 0.0130 | loss_class: 0.0122 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|05:10:33] 	Iter 639300 Done. | loss1: 0.1071 | loss_class: 0.1067 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:11:23] 	Iter 639400 Done. | loss1: 0.1874 | loss_class: 0.1867 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:12:13] 	Iter 639500 Done. | loss1: 0.0419 | loss_class: 0.0413 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:13:04] 	Iter 639600 Done. | loss1: 0.0044 | loss_class: 0.0037 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:13:58] 	Iter 639700 Done. | loss1: 0.0246 | loss_class: 0.0240 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:14:48] 	Iter 639800 Done. | loss1: 0.7546 | loss_class: 0.7540 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:15:39] 	Iter 639900 Done. | loss1: 0.0019 | loss_class: 0.0013 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:16:30] 	Iter 640000 Done. | loss1: 0.0335 | loss_class: 0.0328 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:17:20] 	Iter 640100 Done. | loss1: 0.0545 | loss_class: 0.0538 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:18:11] 	Iter 640200 Done. | loss1: 0.0269 | loss_class: 0.0264 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:19:03] 	Iter 640300 Done. | loss1: 1.3643 | loss_class: 1.3638 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|05:19:55] 	Iter 640400 Done. | loss1: 0.0137 | loss_class: 0.0130 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:20:45] 	Iter 640500 Done. | loss1: 0.5015 | loss_class: 0.5009 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:21:36] 	Iter 640600 Done. | loss1: 0.0031 | loss_class: 0.0025 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:22:26] 	Iter 640700 Done. | loss1: 0.0032 | loss_class: 0.0026 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:23:17] 	Iter 640800 Done. | loss1: 0.7698 | loss_class: 0.7689 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|05:24:07] 	Iter 640900 Done. | loss1: 0.0117 | loss_class: 0.0112 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:24:58] 	Iter 641000 Done. | loss1: 0.0107 | loss_class: 0.0102 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:25:49] 	Iter 641100 Done. | loss1: 0.0219 | loss_class: 0.0210 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|05:26:39] 	Iter 641200 Done. | loss1: 0.5554 | loss_class: 0.5543 | loss_recon: 0.0011 | lr: 0.001000
[06.26.21|05:27:31] 	Iter 641300 Done. | loss1: 0.0137 | loss_class: 0.0131 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:28:24] 	Iter 641400 Done. | loss1: 0.0841 | loss_class: 0.0835 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:28:28] 	mean_loss1: 0.15310833709845734
[06.26.21|05:28:28] 	mean_loss_class: 0.15246090469588341
[06.26.21|05:28:28] 	mean_loss_recon: 0.0006474322795412944
[06.26.21|05:28:28] Time consumption:
[06.26.21|05:28:28] Done.
[06.26.21|05:28:28] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch73_model1.pt.
[06.26.21|05:28:28] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch73_model2.pt.
[06.26.21|05:28:28] Training epoch: 74
[06.26.21|05:29:15] 	Iter 641500 Done. | loss1: 0.1334 | loss_class: 0.1326 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|05:30:06] 	Iter 641600 Done. | loss1: 0.2258 | loss_class: 0.2253 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:30:57] 	Iter 641700 Done. | loss1: 0.0028 | loss_class: 0.0020 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|05:31:48] 	Iter 641800 Done. | loss1: 0.5825 | loss_class: 0.5819 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:32:38] 	Iter 641900 Done. | loss1: 0.0119 | loss_class: 0.0112 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:33:29] 	Iter 642000 Done. | loss1: 0.0542 | loss_class: 0.0537 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:34:21] 	Iter 642100 Done. | loss1: 0.0584 | loss_class: 0.0576 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|05:35:15] 	Iter 642200 Done. | loss1: 0.0054 | loss_class: 0.0050 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:36:05] 	Iter 642300 Done. | loss1: 0.0800 | loss_class: 0.0793 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:36:55] 	Iter 642400 Done. | loss1: 0.0568 | loss_class: 0.0563 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:37:46] 	Iter 642500 Done. | loss1: 0.0331 | loss_class: 0.0325 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:38:36] 	Iter 642600 Done. | loss1: 0.0266 | loss_class: 0.0259 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:39:27] 	Iter 642700 Done. | loss1: 0.1246 | loss_class: 0.1240 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:40:17] 	Iter 642800 Done. | loss1: 0.3933 | loss_class: 0.3926 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:41:08] 	Iter 642900 Done. | loss1: 0.0313 | loss_class: 0.0305 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|05:41:59] 	Iter 643000 Done. | loss1: 0.0646 | loss_class: 0.0639 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:42:49] 	Iter 643100 Done. | loss1: 0.0100 | loss_class: 0.0081 | loss_recon: 0.0019 | lr: 0.001000
[06.26.21|05:43:43] 	Iter 643200 Done. | loss1: 0.1550 | loss_class: 0.1543 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:44:38] 	Iter 643300 Done. | loss1: 0.7850 | loss_class: 0.7845 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:45:33] 	Iter 643400 Done. | loss1: 0.0415 | loss_class: 0.0409 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:46:24] 	Iter 643500 Done. | loss1: 0.0055 | loss_class: 0.0048 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:47:15] 	Iter 643600 Done. | loss1: 0.0179 | loss_class: 0.0172 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:48:05] 	Iter 643700 Done. | loss1: 0.2488 | loss_class: 0.2482 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:48:56] 	Iter 643800 Done. | loss1: 0.1809 | loss_class: 0.1802 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:49:46] 	Iter 643900 Done. | loss1: 0.0042 | loss_class: 0.0037 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:50:40] 	Iter 644000 Done. | loss1: 0.0387 | loss_class: 0.0380 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:51:30] 	Iter 644100 Done. | loss1: 0.0511 | loss_class: 0.0505 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:52:20] 	Iter 644200 Done. | loss1: 0.0209 | loss_class: 0.0204 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:53:11] 	Iter 644300 Done. | loss1: 0.0095 | loss_class: 0.0087 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|05:54:04] 	Iter 644400 Done. | loss1: 0.0086 | loss_class: 0.0077 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|05:54:56] 	Iter 644500 Done. | loss1: 0.9100 | loss_class: 0.9094 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|05:55:46] 	Iter 644600 Done. | loss1: 0.0041 | loss_class: 0.0036 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:56:36] 	Iter 644700 Done. | loss1: 0.0165 | loss_class: 0.0161 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|05:57:26] 	Iter 644800 Done. | loss1: 0.1508 | loss_class: 0.1499 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|05:58:17] 	Iter 644900 Done. | loss1: 0.0100 | loss_class: 0.0094 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|05:59:08] 	Iter 645000 Done. | loss1: 0.0012 | loss_class: 0.0008 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|05:59:58] 	Iter 645100 Done. | loss1: 0.0877 | loss_class: 0.0871 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:00:49] 	Iter 645200 Done. | loss1: 0.0767 | loss_class: 0.0760 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:01:39] 	Iter 645300 Done. | loss1: 0.1314 | loss_class: 0.1309 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|06:02:29] 	Iter 645400 Done. | loss1: 0.8790 | loss_class: 0.8785 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|06:03:20] 	Iter 645500 Done. | loss1: 0.0065 | loss_class: 0.0059 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:04:10] 	Iter 645600 Done. | loss1: 0.1160 | loss_class: 0.1151 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|06:05:03] 	Iter 645700 Done. | loss1: 0.0065 | loss_class: 0.0060 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:05:58] 	Iter 645800 Done. | loss1: 0.0223 | loss_class: 0.0218 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|06:06:52] 	Iter 645900 Done. | loss1: 0.0681 | loss_class: 0.0672 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|06:07:42] 	Iter 646000 Done. | loss1: 0.0657 | loss_class: 0.0651 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:08:32] 	Iter 646100 Done. | loss1: 0.2733 | loss_class: 0.2728 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|06:09:23] 	Iter 646200 Done. | loss1: 0.1117 | loss_class: 0.1107 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|06:10:13] 	Iter 646300 Done. | loss1: 0.0059 | loss_class: 0.0053 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:11:04] 	Iter 646400 Done. | loss1: 0.0282 | loss_class: 0.0275 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:11:55] 	Iter 646500 Done. | loss1: 0.0310 | loss_class: 0.0301 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|06:12:45] 	Iter 646600 Done. | loss1: 0.0141 | loss_class: 0.0136 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|06:13:36] 	Iter 646700 Done. | loss1: 0.0180 | loss_class: 0.0174 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|06:14:27] 	Iter 646800 Done. | loss1: 0.0105 | loss_class: 0.0098 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:15:17] 	Iter 646900 Done. | loss1: 0.0789 | loss_class: 0.0780 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|06:16:07] 	Iter 647000 Done. | loss1: 0.5788 | loss_class: 0.5781 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:16:58] 	Iter 647100 Done. | loss1: 0.0313 | loss_class: 0.0307 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|06:17:49] 	Iter 647200 Done. | loss1: 0.0283 | loss_class: 0.0277 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|06:18:40] 	Iter 647300 Done. | loss1: 0.0646 | loss_class: 0.0638 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:19:30] 	Iter 647400 Done. | loss1: 0.0738 | loss_class: 0.0732 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:20:22] 	Iter 647500 Done. | loss1: 0.0158 | loss_class: 0.0151 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:21:18] 	Iter 647600 Done. | loss1: 0.1303 | loss_class: 0.1296 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:22:09] 	Iter 647700 Done. | loss1: 0.0781 | loss_class: 0.0774 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:23:00] 	Iter 647800 Done. | loss1: 0.0051 | loss_class: 0.0044 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:23:50] 	Iter 647900 Done. | loss1: 0.0158 | loss_class: 0.0152 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|06:24:41] 	Iter 648000 Done. | loss1: 0.0561 | loss_class: 0.0554 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:25:31] 	Iter 648100 Done. | loss1: 0.0125 | loss_class: 0.0118 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:26:22] 	Iter 648200 Done. | loss1: 0.0017 | loss_class: 0.0011 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:27:12] 	Iter 648300 Done. | loss1: 0.2258 | loss_class: 0.2251 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:28:03] 	Iter 648400 Done. | loss1: 0.0243 | loss_class: 0.0237 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:28:53] 	Iter 648500 Done. | loss1: 0.2357 | loss_class: 0.2352 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|06:29:43] 	Iter 648600 Done. | loss1: 0.0132 | loss_class: 0.0126 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|06:30:33] 	Iter 648700 Done. | loss1: 0.0203 | loss_class: 0.0198 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:31:24] 	Iter 648800 Done. | loss1: 0.0944 | loss_class: 0.0938 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:32:15] 	Iter 648900 Done. | loss1: 0.2041 | loss_class: 0.2037 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|06:33:06] 	Iter 649000 Done. | loss1: 0.1030 | loss_class: 0.1025 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:33:57] 	Iter 649100 Done. | loss1: 0.0070 | loss_class: 0.0063 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|06:34:48] 	Iter 649200 Done. | loss1: 0.2057 | loss_class: 0.2051 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:35:40] 	Iter 649300 Done. | loss1: 0.2123 | loss_class: 0.2116 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:36:30] 	Iter 649400 Done. | loss1: 0.0040 | loss_class: 0.0034 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:37:20] 	Iter 649500 Done. | loss1: 0.2960 | loss_class: 0.2955 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|06:38:10] 	Iter 649600 Done. | loss1: 0.6658 | loss_class: 0.6649 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|06:39:00] 	Iter 649700 Done. | loss1: 0.6737 | loss_class: 0.6726 | loss_recon: 0.0011 | lr: 0.001000
[06.26.21|06:39:50] 	Iter 649800 Done. | loss1: 0.0542 | loss_class: 0.0536 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:40:39] 	Iter 649900 Done. | loss1: 0.0342 | loss_class: 0.0337 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:41:29] 	Iter 650000 Done. | loss1: 0.0477 | loss_class: 0.0471 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:42:20] 	Iter 650100 Done. | loss1: 0.3712 | loss_class: 0.3706 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:43:10] 	Iter 650200 Done. | loss1: 1.0225 | loss_class: 1.0220 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:44:00] 	Iter 650300 Done. | loss1: 0.0807 | loss_class: 0.0802 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:44:51] 	Iter 650400 Done. | loss1: 0.0228 | loss_class: 0.0222 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:45:42] 	Iter 650500 Done. | loss1: 0.0565 | loss_class: 0.0558 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:46:36] 	Iter 650600 Done. | loss1: 0.0372 | loss_class: 0.0367 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|06:47:31] 	Iter 650700 Done. | loss1: 0.0454 | loss_class: 0.0447 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:48:22] 	Iter 650800 Done. | loss1: 0.0698 | loss_class: 0.0691 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:49:13] 	Iter 650900 Done. | loss1: 0.2103 | loss_class: 0.2096 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:50:08] 	Iter 651000 Done. | loss1: 0.0729 | loss_class: 0.0723 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:51:03] 	Iter 651100 Done. | loss1: 0.0080 | loss_class: 0.0072 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|06:51:56] 	Iter 651200 Done. | loss1: 0.7444 | loss_class: 0.7438 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:52:47] 	Iter 651300 Done. | loss1: 0.0201 | loss_class: 0.0195 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|06:53:37] 	Iter 651400 Done. | loss1: 0.0394 | loss_class: 0.0388 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|06:53:52] 	mean_loss1: 0.14339219022656796
[06.26.21|06:53:52] 	mean_loss_class: 0.14274594810936728
[06.26.21|06:53:52] 	mean_loss_recon: 0.0006462420889116398
[06.26.21|06:53:52] Time consumption:
[06.26.21|06:53:52] Done.
[06.26.21|06:53:52] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch74_model1.pt.
[06.26.21|06:53:52] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch74_model2.pt.
[06.26.21|06:53:52] Eval epoch: 74
[06.26.21|07:00:21] 	mean_loss1: 0.5959189997177369
[06.26.21|07:00:21] 	mean_loss_class: 0.5955674975258551
[06.26.21|07:00:21] 	mean_loss_recon: 0.035150121319894645
[06.26.21|07:00:21] 

[06.26.21|07:00:21] 	Top1: 85.07%
[06.26.21|07:00:21] 

[06.26.21|07:00:21] 	Top5: 97.17%
[06.26.21|07:00:21] Done.
[06.26.21|07:00:21] Training epoch: 75
[06.26.21|07:00:58] 	Iter 651500 Done. | loss1: 0.0067 | loss_class: 0.0063 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|07:01:49] 	Iter 651600 Done. | loss1: 0.0313 | loss_class: 0.0308 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:02:40] 	Iter 651700 Done. | loss1: 0.1106 | loss_class: 0.1099 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:03:34] 	Iter 651800 Done. | loss1: 0.0297 | loss_class: 0.0291 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:04:27] 	Iter 651900 Done. | loss1: 0.0071 | loss_class: 0.0065 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:05:17] 	Iter 652000 Done. | loss1: 0.0482 | loss_class: 0.0476 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:06:11] 	Iter 652100 Done. | loss1: 0.1440 | loss_class: 0.1435 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|07:07:02] 	Iter 652200 Done. | loss1: 0.0676 | loss_class: 0.0671 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|07:07:55] 	Iter 652300 Done. | loss1: 0.0096 | loss_class: 0.0088 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|07:08:46] 	Iter 652400 Done. | loss1: 0.0017 | loss_class: 0.0010 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:09:37] 	Iter 652500 Done. | loss1: 0.0052 | loss_class: 0.0044 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|07:10:27] 	Iter 652600 Done. | loss1: 0.0508 | loss_class: 0.0502 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:11:17] 	Iter 652700 Done. | loss1: 0.0500 | loss_class: 0.0491 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|07:12:09] 	Iter 652800 Done. | loss1: 0.0331 | loss_class: 0.0322 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|07:13:00] 	Iter 652900 Done. | loss1: 0.0020 | loss_class: 0.0014 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:13:50] 	Iter 653000 Done. | loss1: 0.5055 | loss_class: 0.5049 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|07:14:41] 	Iter 653100 Done. | loss1: 0.0104 | loss_class: 0.0098 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|07:15:32] 	Iter 653200 Done. | loss1: 0.0261 | loss_class: 0.0256 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|07:16:22] 	Iter 653300 Done. | loss1: 0.0179 | loss_class: 0.0170 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|07:17:12] 	Iter 653400 Done. | loss1: 0.0313 | loss_class: 0.0308 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:18:02] 	Iter 653500 Done. | loss1: 0.0418 | loss_class: 0.0410 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:18:53] 	Iter 653600 Done. | loss1: 0.2280 | loss_class: 0.2275 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|07:19:44] 	Iter 653700 Done. | loss1: 0.0214 | loss_class: 0.0204 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|07:20:34] 	Iter 653800 Done. | loss1: 0.2639 | loss_class: 0.2634 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|07:21:24] 	Iter 653900 Done. | loss1: 0.0458 | loss_class: 0.0449 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|07:22:15] 	Iter 654000 Done. | loss1: 0.0202 | loss_class: 0.0196 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:23:06] 	Iter 654100 Done. | loss1: 0.0050 | loss_class: 0.0043 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:24:02] 	Iter 654200 Done. | loss1: 0.0012 | loss_class: 0.0007 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:24:58] 	Iter 654300 Done. | loss1: 0.0821 | loss_class: 0.0814 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:25:53] 	Iter 654400 Done. | loss1: 0.2075 | loss_class: 0.2070 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:26:48] 	Iter 654500 Done. | loss1: 0.0841 | loss_class: 0.0835 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:27:39] 	Iter 654600 Done. | loss1: 0.0206 | loss_class: 0.0200 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:28:29] 	Iter 654700 Done. | loss1: 0.5443 | loss_class: 0.5434 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|07:29:20] 	Iter 654800 Done. | loss1: 0.4551 | loss_class: 0.4545 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|07:30:11] 	Iter 654900 Done. | loss1: 0.0445 | loss_class: 0.0439 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:31:01] 	Iter 655000 Done. | loss1: 0.3351 | loss_class: 0.3345 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:31:52] 	Iter 655100 Done. | loss1: 0.0193 | loss_class: 0.0187 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:32:44] 	Iter 655200 Done. | loss1: 0.0161 | loss_class: 0.0154 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:33:37] 	Iter 655300 Done. | loss1: 0.0420 | loss_class: 0.0415 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:34:29] 	Iter 655400 Done. | loss1: 0.0080 | loss_class: 0.0073 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:35:20] 	Iter 655500 Done. | loss1: 0.0034 | loss_class: 0.0026 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|07:36:11] 	Iter 655600 Done. | loss1: 0.1647 | loss_class: 0.1642 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|07:37:02] 	Iter 655700 Done. | loss1: 1.0591 | loss_class: 1.0587 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|07:37:53] 	Iter 655800 Done. | loss1: 0.0206 | loss_class: 0.0199 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|07:38:46] 	Iter 655900 Done. | loss1: 0.0059 | loss_class: 0.0054 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|07:39:36] 	Iter 656000 Done. | loss1: 0.0092 | loss_class: 0.0084 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|07:40:27] 	Iter 656100 Done. | loss1: 0.0088 | loss_class: 0.0081 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:41:17] 	Iter 656200 Done. | loss1: 0.0159 | loss_class: 0.0150 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|07:42:07] 	Iter 656300 Done. | loss1: 0.0172 | loss_class: 0.0167 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:42:59] 	Iter 656400 Done. | loss1: 0.0044 | loss_class: 0.0038 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:43:49] 	Iter 656500 Done. | loss1: 0.3531 | loss_class: 0.3523 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:44:41] 	Iter 656600 Done. | loss1: 0.1307 | loss_class: 0.1301 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:45:36] 	Iter 656700 Done. | loss1: 0.0028 | loss_class: 0.0021 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:46:28] 	Iter 656800 Done. | loss1: 0.0147 | loss_class: 0.0138 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|07:47:19] 	Iter 656900 Done. | loss1: 0.0576 | loss_class: 0.0569 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:48:10] 	Iter 657000 Done. | loss1: 0.8154 | loss_class: 0.8147 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:49:01] 	Iter 657100 Done. | loss1: 0.5750 | loss_class: 0.5744 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:49:52] 	Iter 657200 Done. | loss1: 0.0053 | loss_class: 0.0047 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:50:43] 	Iter 657300 Done. | loss1: 0.0396 | loss_class: 0.0391 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:51:34] 	Iter 657400 Done. | loss1: 0.2863 | loss_class: 0.2858 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|07:52:25] 	Iter 657500 Done. | loss1: 0.0672 | loss_class: 0.0665 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:53:17] 	Iter 657600 Done. | loss1: 0.3753 | loss_class: 0.3746 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:54:08] 	Iter 657700 Done. | loss1: 0.1689 | loss_class: 0.1683 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:54:59] 	Iter 657800 Done. | loss1: 0.0027 | loss_class: 0.0020 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:55:49] 	Iter 657900 Done. | loss1: 0.0421 | loss_class: 0.0415 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:56:39] 	Iter 658000 Done. | loss1: 0.0059 | loss_class: 0.0054 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|07:57:30] 	Iter 658100 Done. | loss1: 0.0040 | loss_class: 0.0033 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|07:58:21] 	Iter 658200 Done. | loss1: 0.0405 | loss_class: 0.0398 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|07:59:11] 	Iter 658300 Done. | loss1: 0.0097 | loss_class: 0.0091 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|08:00:02] 	Iter 658400 Done. | loss1: 0.1281 | loss_class: 0.1275 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:00:53] 	Iter 658500 Done. | loss1: 0.7143 | loss_class: 0.7137 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:01:44] 	Iter 658600 Done. | loss1: 0.0590 | loss_class: 0.0585 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:02:35] 	Iter 658700 Done. | loss1: 0.0984 | loss_class: 0.0979 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:03:26] 	Iter 658800 Done. | loss1: 0.0169 | loss_class: 0.0161 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|08:04:17] 	Iter 658900 Done. | loss1: 0.1079 | loss_class: 0.1074 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|08:05:08] 	Iter 659000 Done. | loss1: 0.0598 | loss_class: 0.0592 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:05:58] 	Iter 659100 Done. | loss1: 0.2898 | loss_class: 0.2893 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:06:53] 	Iter 659200 Done. | loss1: 0.6927 | loss_class: 0.6922 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:07:48] 	Iter 659300 Done. | loss1: 0.0168 | loss_class: 0.0162 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:08:38] 	Iter 659400 Done. | loss1: 0.0135 | loss_class: 0.0129 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|08:09:28] 	Iter 659500 Done. | loss1: 0.0102 | loss_class: 0.0098 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|08:10:20] 	Iter 659600 Done. | loss1: 0.0045 | loss_class: 0.0039 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:11:15] 	Iter 659700 Done. | loss1: 0.1037 | loss_class: 0.1031 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:12:10] 	Iter 659800 Done. | loss1: 0.3604 | loss_class: 0.3598 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:13:00] 	Iter 659900 Done. | loss1: 0.0326 | loss_class: 0.0321 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:13:51] 	Iter 660000 Done. | loss1: 1.2265 | loss_class: 1.2260 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:14:42] 	Iter 660100 Done. | loss1: 0.0109 | loss_class: 0.0096 | loss_recon: 0.0013 | lr: 0.001000
[06.26.21|08:15:33] 	Iter 660200 Done. | loss1: 0.0614 | loss_class: 0.0605 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|08:16:23] 	Iter 660300 Done. | loss1: 0.1177 | loss_class: 0.1170 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:17:14] 	Iter 660400 Done. | loss1: 0.1047 | loss_class: 0.1038 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|08:18:05] 	Iter 660500 Done. | loss1: 0.0166 | loss_class: 0.0159 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|08:18:56] 	Iter 660600 Done. | loss1: 0.0019 | loss_class: 0.0012 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|08:19:46] 	Iter 660700 Done. | loss1: 0.0507 | loss_class: 0.0501 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:20:37] 	Iter 660800 Done. | loss1: 0.0512 | loss_class: 0.0505 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|08:21:27] 	Iter 660900 Done. | loss1: 0.0205 | loss_class: 0.0200 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:22:18] 	Iter 661000 Done. | loss1: 0.1150 | loss_class: 0.1143 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|08:23:08] 	Iter 661100 Done. | loss1: 0.0073 | loss_class: 0.0066 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|08:23:59] 	Iter 661200 Done. | loss1: 0.0808 | loss_class: 0.0803 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:24:50] 	Iter 661300 Done. | loss1: 0.1429 | loss_class: 0.1423 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|08:25:41] 	Iter 661400 Done. | loss1: 0.0058 | loss_class: 0.0052 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:26:09] 	mean_loss1: 0.1344292172584324
[06.26.21|08:26:09] 	mean_loss_class: 0.13378389193198312
[06.26.21|08:26:09] 	mean_loss_recon: 0.0006453252813571217
[06.26.21|08:26:09] Time consumption:
[06.26.21|08:26:09] Done.
[06.26.21|08:26:09] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch75_model1.pt.
[06.26.21|08:26:09] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch75_model2.pt.
[06.26.21|08:26:09] Training epoch: 76
[06.26.21|08:26:35] 	Iter 661500 Done. | loss1: 0.0139 | loss_class: 0.0134 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:27:26] 	Iter 661600 Done. | loss1: 0.0281 | loss_class: 0.0275 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:28:17] 	Iter 661700 Done. | loss1: 0.0993 | loss_class: 0.0986 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|08:29:10] 	Iter 661800 Done. | loss1: 0.0562 | loss_class: 0.0552 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|08:30:05] 	Iter 661900 Done. | loss1: 0.1195 | loss_class: 0.1190 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:31:01] 	Iter 662000 Done. | loss1: 0.0311 | loss_class: 0.0306 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:31:51] 	Iter 662100 Done. | loss1: 0.4170 | loss_class: 0.4162 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|08:32:41] 	Iter 662200 Done. | loss1: 0.0257 | loss_class: 0.0251 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:33:32] 	Iter 662300 Done. | loss1: 0.0009 | loss_class: 0.0004 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:34:22] 	Iter 662400 Done. | loss1: 0.0263 | loss_class: 0.0259 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:35:12] 	Iter 662500 Done. | loss1: 0.0967 | loss_class: 0.0961 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:36:02] 	Iter 662600 Done. | loss1: 0.0188 | loss_class: 0.0179 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|08:36:52] 	Iter 662700 Done. | loss1: 0.0022 | loss_class: 0.0016 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:37:42] 	Iter 662800 Done. | loss1: 0.0101 | loss_class: 0.0097 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|08:38:32] 	Iter 662900 Done. | loss1: 0.1628 | loss_class: 0.1619 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|08:39:22] 	Iter 663000 Done. | loss1: 0.5978 | loss_class: 0.5972 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:40:13] 	Iter 663100 Done. | loss1: 0.2995 | loss_class: 0.2990 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|08:41:06] 	Iter 663200 Done. | loss1: 0.0061 | loss_class: 0.0051 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|08:41:56] 	Iter 663300 Done. | loss1: 0.0158 | loss_class: 0.0152 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:42:46] 	Iter 663400 Done. | loss1: 0.3157 | loss_class: 0.3154 | loss_recon: 0.0003 | lr: 0.001000
[06.26.21|08:43:36] 	Iter 663500 Done. | loss1: 0.0031 | loss_class: 0.0025 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:44:26] 	Iter 663600 Done. | loss1: 0.0675 | loss_class: 0.0668 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|08:45:16] 	Iter 663700 Done. | loss1: 0.1585 | loss_class: 0.1578 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:46:10] 	Iter 663800 Done. | loss1: 0.0081 | loss_class: 0.0072 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|08:47:05] 	Iter 663900 Done. | loss1: 0.0700 | loss_class: 0.0694 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:47:56] 	Iter 664000 Done. | loss1: 0.4578 | loss_class: 0.4572 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:48:47] 	Iter 664100 Done. | loss1: 0.0172 | loss_class: 0.0167 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:49:38] 	Iter 664200 Done. | loss1: 0.0365 | loss_class: 0.0360 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:50:29] 	Iter 664300 Done. | loss1: 0.2189 | loss_class: 0.2183 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:51:19] 	Iter 664400 Done. | loss1: 0.4130 | loss_class: 0.4121 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|08:52:09] 	Iter 664500 Done. | loss1: 0.0249 | loss_class: 0.0243 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:53:01] 	Iter 664600 Done. | loss1: 0.0026 | loss_class: 0.0018 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|08:53:57] 	Iter 664700 Done. | loss1: 0.0402 | loss_class: 0.0397 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:54:52] 	Iter 664800 Done. | loss1: 0.0019 | loss_class: 0.0014 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|08:55:47] 	Iter 664900 Done. | loss1: 0.1450 | loss_class: 0.1444 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:56:42] 	Iter 665000 Done. | loss1: 0.0135 | loss_class: 0.0129 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:57:33] 	Iter 665100 Done. | loss1: 0.0150 | loss_class: 0.0144 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|08:58:24] 	Iter 665200 Done. | loss1: 0.0053 | loss_class: 0.0049 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|08:59:16] 	Iter 665300 Done. | loss1: 0.0679 | loss_class: 0.0671 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|09:00:12] 	Iter 665400 Done. | loss1: 0.0777 | loss_class: 0.0771 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:01:03] 	Iter 665500 Done. | loss1: 0.0057 | loss_class: 0.0050 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:01:53] 	Iter 665600 Done. | loss1: 0.1231 | loss_class: 0.1226 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:02:44] 	Iter 665700 Done. | loss1: 0.1957 | loss_class: 0.1951 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:03:35] 	Iter 665800 Done. | loss1: 0.1068 | loss_class: 0.1063 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:04:26] 	Iter 665900 Done. | loss1: 0.6165 | loss_class: 0.6158 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:05:16] 	Iter 666000 Done. | loss1: 0.0793 | loss_class: 0.0786 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:06:07] 	Iter 666100 Done. | loss1: 0.3908 | loss_class: 0.3903 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|09:06:59] 	Iter 666200 Done. | loss1: 0.2313 | loss_class: 0.2305 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:07:51] 	Iter 666300 Done. | loss1: 0.0586 | loss_class: 0.0580 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:08:42] 	Iter 666400 Done. | loss1: 0.1656 | loss_class: 0.1649 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:09:32] 	Iter 666500 Done. | loss1: 0.0461 | loss_class: 0.0452 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|09:10:23] 	Iter 666600 Done. | loss1: 0.0473 | loss_class: 0.0467 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:11:18] 	Iter 666700 Done. | loss1: 0.0308 | loss_class: 0.0301 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:12:12] 	Iter 666800 Done. | loss1: 0.0067 | loss_class: 0.0060 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:13:04] 	Iter 666900 Done. | loss1: 0.1680 | loss_class: 0.1672 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|09:13:55] 	Iter 667000 Done. | loss1: 0.0656 | loss_class: 0.0651 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|09:14:45] 	Iter 667100 Done. | loss1: 0.1658 | loss_class: 0.1653 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|09:15:36] 	Iter 667200 Done. | loss1: 0.2210 | loss_class: 0.2205 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:16:27] 	Iter 667300 Done. | loss1: 0.0934 | loss_class: 0.0927 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:17:17] 	Iter 667400 Done. | loss1: 0.0347 | loss_class: 0.0341 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:18:08] 	Iter 667500 Done. | loss1: 0.0958 | loss_class: 0.0952 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:18:59] 	Iter 667600 Done. | loss1: 0.0357 | loss_class: 0.0350 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:19:50] 	Iter 667700 Done. | loss1: 0.0310 | loss_class: 0.0305 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|09:20:41] 	Iter 667800 Done. | loss1: 0.0026 | loss_class: 0.0020 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:21:32] 	Iter 667900 Done. | loss1: 0.0115 | loss_class: 0.0107 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|09:22:23] 	Iter 668000 Done. | loss1: 0.0041 | loss_class: 0.0035 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:23:14] 	Iter 668100 Done. | loss1: 0.0434 | loss_class: 0.0428 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:24:04] 	Iter 668200 Done. | loss1: 0.2780 | loss_class: 0.2775 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:24:55] 	Iter 668300 Done. | loss1: 0.0163 | loss_class: 0.0157 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:25:45] 	Iter 668400 Done. | loss1: 0.5408 | loss_class: 0.5400 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|09:26:36] 	Iter 668500 Done. | loss1: 0.0135 | loss_class: 0.0130 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:27:30] 	Iter 668600 Done. | loss1: 0.2704 | loss_class: 0.2699 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:28:21] 	Iter 668700 Done. | loss1: 0.0067 | loss_class: 0.0063 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|09:29:16] 	Iter 668800 Done. | loss1: 0.0161 | loss_class: 0.0154 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:30:09] 	Iter 668900 Done. | loss1: 0.1814 | loss_class: 0.1807 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:31:01] 	Iter 669000 Done. | loss1: 0.0137 | loss_class: 0.0131 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:31:52] 	Iter 669100 Done. | loss1: 0.0221 | loss_class: 0.0216 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|09:32:42] 	Iter 669200 Done. | loss1: 0.0138 | loss_class: 0.0132 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:33:32] 	Iter 669300 Done. | loss1: 0.0850 | loss_class: 0.0844 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:34:25] 	Iter 669400 Done. | loss1: 0.0512 | loss_class: 0.0505 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|09:35:16] 	Iter 669500 Done. | loss1: 0.0224 | loss_class: 0.0218 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:36:07] 	Iter 669600 Done. | loss1: 0.0111 | loss_class: 0.0104 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:36:57] 	Iter 669700 Done. | loss1: 0.0112 | loss_class: 0.0106 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:37:48] 	Iter 669800 Done. | loss1: 0.0082 | loss_class: 0.0077 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:38:40] 	Iter 669900 Done. | loss1: 0.5107 | loss_class: 0.5100 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:39:31] 	Iter 670000 Done. | loss1: 0.2932 | loss_class: 0.2925 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:40:22] 	Iter 670100 Done. | loss1: 0.0970 | loss_class: 0.0965 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|09:41:12] 	Iter 670200 Done. | loss1: 0.0244 | loss_class: 0.0236 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|09:42:02] 	Iter 670300 Done. | loss1: 0.0157 | loss_class: 0.0150 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:42:53] 	Iter 670400 Done. | loss1: 0.0269 | loss_class: 0.0264 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|09:43:43] 	Iter 670500 Done. | loss1: 0.6097 | loss_class: 0.6089 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|09:44:34] 	Iter 670600 Done. | loss1: 0.0023 | loss_class: 0.0015 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|09:45:25] 	Iter 670700 Done. | loss1: 0.2043 | loss_class: 0.2037 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:46:18] 	Iter 670800 Done. | loss1: 0.0041 | loss_class: 0.0035 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|09:47:11] 	Iter 670900 Done. | loss1: 0.0547 | loss_class: 0.0541 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:48:02] 	Iter 671000 Done. | loss1: 0.0154 | loss_class: 0.0147 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:48:57] 	Iter 671100 Done. | loss1: 0.0081 | loss_class: 0.0076 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:49:51] 	Iter 671200 Done. | loss1: 0.0425 | loss_class: 0.0419 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|09:50:42] 	Iter 671300 Done. | loss1: 0.0191 | loss_class: 0.0187 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|09:51:33] 	Iter 671400 Done. | loss1: 0.0144 | loss_class: 0.0137 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:52:10] 	mean_loss1: 0.12733158312757317
[06.26.21|09:52:10] 	mean_loss_class: 0.12668587049126387
[06.26.21|09:52:10] 	mean_loss_recon: 0.0006457126730277234
[06.26.21|09:52:10] Time consumption:
[06.26.21|09:52:10] Done.
[06.26.21|09:52:10] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch76_model1.pt.
[06.26.21|09:52:10] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch76_model2.pt.
[06.26.21|09:52:10] Training epoch: 77
[06.26.21|09:52:24] 	Iter 671500 Done. | loss1: 0.5634 | loss_class: 0.5627 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:53:16] 	Iter 671600 Done. | loss1: 0.8116 | loss_class: 0.8109 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|09:54:06] 	Iter 671700 Done. | loss1: 0.0082 | loss_class: 0.0077 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:54:58] 	Iter 671800 Done. | loss1: 0.2051 | loss_class: 0.2045 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:55:49] 	Iter 671900 Done. | loss1: 0.2603 | loss_class: 0.2598 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|09:56:40] 	Iter 672000 Done. | loss1: 0.3900 | loss_class: 0.3894 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:57:30] 	Iter 672100 Done. | loss1: 0.0118 | loss_class: 0.0113 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|09:58:21] 	Iter 672200 Done. | loss1: 0.0057 | loss_class: 0.0051 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|09:59:12] 	Iter 672300 Done. | loss1: 0.0229 | loss_class: 0.0222 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|10:00:02] 	Iter 672400 Done. | loss1: 0.0233 | loss_class: 0.0227 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:00:52] 	Iter 672500 Done. | loss1: 0.0118 | loss_class: 0.0114 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|10:01:42] 	Iter 672600 Done. | loss1: 0.0826 | loss_class: 0.0815 | loss_recon: 0.0011 | lr: 0.001000
[06.26.21|10:02:32] 	Iter 672700 Done. | loss1: 0.0054 | loss_class: 0.0049 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:03:22] 	Iter 672800 Done. | loss1: 0.7727 | loss_class: 0.7720 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:04:13] 	Iter 672900 Done. | loss1: 0.0029 | loss_class: 0.0022 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|10:05:03] 	Iter 673000 Done. | loss1: 0.1456 | loss_class: 0.1449 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|10:05:54] 	Iter 673100 Done. | loss1: 0.0989 | loss_class: 0.0981 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|10:06:44] 	Iter 673200 Done. | loss1: 0.1952 | loss_class: 0.1945 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:07:35] 	Iter 673300 Done. | loss1: 0.0092 | loss_class: 0.0086 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:08:26] 	Iter 673400 Done. | loss1: 0.2480 | loss_class: 0.2475 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:09:16] 	Iter 673500 Done. | loss1: 0.1239 | loss_class: 0.1232 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:10:07] 	Iter 673600 Done. | loss1: 1.0722 | loss_class: 1.0713 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|10:10:58] 	Iter 673700 Done. | loss1: 0.0103 | loss_class: 0.0097 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|10:11:48] 	Iter 673800 Done. | loss1: 0.0467 | loss_class: 0.0460 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|10:12:39] 	Iter 673900 Done. | loss1: 0.0205 | loss_class: 0.0199 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|10:13:32] 	Iter 674000 Done. | loss1: 0.0022 | loss_class: 0.0015 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|10:14:23] 	Iter 674100 Done. | loss1: 0.1827 | loss_class: 0.1822 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:15:14] 	Iter 674200 Done. | loss1: 0.1881 | loss_class: 0.1874 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:16:04] 	Iter 674300 Done. | loss1: 0.0052 | loss_class: 0.0046 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|10:16:58] 	Iter 674400 Done. | loss1: 0.0516 | loss_class: 0.0510 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:17:49] 	Iter 674500 Done. | loss1: 0.6460 | loss_class: 0.6454 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:18:41] 	Iter 674600 Done. | loss1: 0.0223 | loss_class: 0.0217 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:19:32] 	Iter 674700 Done. | loss1: 0.0678 | loss_class: 0.0672 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:20:22] 	Iter 674800 Done. | loss1: 0.1700 | loss_class: 0.1693 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:21:13] 	Iter 674900 Done. | loss1: 0.0371 | loss_class: 0.0365 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|10:22:03] 	Iter 675000 Done. | loss1: 0.0377 | loss_class: 0.0371 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:22:54] 	Iter 675100 Done. | loss1: 0.0206 | loss_class: 0.0201 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|10:23:46] 	Iter 675200 Done. | loss1: 0.0621 | loss_class: 0.0616 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|10:24:38] 	Iter 675300 Done. | loss1: 0.0679 | loss_class: 0.0670 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|10:25:29] 	Iter 675400 Done. | loss1: 0.0071 | loss_class: 0.0063 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|10:26:19] 	Iter 675500 Done. | loss1: 0.1163 | loss_class: 0.1156 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:27:12] 	Iter 675600 Done. | loss1: 0.0048 | loss_class: 0.0042 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:28:05] 	Iter 675700 Done. | loss1: 0.0541 | loss_class: 0.0535 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:28:56] 	Iter 675800 Done. | loss1: 0.4694 | loss_class: 0.4690 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|10:29:46] 	Iter 675900 Done. | loss1: 0.0488 | loss_class: 0.0483 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|10:30:37] 	Iter 676000 Done. | loss1: 0.4702 | loss_class: 0.4695 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|10:31:27] 	Iter 676100 Done. | loss1: 0.0012 | loss_class: 0.0004 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|10:32:18] 	Iter 676200 Done. | loss1: 0.3678 | loss_class: 0.3671 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|10:33:09] 	Iter 676300 Done. | loss1: 0.0300 | loss_class: 0.0295 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:34:05] 	Iter 676400 Done. | loss1: 0.8052 | loss_class: 0.8045 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|10:34:56] 	Iter 676500 Done. | loss1: 0.0048 | loss_class: 0.0041 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|10:35:49] 	Iter 676600 Done. | loss1: 0.0728 | loss_class: 0.0720 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|10:36:39] 	Iter 676700 Done. | loss1: 0.2406 | loss_class: 0.2402 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|10:37:31] 	Iter 676800 Done. | loss1: 0.0161 | loss_class: 0.0155 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:38:23] 	Iter 676900 Done. | loss1: 0.0153 | loss_class: 0.0147 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:39:14] 	Iter 677000 Done. | loss1: 0.0124 | loss_class: 0.0119 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|10:40:04] 	Iter 677100 Done. | loss1: 0.0096 | loss_class: 0.0088 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|10:40:55] 	Iter 677200 Done. | loss1: 0.0040 | loss_class: 0.0034 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:41:47] 	Iter 677300 Done. | loss1: 0.0329 | loss_class: 0.0323 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:42:39] 	Iter 677400 Done. | loss1: 0.0038 | loss_class: 0.0031 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|10:43:30] 	Iter 677500 Done. | loss1: 0.0138 | loss_class: 0.0132 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:44:20] 	Iter 677600 Done. | loss1: 0.0443 | loss_class: 0.0436 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|10:45:11] 	Iter 677700 Done. | loss1: 0.0030 | loss_class: 0.0024 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:46:02] 	Iter 677800 Done. | loss1: 0.7529 | loss_class: 0.7524 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|10:46:52] 	Iter 677900 Done. | loss1: 0.0141 | loss_class: 0.0133 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|10:47:42] 	Iter 678000 Done. | loss1: 0.0851 | loss_class: 0.0844 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|10:48:32] 	Iter 678100 Done. | loss1: 0.2702 | loss_class: 0.2695 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|10:49:23] 	Iter 678200 Done. | loss1: 0.1865 | loss_class: 0.1857 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|10:50:13] 	Iter 678300 Done. | loss1: 0.0482 | loss_class: 0.0476 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:51:07] 	Iter 678400 Done. | loss1: 0.0223 | loss_class: 0.0217 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:52:02] 	Iter 678500 Done. | loss1: 0.0031 | loss_class: 0.0023 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|10:52:53] 	Iter 678600 Done. | loss1: 0.0042 | loss_class: 0.0034 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|10:53:43] 	Iter 678700 Done. | loss1: 0.0011 | loss_class: 0.0005 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:54:35] 	Iter 678800 Done. | loss1: 0.3082 | loss_class: 0.3075 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|10:55:30] 	Iter 678900 Done. | loss1: 0.0188 | loss_class: 0.0180 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|10:56:20] 	Iter 679000 Done. | loss1: 0.0045 | loss_class: 0.0036 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|10:57:10] 	Iter 679100 Done. | loss1: 0.0947 | loss_class: 0.0942 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|10:58:01] 	Iter 679200 Done. | loss1: 0.0088 | loss_class: 0.0081 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:58:55] 	Iter 679300 Done. | loss1: 0.0033 | loss_class: 0.0027 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|10:59:46] 	Iter 679400 Done. | loss1: 0.0100 | loss_class: 0.0092 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|11:00:36] 	Iter 679500 Done. | loss1: 0.0390 | loss_class: 0.0384 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:01:27] 	Iter 679600 Done. | loss1: 0.0838 | loss_class: 0.0833 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|11:02:18] 	Iter 679700 Done. | loss1: 0.0100 | loss_class: 0.0091 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|11:03:09] 	Iter 679800 Done. | loss1: 0.0075 | loss_class: 0.0067 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|11:04:01] 	Iter 679900 Done. | loss1: 0.4125 | loss_class: 0.4120 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|11:04:52] 	Iter 680000 Done. | loss1: 0.0064 | loss_class: 0.0058 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:05:42] 	Iter 680100 Done. | loss1: 0.0265 | loss_class: 0.0259 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:06:35] 	Iter 680200 Done. | loss1: 0.3683 | loss_class: 0.3676 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:07:25] 	Iter 680300 Done. | loss1: 0.0110 | loss_class: 0.0105 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|11:08:16] 	Iter 680400 Done. | loss1: 0.2327 | loss_class: 0.2318 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|11:09:06] 	Iter 680500 Done. | loss1: 0.0375 | loss_class: 0.0367 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:09:58] 	Iter 680600 Done. | loss1: 0.0683 | loss_class: 0.0677 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:10:49] 	Iter 680700 Done. | loss1: 0.0261 | loss_class: 0.0254 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:11:39] 	Iter 680800 Done. | loss1: 0.0906 | loss_class: 0.0902 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|11:12:29] 	Iter 680900 Done. | loss1: 0.5584 | loss_class: 0.5578 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:13:20] 	Iter 681000 Done. | loss1: 0.2301 | loss_class: 0.2294 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:14:10] 	Iter 681100 Done. | loss1: 0.0066 | loss_class: 0.0061 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:15:01] 	Iter 681200 Done. | loss1: 0.0394 | loss_class: 0.0388 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|11:15:52] 	Iter 681300 Done. | loss1: 0.0069 | loss_class: 0.0062 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:16:43] 	Iter 681400 Done. | loss1: 0.0157 | loss_class: 0.0151 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|11:17:33] 	mean_loss1: 0.11860204234182874
[06.26.21|11:17:33] 	mean_loss_class: 0.11795643057171436
[06.26.21|11:17:33] 	mean_loss_recon: 0.0006456117682765581
[06.26.21|11:17:33] Time consumption:
[06.26.21|11:17:33] Done.
[06.26.21|11:17:33] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch77_model1.pt.
[06.26.21|11:17:33] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch77_model2.pt.
[06.26.21|11:17:33] Training epoch: 78
[06.26.21|11:17:36] 	Iter 681500 Done. | loss1: 0.0011 | loss_class: 0.0005 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:18:27] 	Iter 681600 Done. | loss1: 0.0577 | loss_class: 0.0571 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:19:18] 	Iter 681700 Done. | loss1: 0.0027 | loss_class: 0.0022 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|11:20:08] 	Iter 681800 Done. | loss1: 0.1001 | loss_class: 0.0995 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:20:58] 	Iter 681900 Done. | loss1: 0.1264 | loss_class: 0.1257 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:21:50] 	Iter 682000 Done. | loss1: 0.0232 | loss_class: 0.0225 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:22:41] 	Iter 682100 Done. | loss1: 0.4931 | loss_class: 0.4924 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:23:32] 	Iter 682200 Done. | loss1: 0.0653 | loss_class: 0.0647 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:24:23] 	Iter 682300 Done. | loss1: 0.1481 | loss_class: 0.1474 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:25:13] 	Iter 682400 Done. | loss1: 0.0093 | loss_class: 0.0088 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:26:04] 	Iter 682500 Done. | loss1: 0.0297 | loss_class: 0.0290 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:26:56] 	Iter 682600 Done. | loss1: 0.2035 | loss_class: 0.2029 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:27:47] 	Iter 682700 Done. | loss1: 0.2047 | loss_class: 0.2041 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:28:38] 	Iter 682800 Done. | loss1: 0.0081 | loss_class: 0.0074 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:29:28] 	Iter 682900 Done. | loss1: 0.0567 | loss_class: 0.0560 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:30:19] 	Iter 683000 Done. | loss1: 0.2204 | loss_class: 0.2197 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:31:11] 	Iter 683100 Done. | loss1: 0.4877 | loss_class: 0.4871 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:32:05] 	Iter 683200 Done. | loss1: 0.0190 | loss_class: 0.0184 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:32:56] 	Iter 683300 Done. | loss1: 0.1816 | loss_class: 0.1812 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|11:33:49] 	Iter 683400 Done. | loss1: 0.0047 | loss_class: 0.0042 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|11:34:39] 	Iter 683500 Done. | loss1: 0.0470 | loss_class: 0.0463 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:35:30] 	Iter 683600 Done. | loss1: 0.0220 | loss_class: 0.0212 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|11:36:21] 	Iter 683700 Done. | loss1: 0.0020 | loss_class: 0.0014 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:37:11] 	Iter 683800 Done. | loss1: 0.0295 | loss_class: 0.0291 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|11:38:06] 	Iter 683900 Done. | loss1: 0.0101 | loss_class: 0.0095 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:38:58] 	Iter 684000 Done. | loss1: 0.0592 | loss_class: 0.0587 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|11:39:49] 	Iter 684100 Done. | loss1: 0.0159 | loss_class: 0.0152 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:40:40] 	Iter 684200 Done. | loss1: 0.0556 | loss_class: 0.0549 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|11:41:31] 	Iter 684300 Done. | loss1: 0.0807 | loss_class: 0.0802 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|11:42:21] 	Iter 684400 Done. | loss1: 0.0140 | loss_class: 0.0134 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:43:12] 	Iter 684500 Done. | loss1: 0.0856 | loss_class: 0.0850 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|11:44:02] 	Iter 684600 Done. | loss1: 0.0482 | loss_class: 0.0476 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|11:44:56] 	Iter 684700 Done. | loss1: 0.0018 | loss_class: 0.0011 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:45:50] 	Iter 684800 Done. | loss1: 0.1007 | loss_class: 0.1001 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:46:41] 	Iter 684900 Done. | loss1: 0.0319 | loss_class: 0.0311 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|11:47:32] 	Iter 685000 Done. | loss1: 0.0021 | loss_class: 0.0014 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:48:22] 	Iter 685100 Done. | loss1: 0.0106 | loss_class: 0.0100 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:49:12] 	Iter 685200 Done. | loss1: 0.0431 | loss_class: 0.0424 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:50:03] 	Iter 685300 Done. | loss1: 0.0024 | loss_class: 0.0013 | loss_recon: 0.0011 | lr: 0.001000
[06.26.21|11:50:54] 	Iter 685400 Done. | loss1: 0.0031 | loss_class: 0.0023 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|11:51:44] 	Iter 685500 Done. | loss1: 1.0243 | loss_class: 1.0237 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:52:35] 	Iter 685600 Done. | loss1: 0.4092 | loss_class: 0.4086 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:53:25] 	Iter 685700 Done. | loss1: 0.1959 | loss_class: 0.1953 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:54:15] 	Iter 685800 Done. | loss1: 0.9268 | loss_class: 0.9262 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:55:06] 	Iter 685900 Done. | loss1: 0.0501 | loss_class: 0.0494 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:55:57] 	Iter 686000 Done. | loss1: 0.1178 | loss_class: 0.1172 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:56:48] 	Iter 686100 Done. | loss1: 0.0051 | loss_class: 0.0045 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|11:57:39] 	Iter 686200 Done. | loss1: 0.0057 | loss_class: 0.0050 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:58:33] 	Iter 686300 Done. | loss1: 0.1765 | loss_class: 0.1758 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|11:59:28] 	Iter 686400 Done. | loss1: 0.3608 | loss_class: 0.3599 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|12:00:24] 	Iter 686500 Done. | loss1: 0.0357 | loss_class: 0.0350 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:01:26] 	Iter 686600 Done. | loss1: 0.3377 | loss_class: 0.3372 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:02:25] 	Iter 686700 Done. | loss1: 1.2409 | loss_class: 1.2404 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:03:16] 	Iter 686800 Done. | loss1: 0.0473 | loss_class: 0.0468 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:04:07] 	Iter 686900 Done. | loss1: 0.0395 | loss_class: 0.0389 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:04:58] 	Iter 687000 Done. | loss1: 0.0102 | loss_class: 0.0095 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:05:48] 	Iter 687100 Done. | loss1: 0.0920 | loss_class: 0.0914 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:06:41] 	Iter 687200 Done. | loss1: 0.0062 | loss_class: 0.0055 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:07:32] 	Iter 687300 Done. | loss1: 0.0061 | loss_class: 0.0056 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:08:22] 	Iter 687400 Done. | loss1: 0.0187 | loss_class: 0.0182 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:09:12] 	Iter 687500 Done. | loss1: 0.4073 | loss_class: 0.4066 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:10:04] 	Iter 687600 Done. | loss1: 0.1066 | loss_class: 0.1061 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:10:59] 	Iter 687700 Done. | loss1: 0.1345 | loss_class: 0.1338 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:11:52] 	Iter 687800 Done. | loss1: 0.0087 | loss_class: 0.0081 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:12:43] 	Iter 687900 Done. | loss1: 0.3694 | loss_class: 0.3688 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:13:34] 	Iter 688000 Done. | loss1: 0.2980 | loss_class: 0.2973 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:14:25] 	Iter 688100 Done. | loss1: 0.0138 | loss_class: 0.0132 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:15:15] 	Iter 688200 Done. | loss1: 0.0288 | loss_class: 0.0282 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:16:06] 	Iter 688300 Done. | loss1: 0.0082 | loss_class: 0.0075 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:16:57] 	Iter 688400 Done. | loss1: 0.0140 | loss_class: 0.0135 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:17:48] 	Iter 688500 Done. | loss1: 0.1347 | loss_class: 0.1342 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:18:38] 	Iter 688600 Done. | loss1: 0.1044 | loss_class: 0.1038 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:19:29] 	Iter 688700 Done. | loss1: 0.0074 | loss_class: 0.0067 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:20:20] 	Iter 688800 Done. | loss1: 0.0066 | loss_class: 0.0059 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:21:10] 	Iter 688900 Done. | loss1: 0.1149 | loss_class: 0.1143 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:22:01] 	Iter 689000 Done. | loss1: 0.0063 | loss_class: 0.0059 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:22:52] 	Iter 689100 Done. | loss1: 0.0068 | loss_class: 0.0063 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:23:42] 	Iter 689200 Done. | loss1: 0.3280 | loss_class: 0.3274 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:24:32] 	Iter 689300 Done. | loss1: 0.1415 | loss_class: 0.1411 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:25:23] 	Iter 689400 Done. | loss1: 0.1872 | loss_class: 0.1867 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|12:26:12] 	Iter 689500 Done. | loss1: 0.2392 | loss_class: 0.2389 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|12:27:02] 	Iter 689600 Done. | loss1: 0.0059 | loss_class: 0.0053 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:27:53] 	Iter 689700 Done. | loss1: 0.0056 | loss_class: 0.0050 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:28:44] 	Iter 689800 Done. | loss1: 0.0349 | loss_class: 0.0342 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:29:38] 	Iter 689900 Done. | loss1: 0.4455 | loss_class: 0.4450 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:30:29] 	Iter 690000 Done. | loss1: 0.0055 | loss_class: 0.0048 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:31:18] 	Iter 690100 Done. | loss1: 1.1302 | loss_class: 1.1295 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:32:08] 	Iter 690200 Done. | loss1: 0.0031 | loss_class: 0.0024 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:32:58] 	Iter 690300 Done. | loss1: 0.0025 | loss_class: 0.0020 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:33:51] 	Iter 690400 Done. | loss1: 0.0103 | loss_class: 0.0097 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:34:44] 	Iter 690500 Done. | loss1: 0.1573 | loss_class: 0.1567 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:35:38] 	Iter 690600 Done. | loss1: 0.0391 | loss_class: 0.0384 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:36:32] 	Iter 690700 Done. | loss1: 0.0214 | loss_class: 0.0208 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:37:23] 	Iter 690800 Done. | loss1: 0.3548 | loss_class: 0.3543 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:38:13] 	Iter 690900 Done. | loss1: 0.1707 | loss_class: 0.1701 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:39:04] 	Iter 691000 Done. | loss1: 0.0570 | loss_class: 0.0563 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|12:39:54] 	Iter 691100 Done. | loss1: 0.0233 | loss_class: 0.0228 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:40:45] 	Iter 691200 Done. | loss1: 0.1011 | loss_class: 0.1005 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:41:36] 	Iter 691300 Done. | loss1: 0.0429 | loss_class: 0.0421 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|12:42:26] 	Iter 691400 Done. | loss1: 0.0446 | loss_class: 0.0441 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|12:43:17] 	Iter 691500 Done. | loss1: 0.7533 | loss_class: 0.7527 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:43:26] 	mean_loss1: 0.11129547109854535
[06.26.21|12:43:26] 	mean_loss_class: 0.11064974080284692
[06.26.21|12:43:26] 	mean_loss_recon: 0.0006457303577747454
[06.26.21|12:43:26] Time consumption:
[06.26.21|12:43:26] Done.
[06.26.21|12:43:26] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch78_model1.pt.
[06.26.21|12:43:26] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch78_model2.pt.
[06.26.21|12:43:26] Training epoch: 79
[06.26.21|12:44:09] 	Iter 691600 Done. | loss1: 0.1084 | loss_class: 0.1078 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:44:59] 	Iter 691700 Done. | loss1: 0.0658 | loss_class: 0.0651 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:45:54] 	Iter 691800 Done. | loss1: 0.0862 | loss_class: 0.0855 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:46:47] 	Iter 691900 Done. | loss1: 0.1308 | loss_class: 0.1301 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:47:38] 	Iter 692000 Done. | loss1: 0.0158 | loss_class: 0.0151 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:48:28] 	Iter 692100 Done. | loss1: 0.0097 | loss_class: 0.0090 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:49:19] 	Iter 692200 Done. | loss1: 0.0828 | loss_class: 0.0820 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|12:50:10] 	Iter 692300 Done. | loss1: 0.0625 | loss_class: 0.0621 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|12:51:01] 	Iter 692400 Done. | loss1: 0.0054 | loss_class: 0.0047 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:51:52] 	Iter 692500 Done. | loss1: 0.0283 | loss_class: 0.0279 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|12:52:43] 	Iter 692600 Done. | loss1: 0.3868 | loss_class: 0.3858 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|12:53:34] 	Iter 692700 Done. | loss1: 0.0107 | loss_class: 0.0100 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:54:25] 	Iter 692800 Done. | loss1: 0.0610 | loss_class: 0.0603 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:55:16] 	Iter 692900 Done. | loss1: 0.0008 | loss_class: 0.0001 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:56:06] 	Iter 693000 Done. | loss1: 0.4978 | loss_class: 0.4970 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|12:56:57] 	Iter 693100 Done. | loss1: 0.0051 | loss_class: 0.0044 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|12:57:48] 	Iter 693200 Done. | loss1: 0.0080 | loss_class: 0.0073 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|12:58:38] 	Iter 693300 Done. | loss1: 0.2324 | loss_class: 0.2317 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|12:59:29] 	Iter 693400 Done. | loss1: 0.2606 | loss_class: 0.2596 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|13:00:19] 	Iter 693500 Done. | loss1: 0.0029 | loss_class: 0.0021 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|13:01:09] 	Iter 693600 Done. | loss1: 0.1242 | loss_class: 0.1235 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:01:59] 	Iter 693700 Done. | loss1: 1.2395 | loss_class: 1.2388 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:02:49] 	Iter 693800 Done. | loss1: 0.0071 | loss_class: 0.0063 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|13:03:39] 	Iter 693900 Done. | loss1: 0.0432 | loss_class: 0.0426 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:04:30] 	Iter 694000 Done. | loss1: 0.0033 | loss_class: 0.0025 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|13:05:20] 	Iter 694100 Done. | loss1: 0.0123 | loss_class: 0.0117 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:06:10] 	Iter 694200 Done. | loss1: 0.0027 | loss_class: 0.0018 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|13:07:00] 	Iter 694300 Done. | loss1: 0.0406 | loss_class: 0.0400 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:07:50] 	Iter 694400 Done. | loss1: 0.0623 | loss_class: 0.0614 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|13:08:40] 	Iter 694500 Done. | loss1: 0.1635 | loss_class: 0.1626 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|13:09:30] 	Iter 694600 Done. | loss1: 0.1673 | loss_class: 0.1667 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:10:20] 	Iter 694700 Done. | loss1: 0.1024 | loss_class: 0.1018 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:11:14] 	Iter 694800 Done. | loss1: 0.0065 | loss_class: 0.0058 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:12:06] 	Iter 694900 Done. | loss1: 0.0084 | loss_class: 0.0078 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:12:58] 	Iter 695000 Done. | loss1: 0.1377 | loss_class: 0.1370 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:13:49] 	Iter 695100 Done. | loss1: 0.0175 | loss_class: 0.0169 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:14:39] 	Iter 695200 Done. | loss1: 0.1192 | loss_class: 0.1185 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:15:29] 	Iter 695300 Done. | loss1: 0.1344 | loss_class: 0.1337 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:16:19] 	Iter 695400 Done. | loss1: 0.0262 | loss_class: 0.0255 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:17:12] 	Iter 695500 Done. | loss1: 0.0016 | loss_class: 0.0007 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|13:18:02] 	Iter 695600 Done. | loss1: 0.0223 | loss_class: 0.0217 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:18:53] 	Iter 695700 Done. | loss1: 0.0037 | loss_class: 0.0031 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:19:43] 	Iter 695800 Done. | loss1: 0.0030 | loss_class: 0.0025 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|13:20:33] 	Iter 695900 Done. | loss1: 0.0063 | loss_class: 0.0055 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|13:21:24] 	Iter 696000 Done. | loss1: 0.0235 | loss_class: 0.0229 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:22:19] 	Iter 696100 Done. | loss1: 0.1471 | loss_class: 0.1466 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|13:23:13] 	Iter 696200 Done. | loss1: 0.0031 | loss_class: 0.0024 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:24:04] 	Iter 696300 Done. | loss1: 0.0197 | loss_class: 0.0190 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:25:00] 	Iter 696400 Done. | loss1: 0.0229 | loss_class: 0.0221 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:25:53] 	Iter 696500 Done. | loss1: 0.0013 | loss_class: 0.0005 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|13:26:44] 	Iter 696600 Done. | loss1: 0.0358 | loss_class: 0.0351 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:27:34] 	Iter 696700 Done. | loss1: 0.0013 | loss_class: 0.0008 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|13:28:25] 	Iter 696800 Done. | loss1: 0.1865 | loss_class: 0.1858 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:29:15] 	Iter 696900 Done. | loss1: 0.0153 | loss_class: 0.0146 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:30:04] 	Iter 697000 Done. | loss1: 0.0268 | loss_class: 0.0262 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:30:54] 	Iter 697100 Done. | loss1: 0.5742 | loss_class: 0.5734 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|13:31:47] 	Iter 697200 Done. | loss1: 0.0355 | loss_class: 0.0349 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:32:43] 	Iter 697300 Done. | loss1: 0.0122 | loss_class: 0.0115 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:33:36] 	Iter 697400 Done. | loss1: 0.3151 | loss_class: 0.3145 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:34:26] 	Iter 697500 Done. | loss1: 0.0084 | loss_class: 0.0079 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|13:35:21] 	Iter 697600 Done. | loss1: 0.1346 | loss_class: 0.1340 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:36:14] 	Iter 697700 Done. | loss1: 0.2321 | loss_class: 0.2315 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:37:05] 	Iter 697800 Done. | loss1: 0.0075 | loss_class: 0.0069 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:37:56] 	Iter 697900 Done. | loss1: 0.0072 | loss_class: 0.0067 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:38:46] 	Iter 698000 Done. | loss1: 0.0155 | loss_class: 0.0149 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:39:37] 	Iter 698100 Done. | loss1: 0.0080 | loss_class: 0.0073 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:40:27] 	Iter 698200 Done. | loss1: 0.0343 | loss_class: 0.0338 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|13:41:18] 	Iter 698300 Done. | loss1: 0.1143 | loss_class: 0.1136 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:42:11] 	Iter 698400 Done. | loss1: 0.0129 | loss_class: 0.0122 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:43:04] 	Iter 698500 Done. | loss1: 0.1699 | loss_class: 0.1693 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:43:55] 	Iter 698600 Done. | loss1: 0.0134 | loss_class: 0.0127 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:44:45] 	Iter 698700 Done. | loss1: 0.1628 | loss_class: 0.1620 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|13:45:35] 	Iter 698800 Done. | loss1: 0.0337 | loss_class: 0.0330 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|13:46:25] 	Iter 698900 Done. | loss1: 0.0071 | loss_class: 0.0065 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:47:16] 	Iter 699000 Done. | loss1: 0.0958 | loss_class: 0.0952 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:48:06] 	Iter 699100 Done. | loss1: 0.1357 | loss_class: 0.1352 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|13:48:57] 	Iter 699200 Done. | loss1: 0.0193 | loss_class: 0.0189 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|13:49:47] 	Iter 699300 Done. | loss1: 0.8464 | loss_class: 0.8458 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:50:38] 	Iter 699400 Done. | loss1: 0.0015 | loss_class: 0.0007 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:51:28] 	Iter 699500 Done. | loss1: 0.5424 | loss_class: 0.5416 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|13:52:19] 	Iter 699600 Done. | loss1: 0.0185 | loss_class: 0.0179 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|13:53:09] 	Iter 699700 Done. | loss1: 0.1207 | loss_class: 0.1202 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|13:53:59] 	Iter 699800 Done. | loss1: 0.0065 | loss_class: 0.0058 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|13:54:51] 	Iter 699900 Done. | loss1: 0.1658 | loss_class: 0.1651 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:55:46] 	Iter 700000 Done. | loss1: 0.0023 | loss_class: 0.0015 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:56:41] 	Iter 700100 Done. | loss1: 0.0062 | loss_class: 0.0058 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|13:57:36] 	Iter 700200 Done. | loss1: 0.1219 | loss_class: 0.1212 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:58:31] 	Iter 700300 Done. | loss1: 0.1469 | loss_class: 0.1462 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|13:59:21] 	Iter 700400 Done. | loss1: 0.0051 | loss_class: 0.0045 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:00:12] 	Iter 700500 Done. | loss1: 1.1229 | loss_class: 1.1224 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:01:03] 	Iter 700600 Done. | loss1: 0.0095 | loss_class: 0.0088 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:01:54] 	Iter 700700 Done. | loss1: 0.2678 | loss_class: 0.2673 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:02:48] 	Iter 700800 Done. | loss1: 0.0011 | loss_class: 0.0005 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:03:39] 	Iter 700900 Done. | loss1: 0.0011 | loss_class: 0.0005 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:04:30] 	Iter 701000 Done. | loss1: 0.1710 | loss_class: 0.1703 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:05:20] 	Iter 701100 Done. | loss1: 0.0085 | loss_class: 0.0080 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:06:10] 	Iter 701200 Done. | loss1: 0.0749 | loss_class: 0.0742 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:06:59] 	Iter 701300 Done. | loss1: 0.0058 | loss_class: 0.0050 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:07:49] 	Iter 701400 Done. | loss1: 0.1161 | loss_class: 0.1153 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|14:08:40] 	Iter 701500 Done. | loss1: 0.1576 | loss_class: 0.1571 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:08:59] 	mean_loss1: 0.10756001613828085
[06.26.21|14:08:59] 	mean_loss_class: 0.10691299990407417
[06.26.21|14:08:59] 	mean_loss_recon: 0.0006470162218324715
[06.26.21|14:08:59] Time consumption:
[06.26.21|14:08:59] Done.
[06.26.21|14:08:59] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch79_model1.pt.
[06.26.21|14:08:59] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch79_model2.pt.
[06.26.21|14:08:59] Eval epoch: 79
[06.26.21|14:15:28] 	mean_loss1: 0.631544004057267
[06.26.21|14:15:28] 	mean_loss_class: 0.6311765241674906
[06.26.21|14:15:28] 	mean_loss_recon: 0.03674800247098355
[06.26.21|14:15:28] 

[06.26.21|14:15:28] 	Top1: 84.87%
[06.26.21|14:15:28] 

[06.26.21|14:15:28] 	Top5: 97.19%
[06.26.21|14:15:28] Done.
[06.26.21|14:15:28] Training epoch: 80
[06.26.21|14:16:00] 	Iter 701600 Done. | loss1: 0.0203 | loss_class: 0.0197 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:16:50] 	Iter 701700 Done. | loss1: 0.0090 | loss_class: 0.0085 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:17:41] 	Iter 701800 Done. | loss1: 0.0849 | loss_class: 0.0842 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:18:32] 	Iter 701900 Done. | loss1: 0.0865 | loss_class: 0.0856 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|14:19:22] 	Iter 702000 Done. | loss1: 0.1157 | loss_class: 0.1152 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:20:14] 	Iter 702100 Done. | loss1: 0.0123 | loss_class: 0.0117 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:21:04] 	Iter 702200 Done. | loss1: 0.0150 | loss_class: 0.0142 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|14:21:55] 	Iter 702300 Done. | loss1: 0.0040 | loss_class: 0.0032 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|14:22:46] 	Iter 702400 Done. | loss1: 0.1295 | loss_class: 0.1290 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:23:36] 	Iter 702500 Done. | loss1: 0.0068 | loss_class: 0.0062 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:24:32] 	Iter 702600 Done. | loss1: 0.0058 | loss_class: 0.0051 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:25:26] 	Iter 702700 Done. | loss1: 0.0155 | loss_class: 0.0149 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:26:17] 	Iter 702800 Done. | loss1: 0.0068 | loss_class: 0.0062 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:27:07] 	Iter 702900 Done. | loss1: 0.0542 | loss_class: 0.0534 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:27:58] 	Iter 703000 Done. | loss1: 0.0018 | loss_class: 0.0013 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:28:49] 	Iter 703100 Done. | loss1: 0.0012 | loss_class: 0.0005 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:29:40] 	Iter 703200 Done. | loss1: 0.2074 | loss_class: 0.2068 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:30:31] 	Iter 703300 Done. | loss1: 0.0130 | loss_class: 0.0124 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:31:24] 	Iter 703400 Done. | loss1: 0.4481 | loss_class: 0.4476 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:32:23] 	Iter 703500 Done. | loss1: 0.0691 | loss_class: 0.0686 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:33:30] 	Iter 703600 Done. | loss1: 0.0043 | loss_class: 0.0036 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:34:21] 	Iter 703700 Done. | loss1: 0.0083 | loss_class: 0.0078 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:35:17] 	Iter 703800 Done. | loss1: 0.0277 | loss_class: 0.0272 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:36:27] 	Iter 703900 Done. | loss1: 0.0802 | loss_class: 0.0795 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:37:38] 	Iter 704000 Done. | loss1: 0.0045 | loss_class: 0.0038 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|14:38:48] 	Iter 704100 Done. | loss1: 0.0068 | loss_class: 0.0061 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:39:58] 	Iter 704200 Done. | loss1: 0.0015 | loss_class: 0.0009 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:41:09] 	Iter 704300 Done. | loss1: 0.0110 | loss_class: 0.0102 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|14:42:19] 	Iter 704400 Done. | loss1: 0.0008 | loss_class: 0.0002 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:43:30] 	Iter 704500 Done. | loss1: 0.0066 | loss_class: 0.0060 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:44:41] 	Iter 704600 Done. | loss1: 0.0064 | loss_class: 0.0056 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|14:45:51] 	Iter 704700 Done. | loss1: 0.0206 | loss_class: 0.0201 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:47:02] 	Iter 704800 Done. | loss1: 0.0076 | loss_class: 0.0069 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:48:13] 	Iter 704900 Done. | loss1: 0.0940 | loss_class: 0.0935 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:49:23] 	Iter 705000 Done. | loss1: 0.2254 | loss_class: 0.2249 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:50:34] 	Iter 705100 Done. | loss1: 0.0069 | loss_class: 0.0063 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:51:45] 	Iter 705200 Done. | loss1: 0.0329 | loss_class: 0.0322 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:52:54] 	Iter 705300 Done. | loss1: 0.0792 | loss_class: 0.0785 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|14:54:05] 	Iter 705400 Done. | loss1: 0.0390 | loss_class: 0.0383 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:55:15] 	Iter 705500 Done. | loss1: 0.0027 | loss_class: 0.0020 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:56:26] 	Iter 705600 Done. | loss1: 0.0143 | loss_class: 0.0137 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:57:36] 	Iter 705700 Done. | loss1: 0.0612 | loss_class: 0.0606 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|14:58:46] 	Iter 705800 Done. | loss1: 0.0346 | loss_class: 0.0340 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|14:59:57] 	Iter 705900 Done. | loss1: 0.0141 | loss_class: 0.0134 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:01:08] 	Iter 706000 Done. | loss1: 0.0397 | loss_class: 0.0391 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|15:02:18] 	Iter 706100 Done. | loss1: 0.0330 | loss_class: 0.0323 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:03:29] 	Iter 706200 Done. | loss1: 0.0184 | loss_class: 0.0179 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|15:04:40] 	Iter 706300 Done. | loss1: 0.0270 | loss_class: 0.0261 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|15:05:52] 	Iter 706400 Done. | loss1: 0.0293 | loss_class: 0.0285 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|15:07:02] 	Iter 706500 Done. | loss1: 0.0209 | loss_class: 0.0202 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|15:08:12] 	Iter 706600 Done. | loss1: 0.1353 | loss_class: 0.1347 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|15:09:23] 	Iter 706700 Done. | loss1: 0.2564 | loss_class: 0.2558 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:10:33] 	Iter 706800 Done. | loss1: 0.0623 | loss_class: 0.0615 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|15:11:45] 	Iter 706900 Done. | loss1: 0.2645 | loss_class: 0.2641 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|15:12:56] 	Iter 707000 Done. | loss1: 0.0185 | loss_class: 0.0178 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:14:06] 	Iter 707100 Done. | loss1: 0.0217 | loss_class: 0.0210 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:15:17] 	Iter 707200 Done. | loss1: 0.0150 | loss_class: 0.0145 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:16:29] 	Iter 707300 Done. | loss1: 0.0112 | loss_class: 0.0105 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:17:41] 	Iter 707400 Done. | loss1: 0.0558 | loss_class: 0.0551 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:18:51] 	Iter 707500 Done. | loss1: 0.1327 | loss_class: 0.1321 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:19:59] 	Iter 707600 Done. | loss1: 0.0216 | loss_class: 0.0210 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:21:08] 	Iter 707700 Done. | loss1: 0.1125 | loss_class: 0.1117 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|15:22:18] 	Iter 707800 Done. | loss1: 0.0062 | loss_class: 0.0056 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:23:29] 	Iter 707900 Done. | loss1: 0.0215 | loss_class: 0.0207 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:24:40] 	Iter 708000 Done. | loss1: 0.4809 | loss_class: 0.4802 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:25:52] 	Iter 708100 Done. | loss1: 0.0044 | loss_class: 0.0037 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:27:02] 	Iter 708200 Done. | loss1: 0.0056 | loss_class: 0.0050 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:28:12] 	Iter 708300 Done. | loss1: 0.0293 | loss_class: 0.0287 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:29:22] 	Iter 708400 Done. | loss1: 0.0587 | loss_class: 0.0580 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:30:32] 	Iter 708500 Done. | loss1: 0.0177 | loss_class: 0.0170 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:31:43] 	Iter 708600 Done. | loss1: 0.0412 | loss_class: 0.0406 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|15:32:54] 	Iter 708700 Done. | loss1: 0.0082 | loss_class: 0.0076 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:34:04] 	Iter 708800 Done. | loss1: 0.0146 | loss_class: 0.0139 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:35:14] 	Iter 708900 Done. | loss1: 0.0456 | loss_class: 0.0449 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:36:25] 	Iter 709000 Done. | loss1: 0.0072 | loss_class: 0.0064 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:37:35] 	Iter 709100 Done. | loss1: 0.0163 | loss_class: 0.0156 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:38:46] 	Iter 709200 Done. | loss1: 0.0080 | loss_class: 0.0075 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|15:39:57] 	Iter 709300 Done. | loss1: 0.0083 | loss_class: 0.0075 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|15:41:08] 	Iter 709400 Done. | loss1: 0.0183 | loss_class: 0.0175 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:42:18] 	Iter 709500 Done. | loss1: 0.0149 | loss_class: 0.0142 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:43:29] 	Iter 709600 Done. | loss1: 0.0047 | loss_class: 0.0041 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:44:39] 	Iter 709700 Done. | loss1: 0.0048 | loss_class: 0.0041 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:45:50] 	Iter 709800 Done. | loss1: 0.2706 | loss_class: 0.2701 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|15:47:00] 	Iter 709900 Done. | loss1: 0.8843 | loss_class: 0.8836 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:48:11] 	Iter 710000 Done. | loss1: 0.0200 | loss_class: 0.0192 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|15:49:22] 	Iter 710100 Done. | loss1: 0.0019 | loss_class: 0.0013 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:50:32] 	Iter 710200 Done. | loss1: 0.0089 | loss_class: 0.0082 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:51:43] 	Iter 710300 Done. | loss1: 0.0153 | loss_class: 0.0146 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:52:54] 	Iter 710400 Done. | loss1: 0.0370 | loss_class: 0.0363 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:54:05] 	Iter 710500 Done. | loss1: 0.0015 | loss_class: 0.0010 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|15:55:15] 	Iter 710600 Done. | loss1: 0.7519 | loss_class: 0.7512 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|15:56:25] 	Iter 710700 Done. | loss1: 0.3120 | loss_class: 0.3116 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|15:57:36] 	Iter 710800 Done. | loss1: 0.1416 | loss_class: 0.1410 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:58:46] 	Iter 710900 Done. | loss1: 0.0493 | loss_class: 0.0487 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|15:59:57] 	Iter 711000 Done. | loss1: 0.0155 | loss_class: 0.0149 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:01:08] 	Iter 711100 Done. | loss1: 0.0707 | loss_class: 0.0701 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:02:19] 	Iter 711200 Done. | loss1: 0.0438 | loss_class: 0.0430 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|16:03:29] 	Iter 711300 Done. | loss1: 0.0087 | loss_class: 0.0081 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:04:40] 	Iter 711400 Done. | loss1: 0.0066 | loss_class: 0.0060 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:05:51] 	Iter 711500 Done. | loss1: 0.1360 | loss_class: 0.1354 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:06:35] 	mean_loss1: 0.10170066465457661
[06.26.21|16:06:35] 	mean_loss_class: 0.10105470462964197
[06.26.21|16:06:35] 	mean_loss_recon: 0.0006459600803620907
[06.26.21|16:06:35] Time consumption:
[06.26.21|16:06:35] Done.
[06.26.21|16:06:35] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch80_model1.pt.
[06.26.21|16:06:35] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch80_model2.pt.
[06.26.21|16:06:35] Training epoch: 81
[06.26.21|16:07:03] 	Iter 711600 Done. | loss1: 0.0010 | loss_class: 0.0003 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|16:08:12] 	Iter 711700 Done. | loss1: 0.0292 | loss_class: 0.0286 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:09:23] 	Iter 711800 Done. | loss1: 0.2667 | loss_class: 0.2661 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:10:33] 	Iter 711900 Done. | loss1: 0.1247 | loss_class: 0.1242 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|16:11:43] 	Iter 712000 Done. | loss1: 0.0063 | loss_class: 0.0056 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:12:54] 	Iter 712100 Done. | loss1: 0.7122 | loss_class: 0.7114 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|16:14:04] 	Iter 712200 Done. | loss1: 0.0591 | loss_class: 0.0583 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|16:15:15] 	Iter 712300 Done. | loss1: 0.0029 | loss_class: 0.0021 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|16:16:25] 	Iter 712400 Done. | loss1: 0.0416 | loss_class: 0.0410 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:17:35] 	Iter 712500 Done. | loss1: 0.0717 | loss_class: 0.0711 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:18:45] 	Iter 712600 Done. | loss1: 0.1588 | loss_class: 0.1583 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:19:56] 	Iter 712700 Done. | loss1: 0.3080 | loss_class: 0.3074 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|16:21:06] 	Iter 712800 Done. | loss1: 0.0074 | loss_class: 0.0066 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|16:22:16] 	Iter 712900 Done. | loss1: 0.0077 | loss_class: 0.0071 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|16:23:27] 	Iter 713000 Done. | loss1: 0.0094 | loss_class: 0.0089 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:24:37] 	Iter 713100 Done. | loss1: 0.1557 | loss_class: 0.1551 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:25:48] 	Iter 713200 Done. | loss1: 0.0126 | loss_class: 0.0120 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:26:59] 	Iter 713300 Done. | loss1: 0.0195 | loss_class: 0.0191 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|16:28:10] 	Iter 713400 Done. | loss1: 0.0077 | loss_class: 0.0070 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|16:29:21] 	Iter 713500 Done. | loss1: 0.0239 | loss_class: 0.0231 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|16:30:31] 	Iter 713600 Done. | loss1: 0.0016 | loss_class: 0.0010 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:31:42] 	Iter 713700 Done. | loss1: 0.0410 | loss_class: 0.0402 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|16:32:53] 	Iter 713800 Done. | loss1: 0.0153 | loss_class: 0.0146 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|16:34:04] 	Iter 713900 Done. | loss1: 0.0830 | loss_class: 0.0823 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|16:35:15] 	Iter 714000 Done. | loss1: 0.2242 | loss_class: 0.2236 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:36:25] 	Iter 714100 Done. | loss1: 0.2966 | loss_class: 0.2960 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:37:36] 	Iter 714200 Done. | loss1: 0.0134 | loss_class: 0.0126 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|16:38:45] 	Iter 714300 Done. | loss1: 0.7397 | loss_class: 0.7390 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:39:54] 	Iter 714400 Done. | loss1: 0.0532 | loss_class: 0.0527 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|16:41:03] 	Iter 714500 Done. | loss1: 0.0264 | loss_class: 0.0256 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|16:42:09] 	Iter 714600 Done. | loss1: 0.0009 | loss_class: 0.0004 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|16:43:08] 	Iter 714700 Done. | loss1: 0.0067 | loss_class: 0.0058 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|16:44:05] 	Iter 714800 Done. | loss1: 0.0660 | loss_class: 0.0653 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|16:45:02] 	Iter 714900 Done. | loss1: 0.0795 | loss_class: 0.0789 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:45:59] 	Iter 715000 Done. | loss1: 0.2898 | loss_class: 0.2893 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:46:57] 	Iter 715100 Done. | loss1: 0.0271 | loss_class: 0.0265 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:47:56] 	Iter 715200 Done. | loss1: 0.0093 | loss_class: 0.0086 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|16:48:53] 	Iter 715300 Done. | loss1: 0.0051 | loss_class: 0.0046 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:49:50] 	Iter 715400 Done. | loss1: 0.0387 | loss_class: 0.0379 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|16:50:48] 	Iter 715500 Done. | loss1: 0.0045 | loss_class: 0.0040 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:51:45] 	Iter 715600 Done. | loss1: 0.0041 | loss_class: 0.0033 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|16:52:43] 	Iter 715700 Done. | loss1: 0.3108 | loss_class: 0.3102 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:53:40] 	Iter 715800 Done. | loss1: 0.0048 | loss_class: 0.0044 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|16:54:38] 	Iter 715900 Done. | loss1: 0.1737 | loss_class: 0.1732 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|16:55:35] 	Iter 716000 Done. | loss1: 0.0008 | loss_class: 0.0003 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|16:56:32] 	Iter 716100 Done. | loss1: 0.0231 | loss_class: 0.0224 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|16:57:29] 	Iter 716200 Done. | loss1: 0.0099 | loss_class: 0.0093 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|16:58:25] 	Iter 716300 Done. | loss1: 0.0151 | loss_class: 0.0146 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|16:59:21] 	Iter 716400 Done. | loss1: 0.1248 | loss_class: 0.1242 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:00:18] 	Iter 716500 Done. | loss1: 0.0359 | loss_class: 0.0352 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:01:15] 	Iter 716600 Done. | loss1: 0.0132 | loss_class: 0.0125 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:02:13] 	Iter 716700 Done. | loss1: 0.0053 | loss_class: 0.0046 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:03:10] 	Iter 716800 Done. | loss1: 0.0026 | loss_class: 0.0020 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|17:04:09] 	Iter 716900 Done. | loss1: 0.0117 | loss_class: 0.0109 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|17:05:07] 	Iter 717000 Done. | loss1: 0.0332 | loss_class: 0.0325 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:06:04] 	Iter 717100 Done. | loss1: 0.5098 | loss_class: 0.5092 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:07:01] 	Iter 717200 Done. | loss1: 0.0113 | loss_class: 0.0105 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:07:59] 	Iter 717300 Done. | loss1: 0.0195 | loss_class: 0.0188 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:08:56] 	Iter 717400 Done. | loss1: 0.0919 | loss_class: 0.0913 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:09:53] 	Iter 717500 Done. | loss1: 0.0685 | loss_class: 0.0679 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:10:50] 	Iter 717600 Done. | loss1: 0.0154 | loss_class: 0.0148 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:11:48] 	Iter 717700 Done. | loss1: 0.0020 | loss_class: 0.0013 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:12:46] 	Iter 717800 Done. | loss1: 0.0061 | loss_class: 0.0053 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|17:13:44] 	Iter 717900 Done. | loss1: 0.0435 | loss_class: 0.0426 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|17:14:40] 	Iter 718000 Done. | loss1: 0.0119 | loss_class: 0.0112 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:15:38] 	Iter 718100 Done. | loss1: 0.0872 | loss_class: 0.0866 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:16:34] 	Iter 718200 Done. | loss1: 0.0826 | loss_class: 0.0819 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:17:30] 	Iter 718300 Done. | loss1: 0.0089 | loss_class: 0.0081 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|17:18:29] 	Iter 718400 Done. | loss1: 0.6506 | loss_class: 0.6500 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:19:27] 	Iter 718500 Done. | loss1: 0.0449 | loss_class: 0.0444 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|17:20:25] 	Iter 718600 Done. | loss1: 0.5867 | loss_class: 0.5860 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:21:23] 	Iter 718700 Done. | loss1: 0.0064 | loss_class: 0.0052 | loss_recon: 0.0012 | lr: 0.001000
[06.26.21|17:22:20] 	Iter 718800 Done. | loss1: 0.0076 | loss_class: 0.0070 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:23:17] 	Iter 718900 Done. | loss1: 0.0150 | loss_class: 0.0143 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:24:14] 	Iter 719000 Done. | loss1: 0.0281 | loss_class: 0.0274 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:25:11] 	Iter 719100 Done. | loss1: 0.2012 | loss_class: 0.2006 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:26:08] 	Iter 719200 Done. | loss1: 0.0039 | loss_class: 0.0033 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|17:27:07] 	Iter 719300 Done. | loss1: 0.0064 | loss_class: 0.0056 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|17:28:04] 	Iter 719400 Done. | loss1: 0.0100 | loss_class: 0.0094 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:29:01] 	Iter 719500 Done. | loss1: 0.0125 | loss_class: 0.0118 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:29:58] 	Iter 719600 Done. | loss1: 0.0037 | loss_class: 0.0031 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:30:56] 	Iter 719700 Done. | loss1: 0.1230 | loss_class: 0.1225 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|17:31:54] 	Iter 719800 Done. | loss1: 0.0327 | loss_class: 0.0318 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|17:32:54] 	Iter 719900 Done. | loss1: 0.0991 | loss_class: 0.0985 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:33:50] 	Iter 720000 Done. | loss1: 0.0166 | loss_class: 0.0159 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:34:48] 	Iter 720100 Done. | loss1: 0.0069 | loss_class: 0.0062 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:35:46] 	Iter 720200 Done. | loss1: 0.0305 | loss_class: 0.0301 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|17:36:43] 	Iter 720300 Done. | loss1: 0.0021 | loss_class: 0.0015 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:37:41] 	Iter 720400 Done. | loss1: 0.0266 | loss_class: 0.0259 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|17:38:39] 	Iter 720500 Done. | loss1: 0.1133 | loss_class: 0.1128 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|17:39:36] 	Iter 720600 Done. | loss1: 0.0103 | loss_class: 0.0096 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:40:33] 	Iter 720700 Done. | loss1: 0.0015 | loss_class: 0.0007 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:41:30] 	Iter 720800 Done. | loss1: 0.4240 | loss_class: 0.4233 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:42:28] 	Iter 720900 Done. | loss1: 0.0031 | loss_class: 0.0026 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|17:43:27] 	Iter 721000 Done. | loss1: 0.0112 | loss_class: 0.0106 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:44:23] 	Iter 721100 Done. | loss1: 0.0107 | loss_class: 0.0099 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:45:21] 	Iter 721200 Done. | loss1: 0.5982 | loss_class: 0.5973 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|17:46:19] 	Iter 721300 Done. | loss1: 0.0965 | loss_class: 0.0958 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:47:16] 	Iter 721400 Done. | loss1: 0.0333 | loss_class: 0.0327 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:48:13] 	Iter 721500 Done. | loss1: 0.1298 | loss_class: 0.1293 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|17:49:01] 	mean_loss1: 0.0963348348587624
[06.26.21|17:49:01] 	mean_loss_class: 0.09568931906964509
[06.26.21|17:49:01] 	mean_loss_recon: 0.0006455158480433581
[06.26.21|17:49:01] Time consumption:
[06.26.21|17:49:01] Done.
[06.26.21|17:49:01] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch81_model1.pt.
[06.26.21|17:49:01] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch81_model2.pt.
[06.26.21|17:49:01] Training epoch: 82
[06.26.21|17:49:11] 	Iter 721600 Done. | loss1: 0.0604 | loss_class: 0.0598 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:50:07] 	Iter 721700 Done. | loss1: 0.0032 | loss_class: 0.0026 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:51:04] 	Iter 721800 Done. | loss1: 0.0256 | loss_class: 0.0248 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|17:52:00] 	Iter 721900 Done. | loss1: 0.0105 | loss_class: 0.0099 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|17:52:58] 	Iter 722000 Done. | loss1: 0.1268 | loss_class: 0.1264 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|17:53:56] 	Iter 722100 Done. | loss1: 0.0118 | loss_class: 0.0112 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|17:54:53] 	Iter 722200 Done. | loss1: 0.0716 | loss_class: 0.0710 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:55:51] 	Iter 722300 Done. | loss1: 0.0046 | loss_class: 0.0038 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:56:47] 	Iter 722400 Done. | loss1: 0.0105 | loss_class: 0.0098 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:57:43] 	Iter 722500 Done. | loss1: 0.1113 | loss_class: 0.1107 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|17:58:40] 	Iter 722600 Done. | loss1: 0.0040 | loss_class: 0.0034 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|17:59:38] 	Iter 722700 Done. | loss1: 0.0291 | loss_class: 0.0286 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:00:36] 	Iter 722800 Done. | loss1: 0.0095 | loss_class: 0.0090 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:01:33] 	Iter 722900 Done. | loss1: 0.0072 | loss_class: 0.0065 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:02:30] 	Iter 723000 Done. | loss1: 0.0288 | loss_class: 0.0282 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:03:28] 	Iter 723100 Done. | loss1: 0.0086 | loss_class: 0.0079 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:04:26] 	Iter 723200 Done. | loss1: 0.0032 | loss_class: 0.0024 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:05:23] 	Iter 723300 Done. | loss1: 0.0673 | loss_class: 0.0667 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:06:20] 	Iter 723400 Done. | loss1: 0.0157 | loss_class: 0.0152 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:07:17] 	Iter 723500 Done. | loss1: 0.3636 | loss_class: 0.3631 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:08:13] 	Iter 723600 Done. | loss1: 1.0119 | loss_class: 1.0114 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:09:11] 	Iter 723700 Done. | loss1: 0.0567 | loss_class: 0.0561 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:10:08] 	Iter 723800 Done. | loss1: 0.0282 | loss_class: 0.0276 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:11:05] 	Iter 723900 Done. | loss1: 0.0057 | loss_class: 0.0051 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:12:03] 	Iter 724000 Done. | loss1: 0.0627 | loss_class: 0.0621 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:13:01] 	Iter 724100 Done. | loss1: 0.0016 | loss_class: 0.0010 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:13:58] 	Iter 724200 Done. | loss1: 0.0042 | loss_class: 0.0035 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:14:56] 	Iter 724300 Done. | loss1: 0.0147 | loss_class: 0.0142 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:15:54] 	Iter 724400 Done. | loss1: 0.4045 | loss_class: 0.4040 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:16:51] 	Iter 724500 Done. | loss1: 0.1155 | loss_class: 0.1149 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:17:47] 	Iter 724600 Done. | loss1: 0.9707 | loss_class: 0.9700 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:18:45] 	Iter 724700 Done. | loss1: 0.1824 | loss_class: 0.1818 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:19:42] 	Iter 724800 Done. | loss1: 0.0109 | loss_class: 0.0101 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:20:39] 	Iter 724900 Done. | loss1: 0.0036 | loss_class: 0.0030 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:21:37] 	Iter 725000 Done. | loss1: 0.0094 | loss_class: 0.0089 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:22:35] 	Iter 725100 Done. | loss1: 0.0035 | loss_class: 0.0028 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|18:23:33] 	Iter 725200 Done. | loss1: 0.0426 | loss_class: 0.0419 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:24:32] 	Iter 725300 Done. | loss1: 0.0021 | loss_class: 0.0015 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:25:28] 	Iter 725400 Done. | loss1: 0.0740 | loss_class: 0.0735 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:26:25] 	Iter 725500 Done. | loss1: 0.0022 | loss_class: 0.0016 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:27:22] 	Iter 725600 Done. | loss1: 0.5492 | loss_class: 0.5486 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:28:19] 	Iter 725700 Done. | loss1: 0.1480 | loss_class: 0.1472 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|18:29:17] 	Iter 725800 Done. | loss1: 0.1035 | loss_class: 0.1027 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:30:13] 	Iter 725900 Done. | loss1: 0.0018 | loss_class: 0.0014 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:31:09] 	Iter 726000 Done. | loss1: 0.0534 | loss_class: 0.0528 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:32:06] 	Iter 726100 Done. | loss1: 0.0692 | loss_class: 0.0685 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:33:03] 	Iter 726200 Done. | loss1: 0.0550 | loss_class: 0.0542 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|18:34:00] 	Iter 726300 Done. | loss1: 0.4087 | loss_class: 0.4080 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:34:57] 	Iter 726400 Done. | loss1: 0.2719 | loss_class: 0.2713 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:35:56] 	Iter 726500 Done. | loss1: 0.1375 | loss_class: 0.1368 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:36:54] 	Iter 726600 Done. | loss1: 0.0198 | loss_class: 0.0193 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:37:51] 	Iter 726700 Done. | loss1: 0.0170 | loss_class: 0.0163 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|18:38:48] 	Iter 726800 Done. | loss1: 0.0270 | loss_class: 0.0264 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:39:43] 	Iter 726900 Done. | loss1: 0.1140 | loss_class: 0.1133 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:40:40] 	Iter 727000 Done. | loss1: 0.0278 | loss_class: 0.0271 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:41:38] 	Iter 727100 Done. | loss1: 0.0073 | loss_class: 0.0067 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:42:35] 	Iter 727200 Done. | loss1: 0.0103 | loss_class: 0.0098 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:43:30] 	Iter 727300 Done. | loss1: 0.1289 | loss_class: 0.1281 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:44:28] 	Iter 727400 Done. | loss1: 0.0053 | loss_class: 0.0046 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:45:25] 	Iter 727500 Done. | loss1: 0.0012 | loss_class: 0.0005 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|18:46:23] 	Iter 727600 Done. | loss1: 0.1757 | loss_class: 0.1749 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|18:47:20] 	Iter 727700 Done. | loss1: 0.0039 | loss_class: 0.0031 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|18:48:16] 	Iter 727800 Done. | loss1: 0.0316 | loss_class: 0.0310 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:49:13] 	Iter 727900 Done. | loss1: 0.0285 | loss_class: 0.0279 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:50:10] 	Iter 728000 Done. | loss1: 0.0157 | loss_class: 0.0150 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:51:09] 	Iter 728100 Done. | loss1: 0.0392 | loss_class: 0.0386 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:52:07] 	Iter 728200 Done. | loss1: 0.0042 | loss_class: 0.0036 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:53:06] 	Iter 728300 Done. | loss1: 0.0038 | loss_class: 0.0033 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:54:01] 	Iter 728400 Done. | loss1: 0.1621 | loss_class: 0.1615 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:54:58] 	Iter 728500 Done. | loss1: 0.0338 | loss_class: 0.0332 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|18:55:55] 	Iter 728600 Done. | loss1: 0.0087 | loss_class: 0.0081 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|18:56:52] 	Iter 728700 Done. | loss1: 0.0087 | loss_class: 0.0083 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|18:57:49] 	Iter 728800 Done. | loss1: 0.0425 | loss_class: 0.0420 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:58:46] 	Iter 728900 Done. | loss1: 0.0806 | loss_class: 0.0800 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|18:59:44] 	Iter 729000 Done. | loss1: 0.3195 | loss_class: 0.3189 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:00:42] 	Iter 729100 Done. | loss1: 0.0280 | loss_class: 0.0273 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|19:01:41] 	Iter 729200 Done. | loss1: 0.0562 | loss_class: 0.0555 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:02:39] 	Iter 729300 Done. | loss1: 0.0057 | loss_class: 0.0048 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|19:03:35] 	Iter 729400 Done. | loss1: 0.0015 | loss_class: 0.0009 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:04:32] 	Iter 729500 Done. | loss1: 0.5711 | loss_class: 0.5706 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|19:05:30] 	Iter 729600 Done. | loss1: 0.2407 | loss_class: 0.2401 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:06:26] 	Iter 729700 Done. | loss1: 0.0626 | loss_class: 0.0618 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|19:07:24] 	Iter 729800 Done. | loss1: 0.0388 | loss_class: 0.0384 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|19:08:20] 	Iter 729900 Done. | loss1: 0.0116 | loss_class: 0.0109 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:09:18] 	Iter 730000 Done. | loss1: 0.0530 | loss_class: 0.0524 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:10:15] 	Iter 730100 Done. | loss1: 0.7237 | loss_class: 0.7230 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|19:11:11] 	Iter 730200 Done. | loss1: 0.0462 | loss_class: 0.0454 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:12:09] 	Iter 730300 Done. | loss1: 0.0022 | loss_class: 0.0015 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:13:06] 	Iter 730400 Done. | loss1: 0.0251 | loss_class: 0.0245 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:14:04] 	Iter 730500 Done. | loss1: 0.0204 | loss_class: 0.0199 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|19:15:01] 	Iter 730600 Done. | loss1: 0.0014 | loss_class: 0.0005 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|19:15:57] 	Iter 730700 Done. | loss1: 0.0261 | loss_class: 0.0257 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|19:16:54] 	Iter 730800 Done. | loss1: 0.0080 | loss_class: 0.0073 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:17:52] 	Iter 730900 Done. | loss1: 0.0030 | loss_class: 0.0023 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:18:50] 	Iter 731000 Done. | loss1: 0.0090 | loss_class: 0.0081 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|19:19:47] 	Iter 731100 Done. | loss1: 0.0071 | loss_class: 0.0064 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|19:20:44] 	Iter 731200 Done. | loss1: 0.0014 | loss_class: 0.0008 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:21:42] 	Iter 731300 Done. | loss1: 0.0016 | loss_class: 0.0008 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:22:38] 	Iter 731400 Done. | loss1: 0.0460 | loss_class: 0.0452 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|19:23:35] 	Iter 731500 Done. | loss1: 0.1237 | loss_class: 0.1229 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|19:24:30] 	Iter 731600 Done. | loss1: 0.4846 | loss_class: 0.4840 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:24:33] 	mean_loss1: 0.09180928394577098
[06.26.21|19:24:33] 	mean_loss_class: 0.09116352391650188
[06.26.21|19:24:33] 	mean_loss_recon: 0.0006457600313921076
[06.26.21|19:24:33] Time consumption:
[06.26.21|19:24:33] Done.
[06.26.21|19:24:33] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch82_model1.pt.
[06.26.21|19:24:33] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch82_model2.pt.
[06.26.21|19:24:33] Training epoch: 83
[06.26.21|19:25:28] 	Iter 731700 Done. | loss1: 0.6913 | loss_class: 0.6905 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|19:26:26] 	Iter 731800 Done. | loss1: 0.0011 | loss_class: 0.0004 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:27:24] 	Iter 731900 Done. | loss1: 0.6255 | loss_class: 0.6247 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|19:28:21] 	Iter 732000 Done. | loss1: 0.0110 | loss_class: 0.0105 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:29:18] 	Iter 732100 Done. | loss1: 0.1697 | loss_class: 0.1690 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:30:15] 	Iter 732200 Done. | loss1: 0.1595 | loss_class: 0.1590 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|19:31:12] 	Iter 732300 Done. | loss1: 0.3659 | loss_class: 0.3654 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:32:07] 	Iter 732400 Done. | loss1: 0.0206 | loss_class: 0.0202 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|19:33:04] 	Iter 732500 Done. | loss1: 0.0010 | loss_class: 0.0006 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|19:34:00] 	Iter 732600 Done. | loss1: 0.0505 | loss_class: 0.0499 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:34:59] 	Iter 732700 Done. | loss1: 0.0450 | loss_class: 0.0443 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:35:55] 	Iter 732800 Done. | loss1: 0.0574 | loss_class: 0.0567 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:36:53] 	Iter 732900 Done. | loss1: 0.0042 | loss_class: 0.0035 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:37:49] 	Iter 733000 Done. | loss1: 0.0443 | loss_class: 0.0437 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:38:46] 	Iter 733100 Done. | loss1: 0.1774 | loss_class: 0.1768 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:39:44] 	Iter 733200 Done. | loss1: 0.0444 | loss_class: 0.0439 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:40:41] 	Iter 733300 Done. | loss1: 0.0049 | loss_class: 0.0043 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:41:39] 	Iter 733400 Done. | loss1: 0.0123 | loss_class: 0.0117 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|19:42:37] 	Iter 733500 Done. | loss1: 0.6134 | loss_class: 0.6128 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|19:43:35] 	Iter 733600 Done. | loss1: 0.0814 | loss_class: 0.0808 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:44:32] 	Iter 733700 Done. | loss1: 0.0110 | loss_class: 0.0103 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:45:29] 	Iter 733800 Done. | loss1: 0.0044 | loss_class: 0.0039 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|19:46:26] 	Iter 733900 Done. | loss1: 0.0139 | loss_class: 0.0132 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|19:47:23] 	Iter 734000 Done. | loss1: 0.1041 | loss_class: 0.1033 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:48:21] 	Iter 734100 Done. | loss1: 0.6813 | loss_class: 0.6804 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|19:49:18] 	Iter 734200 Done. | loss1: 0.5021 | loss_class: 0.5012 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|19:50:15] 	Iter 734300 Done. | loss1: 0.1874 | loss_class: 0.1867 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:51:11] 	Iter 734400 Done. | loss1: 0.0373 | loss_class: 0.0367 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:52:08] 	Iter 734500 Done. | loss1: 0.0055 | loss_class: 0.0048 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:53:06] 	Iter 734600 Done. | loss1: 0.1427 | loss_class: 0.1421 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:54:04] 	Iter 734700 Done. | loss1: 0.0154 | loss_class: 0.0148 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|19:55:01] 	Iter 734800 Done. | loss1: 0.0051 | loss_class: 0.0045 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:55:57] 	Iter 734900 Done. | loss1: 0.0231 | loss_class: 0.0226 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|19:56:54] 	Iter 735000 Done. | loss1: 0.0570 | loss_class: 0.0564 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|19:57:50] 	Iter 735100 Done. | loss1: 0.0043 | loss_class: 0.0036 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|19:58:48] 	Iter 735200 Done. | loss1: 0.7194 | loss_class: 0.7188 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|19:59:46] 	Iter 735300 Done. | loss1: 0.0009 | loss_class: 0.0004 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:00:43] 	Iter 735400 Done. | loss1: 0.0594 | loss_class: 0.0585 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|20:01:42] 	Iter 735500 Done. | loss1: 0.0150 | loss_class: 0.0146 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|20:02:39] 	Iter 735600 Done. | loss1: 0.5633 | loss_class: 0.5629 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|20:03:37] 	Iter 735700 Done. | loss1: 0.0012 | loss_class: 0.0004 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|20:04:34] 	Iter 735800 Done. | loss1: 0.0937 | loss_class: 0.0933 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|20:05:31] 	Iter 735900 Done. | loss1: 0.5365 | loss_class: 0.5359 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:06:28] 	Iter 736000 Done. | loss1: 0.0706 | loss_class: 0.0703 | loss_recon: 0.0003 | lr: 0.001000
[06.26.21|20:07:25] 	Iter 736100 Done. | loss1: 0.0097 | loss_class: 0.0089 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|20:08:21] 	Iter 736200 Done. | loss1: 0.0037 | loss_class: 0.0030 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|20:09:19] 	Iter 736300 Done. | loss1: 0.0105 | loss_class: 0.0097 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|20:10:15] 	Iter 736400 Done. | loss1: 0.7558 | loss_class: 0.7552 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:11:13] 	Iter 736500 Done. | loss1: 0.1661 | loss_class: 0.1653 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|20:12:11] 	Iter 736600 Done. | loss1: 0.0257 | loss_class: 0.0249 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|20:13:08] 	Iter 736700 Done. | loss1: 0.0118 | loss_class: 0.0113 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:14:04] 	Iter 736800 Done. | loss1: 0.0250 | loss_class: 0.0244 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:15:01] 	Iter 736900 Done. | loss1: 0.0393 | loss_class: 0.0386 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|20:15:58] 	Iter 737000 Done. | loss1: 0.0210 | loss_class: 0.0204 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:16:55] 	Iter 737100 Done. | loss1: 0.0020 | loss_class: 0.0012 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|20:17:53] 	Iter 737200 Done. | loss1: 0.0054 | loss_class: 0.0049 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|20:18:49] 	Iter 737300 Done. | loss1: 0.0242 | loss_class: 0.0237 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|20:19:46] 	Iter 737400 Done. | loss1: 0.0076 | loss_class: 0.0070 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:20:44] 	Iter 737500 Done. | loss1: 0.0044 | loss_class: 0.0036 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|20:21:41] 	Iter 737600 Done. | loss1: 0.0106 | loss_class: 0.0100 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:22:37] 	Iter 737700 Done. | loss1: 0.0075 | loss_class: 0.0071 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|20:23:34] 	Iter 737800 Done. | loss1: 0.1811 | loss_class: 0.1805 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|20:24:32] 	Iter 737900 Done. | loss1: 0.0245 | loss_class: 0.0239 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:25:30] 	Iter 738000 Done. | loss1: 0.0103 | loss_class: 0.0097 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:26:26] 	Iter 738100 Done. | loss1: 0.0124 | loss_class: 0.0118 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:27:23] 	Iter 738200 Done. | loss1: 0.0250 | loss_class: 0.0242 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|20:28:21] 	Iter 738300 Done. | loss1: 0.0625 | loss_class: 0.0617 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|20:29:17] 	Iter 738400 Done. | loss1: 0.0045 | loss_class: 0.0036 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|20:30:12] 	Iter 738500 Done. | loss1: 0.0299 | loss_class: 0.0292 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|20:31:10] 	Iter 738600 Done. | loss1: 0.0213 | loss_class: 0.0206 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|20:32:08] 	Iter 738700 Done. | loss1: 0.4477 | loss_class: 0.4468 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|20:33:06] 	Iter 738800 Done. | loss1: 0.0265 | loss_class: 0.0260 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|20:34:03] 	Iter 738900 Done. | loss1: 0.0154 | loss_class: 0.0149 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|20:35:00] 	Iter 739000 Done. | loss1: 0.0058 | loss_class: 0.0052 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|20:35:57] 	Iter 739100 Done. | loss1: 0.6052 | loss_class: 0.6044 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|20:36:54] 	Iter 739200 Done. | loss1: 0.0027 | loss_class: 0.0021 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:37:51] 	Iter 739300 Done. | loss1: 0.0037 | loss_class: 0.0030 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|20:38:48] 	Iter 739400 Done. | loss1: 0.0108 | loss_class: 0.0102 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:39:44] 	Iter 739500 Done. | loss1: 0.0141 | loss_class: 0.0134 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|20:40:42] 	Iter 739600 Done. | loss1: 0.0339 | loss_class: 0.0333 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:41:39] 	Iter 739700 Done. | loss1: 0.0621 | loss_class: 0.0615 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:42:36] 	Iter 739800 Done. | loss1: 0.0504 | loss_class: 0.0498 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:43:33] 	Iter 739900 Done. | loss1: 0.0112 | loss_class: 0.0107 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:44:32] 	Iter 740000 Done. | loss1: 0.0074 | loss_class: 0.0069 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|20:45:27] 	Iter 740100 Done. | loss1: 0.0015 | loss_class: 0.0007 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|20:46:21] 	Iter 740200 Done. | loss1: 0.0175 | loss_class: 0.0167 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|20:47:18] 	Iter 740300 Done. | loss1: 0.0655 | loss_class: 0.0649 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:48:15] 	Iter 740400 Done. | loss1: 0.1747 | loss_class: 0.1739 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|20:49:12] 	Iter 740500 Done. | loss1: 0.0922 | loss_class: 0.0917 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:50:09] 	Iter 740600 Done. | loss1: 0.0008 | loss_class: 0.0002 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:51:06] 	Iter 740700 Done. | loss1: 0.0090 | loss_class: 0.0085 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:52:02] 	Iter 740800 Done. | loss1: 0.0029 | loss_class: 0.0021 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|20:52:59] 	Iter 740900 Done. | loss1: 0.0021 | loss_class: 0.0014 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:53:56] 	Iter 741000 Done. | loss1: 0.0376 | loss_class: 0.0372 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|20:54:52] 	Iter 741100 Done. | loss1: 0.0424 | loss_class: 0.0418 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|20:55:48] 	Iter 741200 Done. | loss1: 0.0156 | loss_class: 0.0152 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|20:56:45] 	Iter 741300 Done. | loss1: 0.7056 | loss_class: 0.7050 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:57:41] 	Iter 741400 Done. | loss1: 0.0213 | loss_class: 0.0209 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|20:58:38] 	Iter 741500 Done. | loss1: 0.0073 | loss_class: 0.0065 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|20:59:34] 	Iter 741600 Done. | loss1: 0.0027 | loss_class: 0.0021 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|20:59:50] 	mean_loss1: 0.08933390032248449
[06.26.21|20:59:50] 	mean_loss_class: 0.08868666749413759
[06.26.21|20:59:50] 	mean_loss_recon: 0.0006472329395473586
[06.26.21|20:59:50] Time consumption:
[06.26.21|20:59:50] Done.
[06.26.21|20:59:50] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch83_model1.pt.
[06.26.21|20:59:50] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch83_model2.pt.
[06.26.21|20:59:50] Training epoch: 84
[06.26.21|21:00:30] 	Iter 741700 Done. | loss1: 0.0404 | loss_class: 0.0398 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:01:26] 	Iter 741800 Done. | loss1: 0.0099 | loss_class: 0.0092 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:02:21] 	Iter 741900 Done. | loss1: 0.0031 | loss_class: 0.0024 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:03:17] 	Iter 742000 Done. | loss1: 0.0036 | loss_class: 0.0030 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:04:13] 	Iter 742100 Done. | loss1: 0.6317 | loss_class: 0.6308 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|21:05:10] 	Iter 742200 Done. | loss1: 0.0258 | loss_class: 0.0251 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:06:08] 	Iter 742300 Done. | loss1: 0.0825 | loss_class: 0.0820 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|21:07:04] 	Iter 742400 Done. | loss1: 0.0014 | loss_class: 0.0005 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|21:08:03] 	Iter 742500 Done. | loss1: 0.0045 | loss_class: 0.0039 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:08:59] 	Iter 742600 Done. | loss1: 0.0165 | loss_class: 0.0160 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|21:09:55] 	Iter 742700 Done. | loss1: 0.0614 | loss_class: 0.0607 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:10:54] 	Iter 742800 Done. | loss1: 0.1223 | loss_class: 0.1219 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|21:11:49] 	Iter 742900 Done. | loss1: 0.0301 | loss_class: 0.0293 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|21:12:47] 	Iter 743000 Done. | loss1: 0.0766 | loss_class: 0.0756 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|21:13:42] 	Iter 743100 Done. | loss1: 0.0087 | loss_class: 0.0081 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:14:37] 	Iter 743200 Done. | loss1: 0.0042 | loss_class: 0.0037 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|21:15:32] 	Iter 743300 Done. | loss1: 0.0147 | loss_class: 0.0141 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:16:27] 	Iter 743400 Done. | loss1: 0.1600 | loss_class: 0.1593 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:17:23] 	Iter 743500 Done. | loss1: 0.0228 | loss_class: 0.0223 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|21:18:19] 	Iter 743600 Done. | loss1: 0.0295 | loss_class: 0.0291 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|21:19:13] 	Iter 743700 Done. | loss1: 0.1975 | loss_class: 0.1968 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:20:09] 	Iter 743800 Done. | loss1: 0.0384 | loss_class: 0.0378 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:21:06] 	Iter 743900 Done. | loss1: 0.0083 | loss_class: 0.0076 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:22:01] 	Iter 744000 Done. | loss1: 0.0078 | loss_class: 0.0073 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|21:22:59] 	Iter 744100 Done. | loss1: 0.0204 | loss_class: 0.0198 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:23:56] 	Iter 744200 Done. | loss1: 0.0943 | loss_class: 0.0936 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:24:52] 	Iter 744300 Done. | loss1: 0.0059 | loss_class: 0.0049 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|21:25:47] 	Iter 744400 Done. | loss1: 0.1522 | loss_class: 0.1513 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|21:26:44] 	Iter 744500 Done. | loss1: 0.0090 | loss_class: 0.0083 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:27:39] 	Iter 744600 Done. | loss1: 0.0348 | loss_class: 0.0343 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|21:28:36] 	Iter 744700 Done. | loss1: 0.0744 | loss_class: 0.0736 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|21:29:32] 	Iter 744800 Done. | loss1: 0.0020 | loss_class: 0.0013 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:30:28] 	Iter 744900 Done. | loss1: 0.0280 | loss_class: 0.0273 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:31:23] 	Iter 745000 Done. | loss1: 0.0030 | loss_class: 0.0021 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|21:32:20] 	Iter 745100 Done. | loss1: 0.2658 | loss_class: 0.2654 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|21:33:15] 	Iter 745200 Done. | loss1: 0.0054 | loss_class: 0.0047 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:34:12] 	Iter 745300 Done. | loss1: 0.0013 | loss_class: 0.0007 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:35:07] 	Iter 745400 Done. | loss1: 0.3994 | loss_class: 0.3986 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|21:36:02] 	Iter 745500 Done. | loss1: 0.0122 | loss_class: 0.0115 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:36:58] 	Iter 745600 Done. | loss1: 1.0435 | loss_class: 1.0429 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:37:53] 	Iter 745700 Done. | loss1: 0.0093 | loss_class: 0.0085 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|21:38:50] 	Iter 745800 Done. | loss1: 0.0065 | loss_class: 0.0057 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|21:39:47] 	Iter 745900 Done. | loss1: 0.0094 | loss_class: 0.0087 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:40:43] 	Iter 746000 Done. | loss1: 0.0039 | loss_class: 0.0034 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:41:40] 	Iter 746100 Done. | loss1: 0.0037 | loss_class: 0.0032 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|21:42:35] 	Iter 746200 Done. | loss1: 0.0373 | loss_class: 0.0367 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:43:31] 	Iter 746300 Done. | loss1: 0.1653 | loss_class: 0.1646 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:44:27] 	Iter 746400 Done. | loss1: 0.0365 | loss_class: 0.0359 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|21:45:23] 	Iter 746500 Done. | loss1: 0.2775 | loss_class: 0.2769 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:46:19] 	Iter 746600 Done. | loss1: 0.0037 | loss_class: 0.0030 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:47:15] 	Iter 746700 Done. | loss1: 0.0024 | loss_class: 0.0016 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:48:10] 	Iter 746800 Done. | loss1: 0.0593 | loss_class: 0.0588 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:49:06] 	Iter 746900 Done. | loss1: 0.0415 | loss_class: 0.0409 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|21:50:01] 	Iter 747000 Done. | loss1: 0.0054 | loss_class: 0.0048 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:50:58] 	Iter 747100 Done. | loss1: 0.3522 | loss_class: 0.3516 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:51:55] 	Iter 747200 Done. | loss1: 0.0431 | loss_class: 0.0426 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|21:52:50] 	Iter 747300 Done. | loss1: 0.0025 | loss_class: 0.0018 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:53:46] 	Iter 747400 Done. | loss1: 0.0209 | loss_class: 0.0202 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|21:54:41] 	Iter 747500 Done. | loss1: 0.0202 | loss_class: 0.0194 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|21:55:36] 	Iter 747600 Done. | loss1: 0.0010 | loss_class: 0.0004 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:56:30] 	Iter 747700 Done. | loss1: 0.6772 | loss_class: 0.6767 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|21:57:28] 	Iter 747800 Done. | loss1: 0.2043 | loss_class: 0.2038 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|21:58:24] 	Iter 747900 Done. | loss1: 0.1034 | loss_class: 0.1029 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|21:59:20] 	Iter 748000 Done. | loss1: 0.4712 | loss_class: 0.4706 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:00:16] 	Iter 748100 Done. | loss1: 0.0045 | loss_class: 0.0037 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|22:01:12] 	Iter 748200 Done. | loss1: 0.6489 | loss_class: 0.6484 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|22:02:08] 	Iter 748300 Done. | loss1: 0.0398 | loss_class: 0.0392 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:03:02] 	Iter 748400 Done. | loss1: 0.0679 | loss_class: 0.0672 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:03:57] 	Iter 748500 Done. | loss1: 0.0026 | loss_class: 0.0019 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:04:54] 	Iter 748600 Done. | loss1: 0.0161 | loss_class: 0.0156 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|22:05:50] 	Iter 748700 Done. | loss1: 0.0628 | loss_class: 0.0624 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|22:06:47] 	Iter 748800 Done. | loss1: 0.1759 | loss_class: 0.1754 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:07:43] 	Iter 748900 Done. | loss1: 0.0793 | loss_class: 0.0786 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:08:37] 	Iter 749000 Done. | loss1: 0.1327 | loss_class: 0.1320 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:09:32] 	Iter 749100 Done. | loss1: 0.1169 | loss_class: 0.1161 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|22:10:28] 	Iter 749200 Done. | loss1: 0.0052 | loss_class: 0.0046 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:11:24] 	Iter 749300 Done. | loss1: 0.0023 | loss_class: 0.0016 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:12:19] 	Iter 749400 Done. | loss1: 0.0010 | loss_class: 0.0006 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|22:13:15] 	Iter 749500 Done. | loss1: 0.0057 | loss_class: 0.0051 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:14:11] 	Iter 749600 Done. | loss1: 0.0047 | loss_class: 0.0040 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:15:07] 	Iter 749700 Done. | loss1: 0.2765 | loss_class: 0.2759 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:16:02] 	Iter 749800 Done. | loss1: 0.0033 | loss_class: 0.0026 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:16:58] 	Iter 749900 Done. | loss1: 0.0047 | loss_class: 0.0040 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:17:52] 	Iter 750000 Done. | loss1: 0.0076 | loss_class: 0.0069 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:18:50] 	Iter 750100 Done. | loss1: 0.0031 | loss_class: 0.0024 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:19:46] 	Iter 750200 Done. | loss1: 0.0480 | loss_class: 0.0472 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:20:42] 	Iter 750300 Done. | loss1: 0.3395 | loss_class: 0.3391 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|22:21:38] 	Iter 750400 Done. | loss1: 0.1644 | loss_class: 0.1637 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:22:34] 	Iter 750500 Done. | loss1: 0.0631 | loss_class: 0.0627 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|22:23:29] 	Iter 750600 Done. | loss1: 0.0073 | loss_class: 0.0069 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|22:24:26] 	Iter 750700 Done. | loss1: 0.0219 | loss_class: 0.0212 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:25:23] 	Iter 750800 Done. | loss1: 0.0205 | loss_class: 0.0198 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:26:17] 	Iter 750900 Done. | loss1: 0.0697 | loss_class: 0.0691 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:27:12] 	Iter 751000 Done. | loss1: 0.0044 | loss_class: 0.0039 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|22:28:07] 	Iter 751100 Done. | loss1: 0.0862 | loss_class: 0.0853 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|22:29:02] 	Iter 751200 Done. | loss1: 0.1012 | loss_class: 0.1006 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:29:57] 	Iter 751300 Done. | loss1: 0.0049 | loss_class: 0.0042 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:30:52] 	Iter 751400 Done. | loss1: 0.0305 | loss_class: 0.0301 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|22:31:47] 	Iter 751500 Done. | loss1: 0.0223 | loss_class: 0.0214 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|22:32:45] 	Iter 751600 Done. | loss1: 0.0054 | loss_class: 0.0048 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:33:11] 	mean_loss1: 0.08239479530969991
[06.26.21|22:33:11] 	mean_loss_class: 0.0817483558487765
[06.26.21|22:33:11] 	mean_loss_recon: 0.0006464394699091493
[06.26.21|22:33:11] Time consumption:
[06.26.21|22:33:11] Done.
[06.26.21|22:33:11] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch84_model1.pt.
[06.26.21|22:33:11] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch84_model2.pt.
[06.26.21|22:33:11] Eval epoch: 84
[06.26.21|22:39:52] 	mean_loss1: 0.6605450200086619
[06.26.21|22:39:52] 	mean_loss_class: 0.6601820287291076
[06.26.21|22:39:52] 	mean_loss_recon: 0.036299152574104855
[06.26.21|22:39:52] 

[06.26.21|22:39:52] 	Top1: 84.55%
[06.26.21|22:39:52] 

[06.26.21|22:39:52] 	Top5: 96.91%
[06.26.21|22:39:52] Done.
[06.26.21|22:39:52] Training epoch: 85
[06.26.21|22:40:21] 	Iter 751700 Done. | loss1: 0.0128 | loss_class: 0.0123 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|22:41:16] 	Iter 751800 Done. | loss1: 0.0698 | loss_class: 0.0691 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:42:13] 	Iter 751900 Done. | loss1: 0.0024 | loss_class: 0.0018 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:43:08] 	Iter 752000 Done. | loss1: 0.0060 | loss_class: 0.0053 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:44:04] 	Iter 752100 Done. | loss1: 0.1275 | loss_class: 0.1269 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:45:00] 	Iter 752200 Done. | loss1: 0.0059 | loss_class: 0.0052 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:45:56] 	Iter 752300 Done. | loss1: 0.1094 | loss_class: 0.1089 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|22:46:51] 	Iter 752400 Done. | loss1: 0.0302 | loss_class: 0.0295 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:47:49] 	Iter 752500 Done. | loss1: 0.1011 | loss_class: 0.1005 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:48:46] 	Iter 752600 Done. | loss1: 0.0184 | loss_class: 0.0177 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:49:42] 	Iter 752700 Done. | loss1: 0.0137 | loss_class: 0.0128 | loss_recon: 0.0010 | lr: 0.001000
[06.26.21|22:50:38] 	Iter 752800 Done. | loss1: 0.1879 | loss_class: 0.1873 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:51:35] 	Iter 752900 Done. | loss1: 0.3195 | loss_class: 0.3189 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:52:31] 	Iter 753000 Done. | loss1: 0.0031 | loss_class: 0.0022 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|22:53:27] 	Iter 753100 Done. | loss1: 0.0313 | loss_class: 0.0307 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:54:22] 	Iter 753200 Done. | loss1: 0.1341 | loss_class: 0.1336 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|22:55:17] 	Iter 753300 Done. | loss1: 0.0986 | loss_class: 0.0981 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|22:56:13] 	Iter 753400 Done. | loss1: 0.0019 | loss_class: 0.0013 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|22:57:09] 	Iter 753500 Done. | loss1: 0.0115 | loss_class: 0.0108 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|22:58:06] 	Iter 753600 Done. | loss1: 0.1689 | loss_class: 0.1683 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|22:59:01] 	Iter 753700 Done. | loss1: 0.0209 | loss_class: 0.0201 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|23:00:00] 	Iter 753800 Done. | loss1: 0.0143 | loss_class: 0.0137 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:00:55] 	Iter 753900 Done. | loss1: 0.0013 | loss_class: 0.0007 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:01:50] 	Iter 754000 Done. | loss1: 0.2490 | loss_class: 0.2483 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:02:47] 	Iter 754100 Done. | loss1: 0.2553 | loss_class: 0.2549 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|23:03:42] 	Iter 754200 Done. | loss1: 0.0018 | loss_class: 0.0012 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:04:37] 	Iter 754300 Done. | loss1: 0.0046 | loss_class: 0.0041 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:05:33] 	Iter 754400 Done. | loss1: 0.1618 | loss_class: 0.1613 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:06:28] 	Iter 754500 Done. | loss1: 0.0155 | loss_class: 0.0148 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:07:23] 	Iter 754600 Done. | loss1: 0.0174 | loss_class: 0.0166 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|23:08:19] 	Iter 754700 Done. | loss1: 0.0823 | loss_class: 0.0817 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:09:14] 	Iter 754800 Done. | loss1: 0.0047 | loss_class: 0.0042 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|23:10:11] 	Iter 754900 Done. | loss1: 0.0281 | loss_class: 0.0274 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:11:05] 	Iter 755000 Done. | loss1: 0.0133 | loss_class: 0.0127 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:12:01] 	Iter 755100 Done. | loss1: 0.1568 | loss_class: 0.1561 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:12:55] 	Iter 755200 Done. | loss1: 0.0716 | loss_class: 0.0710 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|23:13:52] 	Iter 755300 Done. | loss1: 0.4245 | loss_class: 0.4239 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:14:49] 	Iter 755400 Done. | loss1: 0.1151 | loss_class: 0.1144 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:15:44] 	Iter 755500 Done. | loss1: 0.0227 | loss_class: 0.0223 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|23:16:39] 	Iter 755600 Done. | loss1: 0.0015 | loss_class: 0.0006 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|23:17:35] 	Iter 755700 Done. | loss1: 0.0035 | loss_class: 0.0029 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:18:31] 	Iter 755800 Done. | loss1: 0.0387 | loss_class: 0.0381 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:19:26] 	Iter 755900 Done. | loss1: 0.1421 | loss_class: 0.1415 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:20:23] 	Iter 756000 Done. | loss1: 0.0011 | loss_class: 0.0005 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:21:19] 	Iter 756100 Done. | loss1: 0.0098 | loss_class: 0.0091 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:22:14] 	Iter 756200 Done. | loss1: 0.0209 | loss_class: 0.0202 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:23:12] 	Iter 756300 Done. | loss1: 0.0041 | loss_class: 0.0034 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:24:10] 	Iter 756400 Done. | loss1: 0.0057 | loss_class: 0.0051 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:25:09] 	Iter 756500 Done. | loss1: 0.0214 | loss_class: 0.0208 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|23:26:04] 	Iter 756600 Done. | loss1: 0.0037 | loss_class: 0.0030 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:27:01] 	Iter 756700 Done. | loss1: 0.0528 | loss_class: 0.0520 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|23:27:58] 	Iter 756800 Done. | loss1: 0.1806 | loss_class: 0.1798 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|23:28:54] 	Iter 756900 Done. | loss1: 0.0238 | loss_class: 0.0232 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:29:50] 	Iter 757000 Done. | loss1: 0.0139 | loss_class: 0.0133 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|23:30:47] 	Iter 757100 Done. | loss1: 0.0659 | loss_class: 0.0653 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:31:42] 	Iter 757200 Done. | loss1: 0.0181 | loss_class: 0.0174 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:32:38] 	Iter 757300 Done. | loss1: 0.0065 | loss_class: 0.0058 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:33:36] 	Iter 757400 Done. | loss1: 0.0026 | loss_class: 0.0020 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:34:31] 	Iter 757500 Done. | loss1: 0.0036 | loss_class: 0.0030 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:35:26] 	Iter 757600 Done. | loss1: 0.0028 | loss_class: 0.0023 | loss_recon: 0.0004 | lr: 0.001000
[06.26.21|23:36:22] 	Iter 757700 Done. | loss1: 0.0107 | loss_class: 0.0100 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:37:19] 	Iter 757800 Done. | loss1: 0.2470 | loss_class: 0.2463 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:38:15] 	Iter 757900 Done. | loss1: 0.0525 | loss_class: 0.0518 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:39:12] 	Iter 758000 Done. | loss1: 0.0211 | loss_class: 0.0205 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:40:07] 	Iter 758100 Done. | loss1: 0.1617 | loss_class: 0.1611 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:41:03] 	Iter 758200 Done. | loss1: 0.0016 | loss_class: 0.0010 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:41:59] 	Iter 758300 Done. | loss1: 0.0165 | loss_class: 0.0160 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|23:42:55] 	Iter 758400 Done. | loss1: 0.0334 | loss_class: 0.0328 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:43:51] 	Iter 758500 Done. | loss1: 0.0341 | loss_class: 0.0335 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|23:44:47] 	Iter 758600 Done. | loss1: 0.0041 | loss_class: 0.0034 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:45:41] 	Iter 758700 Done. | loss1: 0.0062 | loss_class: 0.0055 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:46:38] 	Iter 758800 Done. | loss1: 0.0030 | loss_class: 0.0023 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|23:47:35] 	Iter 758900 Done. | loss1: 0.0015 | loss_class: 0.0009 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:48:32] 	Iter 759000 Done. | loss1: 0.0127 | loss_class: 0.0120 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:49:28] 	Iter 759100 Done. | loss1: 0.0073 | loss_class: 0.0068 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:50:23] 	Iter 759200 Done. | loss1: 0.0136 | loss_class: 0.0130 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|23:51:19] 	Iter 759300 Done. | loss1: 0.1000 | loss_class: 0.0993 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:52:16] 	Iter 759400 Done. | loss1: 0.4536 | loss_class: 0.4529 | loss_recon: 0.0007 | lr: 0.001000
[06.26.21|23:53:12] 	Iter 759500 Done. | loss1: 0.0094 | loss_class: 0.0088 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|23:54:09] 	Iter 759600 Done. | loss1: 0.0012 | loss_class: 0.0006 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:55:05] 	Iter 759700 Done. | loss1: 0.0495 | loss_class: 0.0489 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|23:56:01] 	Iter 759800 Done. | loss1: 0.1126 | loss_class: 0.1118 | loss_recon: 0.0008 | lr: 0.001000
[06.26.21|23:56:57] 	Iter 759900 Done. | loss1: 0.0225 | loss_class: 0.0219 | loss_recon: 0.0006 | lr: 0.001000
[06.26.21|23:57:52] 	Iter 760000 Done. | loss1: 0.0054 | loss_class: 0.0045 | loss_recon: 0.0009 | lr: 0.001000
[06.26.21|23:58:48] 	Iter 760100 Done. | loss1: 0.0065 | loss_class: 0.0060 | loss_recon: 0.0005 | lr: 0.001000
[06.26.21|23:59:44] 	Iter 760200 Done. | loss1: 0.2603 | loss_class: 0.2596 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:00:39] 	Iter 760300 Done. | loss1: 0.2359 | loss_class: 0.2351 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|00:01:34] 	Iter 760400 Done. | loss1: 0.0136 | loss_class: 0.0130 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:02:29] 	Iter 760500 Done. | loss1: 0.0028 | loss_class: 0.0021 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:03:27] 	Iter 760600 Done. | loss1: 0.1784 | loss_class: 0.1777 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:04:24] 	Iter 760700 Done. | loss1: 0.0032 | loss_class: 0.0027 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|00:05:20] 	Iter 760800 Done. | loss1: 0.0017 | loss_class: 0.0012 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|00:06:17] 	Iter 760900 Done. | loss1: 0.0138 | loss_class: 0.0129 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|00:07:13] 	Iter 761000 Done. | loss1: 0.0102 | loss_class: 0.0094 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|00:08:08] 	Iter 761100 Done. | loss1: 0.0172 | loss_class: 0.0165 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|00:09:04] 	Iter 761200 Done. | loss1: 0.0043 | loss_class: 0.0036 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:10:00] 	Iter 761300 Done. | loss1: 0.0680 | loss_class: 0.0673 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:10:55] 	Iter 761400 Done. | loss1: 0.0249 | loss_class: 0.0244 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|00:11:51] 	Iter 761500 Done. | loss1: 0.0036 | loss_class: 0.0029 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:12:46] 	Iter 761600 Done. | loss1: 0.1478 | loss_class: 0.1470 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|00:13:25] 	mean_loss1: 0.08114116400810814
[06.27.21|00:13:25] 	mean_loss_class: 0.08049533135241431
[06.27.21|00:13:25] 	mean_loss_recon: 0.0006458326010395906
[06.27.21|00:13:25] Time consumption:
[06.27.21|00:13:25] Done.
[06.27.21|00:13:25] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch85_model1.pt.
[06.27.21|00:13:26] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch85_model2.pt.
[06.27.21|00:13:26] Training epoch: 86
[06.27.21|00:13:43] 	Iter 761700 Done. | loss1: 0.0042 | loss_class: 0.0037 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|00:14:39] 	Iter 761800 Done. | loss1: 0.0211 | loss_class: 0.0206 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:15:37] 	Iter 761900 Done. | loss1: 0.0272 | loss_class: 0.0265 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:16:34] 	Iter 762000 Done. | loss1: 0.0429 | loss_class: 0.0422 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:17:31] 	Iter 762100 Done. | loss1: 0.0151 | loss_class: 0.0144 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:18:27] 	Iter 762200 Done. | loss1: 0.0085 | loss_class: 0.0079 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:19:22] 	Iter 762300 Done. | loss1: 0.0712 | loss_class: 0.0705 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:20:19] 	Iter 762400 Done. | loss1: 0.0085 | loss_class: 0.0078 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:21:16] 	Iter 762500 Done. | loss1: 0.0081 | loss_class: 0.0072 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|00:22:13] 	Iter 762600 Done. | loss1: 0.0100 | loss_class: 0.0093 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:23:09] 	Iter 762700 Done. | loss1: 0.0646 | loss_class: 0.0641 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:24:05] 	Iter 762800 Done. | loss1: 0.0326 | loss_class: 0.0321 | loss_recon: 0.0004 | lr: 0.001000
[06.27.21|00:25:01] 	Iter 762900 Done. | loss1: 0.0491 | loss_class: 0.0486 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|00:25:55] 	Iter 763000 Done. | loss1: 0.0268 | loss_class: 0.0260 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|00:26:51] 	Iter 763100 Done. | loss1: 0.0088 | loss_class: 0.0082 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:27:47] 	Iter 763200 Done. | loss1: 0.0067 | loss_class: 0.0061 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:28:44] 	Iter 763300 Done. | loss1: 0.1392 | loss_class: 0.1386 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:29:39] 	Iter 763400 Done. | loss1: 0.0015 | loss_class: 0.0009 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:30:36] 	Iter 763500 Done. | loss1: 0.0274 | loss_class: 0.0267 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:31:33] 	Iter 763600 Done. | loss1: 0.1253 | loss_class: 0.1248 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|00:32:28] 	Iter 763700 Done. | loss1: 0.0510 | loss_class: 0.0504 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:33:22] 	Iter 763800 Done. | loss1: 0.0442 | loss_class: 0.0437 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|00:34:19] 	Iter 763900 Done. | loss1: 0.2390 | loss_class: 0.2384 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:35:14] 	Iter 764000 Done. | loss1: 0.0308 | loss_class: 0.0302 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:36:10] 	Iter 764100 Done. | loss1: 0.0137 | loss_class: 0.0131 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:37:07] 	Iter 764200 Done. | loss1: 0.0016 | loss_class: 0.0011 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|00:38:02] 	Iter 764300 Done. | loss1: 0.0760 | loss_class: 0.0753 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:38:58] 	Iter 764400 Done. | loss1: 0.0039 | loss_class: 0.0034 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|00:39:54] 	Iter 764500 Done. | loss1: 0.0014 | loss_class: 0.0007 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:40:48] 	Iter 764600 Done. | loss1: 0.1388 | loss_class: 0.1380 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|00:41:44] 	Iter 764700 Done. | loss1: 0.0561 | loss_class: 0.0555 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:42:42] 	Iter 764800 Done. | loss1: 0.0414 | loss_class: 0.0409 | loss_recon: 0.0004 | lr: 0.001000
[06.27.21|00:43:37] 	Iter 764900 Done. | loss1: 0.1055 | loss_class: 0.1048 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:44:33] 	Iter 765000 Done. | loss1: 0.0843 | loss_class: 0.0838 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|00:45:30] 	Iter 765100 Done. | loss1: 0.3127 | loss_class: 0.3118 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|00:46:24] 	Iter 765200 Done. | loss1: 0.0694 | loss_class: 0.0687 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|00:47:20] 	Iter 765300 Done. | loss1: 0.0110 | loss_class: 0.0105 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|00:48:16] 	Iter 765400 Done. | loss1: 0.0014 | loss_class: 0.0005 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|00:49:10] 	Iter 765500 Done. | loss1: 0.0071 | loss_class: 0.0065 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:50:05] 	Iter 765600 Done. | loss1: 0.0723 | loss_class: 0.0716 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|00:51:02] 	Iter 765700 Done. | loss1: 0.1603 | loss_class: 0.1596 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:51:57] 	Iter 765800 Done. | loss1: 0.0180 | loss_class: 0.0174 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:52:54] 	Iter 765900 Done. | loss1: 0.0328 | loss_class: 0.0322 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:53:48] 	Iter 766000 Done. | loss1: 0.1283 | loss_class: 0.1277 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:54:43] 	Iter 766100 Done. | loss1: 0.0526 | loss_class: 0.0520 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:55:39] 	Iter 766200 Done. | loss1: 0.0137 | loss_class: 0.0130 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:56:35] 	Iter 766300 Done. | loss1: 0.0455 | loss_class: 0.0448 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|00:57:30] 	Iter 766400 Done. | loss1: 0.0139 | loss_class: 0.0134 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:58:26] 	Iter 766500 Done. | loss1: 0.0094 | loss_class: 0.0088 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|00:59:22] 	Iter 766600 Done. | loss1: 0.0894 | loss_class: 0.0887 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:00:18] 	Iter 766700 Done. | loss1: 0.0044 | loss_class: 0.0036 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:01:13] 	Iter 766800 Done. | loss1: 0.0106 | loss_class: 0.0101 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|01:02:08] 	Iter 766900 Done. | loss1: 0.0921 | loss_class: 0.0916 | loss_recon: 0.0004 | lr: 0.001000
[06.27.21|01:03:05] 	Iter 767000 Done. | loss1: 0.0493 | loss_class: 0.0486 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:04:00] 	Iter 767100 Done. | loss1: 0.0211 | loss_class: 0.0204 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:04:57] 	Iter 767200 Done. | loss1: 0.0310 | loss_class: 0.0304 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:05:52] 	Iter 767300 Done. | loss1: 0.0123 | loss_class: 0.0117 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:06:48] 	Iter 767400 Done. | loss1: 0.2151 | loss_class: 0.2145 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:07:45] 	Iter 767500 Done. | loss1: 0.1449 | loss_class: 0.1442 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:08:39] 	Iter 767600 Done. | loss1: 0.0073 | loss_class: 0.0066 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:09:37] 	Iter 767700 Done. | loss1: 0.8438 | loss_class: 0.8430 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|01:10:33] 	Iter 767800 Done. | loss1: 0.0011 | loss_class: 0.0004 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:11:29] 	Iter 767900 Done. | loss1: 0.4084 | loss_class: 0.4077 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:12:24] 	Iter 768000 Done. | loss1: 0.0206 | loss_class: 0.0201 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:13:20] 	Iter 768100 Done. | loss1: 0.0028 | loss_class: 0.0023 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|01:14:16] 	Iter 768200 Done. | loss1: 0.0128 | loss_class: 0.0124 | loss_recon: 0.0004 | lr: 0.001000
[06.27.21|01:15:12] 	Iter 768300 Done. | loss1: 0.0042 | loss_class: 0.0035 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:16:09] 	Iter 768400 Done. | loss1: 0.0654 | loss_class: 0.0649 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|01:17:04] 	Iter 768500 Done. | loss1: 0.0329 | loss_class: 0.0323 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:17:59] 	Iter 768600 Done. | loss1: 0.0170 | loss_class: 0.0163 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|01:18:56] 	Iter 768700 Done. | loss1: 0.0046 | loss_class: 0.0040 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:19:51] 	Iter 768800 Done. | loss1: 0.0018 | loss_class: 0.0011 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:20:48] 	Iter 768900 Done. | loss1: 0.0975 | loss_class: 0.0967 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|01:21:44] 	Iter 769000 Done. | loss1: 0.0869 | loss_class: 0.0861 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|01:22:39] 	Iter 769100 Done. | loss1: 0.0326 | loss_class: 0.0319 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:23:35] 	Iter 769200 Done. | loss1: 0.0094 | loss_class: 0.0089 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:24:31] 	Iter 769300 Done. | loss1: 0.0305 | loss_class: 0.0296 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|01:25:27] 	Iter 769400 Done. | loss1: 0.0249 | loss_class: 0.0244 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|01:26:23] 	Iter 769500 Done. | loss1: 0.0011 | loss_class: 0.0005 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:27:18] 	Iter 769600 Done. | loss1: 0.2667 | loss_class: 0.2661 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:28:15] 	Iter 769700 Done. | loss1: 0.1307 | loss_class: 0.1297 | loss_recon: 0.0010 | lr: 0.001000
[06.27.21|01:29:10] 	Iter 769800 Done. | loss1: 0.0601 | loss_class: 0.0595 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:30:06] 	Iter 769900 Done. | loss1: 0.1046 | loss_class: 0.1041 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|01:31:01] 	Iter 770000 Done. | loss1: 0.0019 | loss_class: 0.0010 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|01:31:58] 	Iter 770100 Done. | loss1: 0.0934 | loss_class: 0.0929 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|01:32:53] 	Iter 770200 Done. | loss1: 0.1183 | loss_class: 0.1177 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:33:49] 	Iter 770300 Done. | loss1: 0.0219 | loss_class: 0.0214 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|01:34:46] 	Iter 770400 Done. | loss1: 0.0023 | loss_class: 0.0017 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:35:40] 	Iter 770500 Done. | loss1: 0.0679 | loss_class: 0.0674 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|01:36:36] 	Iter 770600 Done. | loss1: 0.0131 | loss_class: 0.0124 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:37:31] 	Iter 770700 Done. | loss1: 0.0315 | loss_class: 0.0308 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:38:28] 	Iter 770800 Done. | loss1: 0.0083 | loss_class: 0.0078 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|01:39:22] 	Iter 770900 Done. | loss1: 0.0360 | loss_class: 0.0354 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:40:17] 	Iter 771000 Done. | loss1: 0.0328 | loss_class: 0.0322 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:41:13] 	Iter 771100 Done. | loss1: 0.3102 | loss_class: 0.3096 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|01:42:11] 	Iter 771200 Done. | loss1: 0.0107 | loss_class: 0.0101 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:43:07] 	Iter 771300 Done. | loss1: 0.0174 | loss_class: 0.0169 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:44:04] 	Iter 771400 Done. | loss1: 0.0210 | loss_class: 0.0204 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:45:02] 	Iter 771500 Done. | loss1: 0.0121 | loss_class: 0.0111 | loss_recon: 0.0011 | lr: 0.001000
[06.27.21|01:45:58] 	Iter 771600 Done. | loss1: 0.0186 | loss_class: 0.0179 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:46:49] 	mean_loss1: 0.0767932288543434
[06.27.21|01:46:49] 	mean_loss_class: 0.07614770668083028
[06.27.21|01:46:49] 	mean_loss_recon: 0.0006455221704951341
[06.27.21|01:46:49] Time consumption:
[06.27.21|01:46:49] Done.
[06.27.21|01:46:49] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch86_model1.pt.
[06.27.21|01:46:49] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch86_model2.pt.
[06.27.21|01:46:49] Training epoch: 87
[06.27.21|01:46:53] 	Iter 771700 Done. | loss1: 0.0212 | loss_class: 0.0205 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:47:49] 	Iter 771800 Done. | loss1: 0.0798 | loss_class: 0.0793 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:48:45] 	Iter 771900 Done. | loss1: 0.0023 | loss_class: 0.0017 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:49:42] 	Iter 772000 Done. | loss1: 0.0057 | loss_class: 0.0051 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:50:37] 	Iter 772100 Done. | loss1: 0.0014 | loss_class: 0.0007 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:51:33] 	Iter 772200 Done. | loss1: 0.3187 | loss_class: 0.3181 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|01:52:29] 	Iter 772300 Done. | loss1: 0.0828 | loss_class: 0.0822 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|01:53:26] 	Iter 772400 Done. | loss1: 0.0209 | loss_class: 0.0204 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|01:54:21] 	Iter 772500 Done. | loss1: 0.0543 | loss_class: 0.0537 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:55:16] 	Iter 772600 Done. | loss1: 0.0809 | loss_class: 0.0803 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:56:11] 	Iter 772700 Done. | loss1: 0.0243 | loss_class: 0.0235 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|01:57:07] 	Iter 772800 Done. | loss1: 0.0347 | loss_class: 0.0340 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|01:58:04] 	Iter 772900 Done. | loss1: 0.0016 | loss_class: 0.0010 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|01:59:00] 	Iter 773000 Done. | loss1: 0.1658 | loss_class: 0.1654 | loss_recon: 0.0004 | lr: 0.001000
[06.27.21|01:59:57] 	Iter 773100 Done. | loss1: 0.0547 | loss_class: 0.0537 | loss_recon: 0.0010 | lr: 0.001000
[06.27.21|02:00:53] 	Iter 773200 Done. | loss1: 0.0045 | loss_class: 0.0040 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:01:49] 	Iter 773300 Done. | loss1: 0.0188 | loss_class: 0.0182 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:02:45] 	Iter 773400 Done. | loss1: 0.0010 | loss_class: 0.0004 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:03:41] 	Iter 773500 Done. | loss1: 0.0210 | loss_class: 0.0203 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:04:36] 	Iter 773600 Done. | loss1: 0.1071 | loss_class: 0.1065 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:05:32] 	Iter 773700 Done. | loss1: 0.6600 | loss_class: 0.6595 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:06:28] 	Iter 773800 Done. | loss1: 0.0017 | loss_class: 0.0010 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:07:24] 	Iter 773900 Done. | loss1: 0.0202 | loss_class: 0.0196 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:08:20] 	Iter 774000 Done. | loss1: 0.0276 | loss_class: 0.0268 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|02:09:17] 	Iter 774100 Done. | loss1: 0.0185 | loss_class: 0.0178 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:10:13] 	Iter 774200 Done. | loss1: 0.0540 | loss_class: 0.0534 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:11:10] 	Iter 774300 Done. | loss1: 0.0015 | loss_class: 0.0008 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:12:06] 	Iter 774400 Done. | loss1: 0.0047 | loss_class: 0.0041 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:13:02] 	Iter 774500 Done. | loss1: 0.0143 | loss_class: 0.0135 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|02:13:59] 	Iter 774600 Done. | loss1: 0.0684 | loss_class: 0.0677 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:14:54] 	Iter 774700 Done. | loss1: 0.0693 | loss_class: 0.0687 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:15:49] 	Iter 774800 Done. | loss1: 0.0025 | loss_class: 0.0019 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:16:45] 	Iter 774900 Done. | loss1: 0.0047 | loss_class: 0.0042 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:17:40] 	Iter 775000 Done. | loss1: 0.0181 | loss_class: 0.0176 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:18:35] 	Iter 775100 Done. | loss1: 0.0049 | loss_class: 0.0042 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:19:31] 	Iter 775200 Done. | loss1: 0.0921 | loss_class: 0.0914 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:20:27] 	Iter 775300 Done. | loss1: 0.0063 | loss_class: 0.0057 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:21:22] 	Iter 775400 Done. | loss1: 0.0013 | loss_class: 0.0008 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:22:18] 	Iter 775500 Done. | loss1: 0.0026 | loss_class: 0.0019 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:23:13] 	Iter 775600 Done. | loss1: 0.0039 | loss_class: 0.0033 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:24:10] 	Iter 775700 Done. | loss1: 0.0012 | loss_class: 0.0007 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:25:06] 	Iter 775800 Done. | loss1: 0.0245 | loss_class: 0.0237 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|02:26:02] 	Iter 775900 Done. | loss1: 0.0698 | loss_class: 0.0692 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:26:58] 	Iter 776000 Done. | loss1: 0.0099 | loss_class: 0.0091 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|02:27:53] 	Iter 776100 Done. | loss1: 0.1485 | loss_class: 0.1480 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:28:48] 	Iter 776200 Done. | loss1: 0.0029 | loss_class: 0.0022 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:29:44] 	Iter 776300 Done. | loss1: 0.0119 | loss_class: 0.0113 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:30:39] 	Iter 776400 Done. | loss1: 0.0045 | loss_class: 0.0040 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:31:37] 	Iter 776500 Done. | loss1: 0.0096 | loss_class: 0.0087 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|02:32:32] 	Iter 776600 Done. | loss1: 0.0056 | loss_class: 0.0052 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:33:28] 	Iter 776700 Done. | loss1: 0.1594 | loss_class: 0.1588 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:34:24] 	Iter 776800 Done. | loss1: 0.0027 | loss_class: 0.0022 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:35:20] 	Iter 776900 Done. | loss1: 0.0018 | loss_class: 0.0011 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:36:17] 	Iter 777000 Done. | loss1: 0.0046 | loss_class: 0.0037 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|02:37:13] 	Iter 777100 Done. | loss1: 0.0019 | loss_class: 0.0012 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:38:11] 	Iter 777200 Done. | loss1: 0.0017 | loss_class: 0.0011 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:39:07] 	Iter 777300 Done. | loss1: 0.0355 | loss_class: 0.0349 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:40:03] 	Iter 777400 Done. | loss1: 0.0052 | loss_class: 0.0047 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:40:59] 	Iter 777500 Done. | loss1: 0.1310 | loss_class: 0.1304 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:41:56] 	Iter 777600 Done. | loss1: 0.0176 | loss_class: 0.0170 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:42:52] 	Iter 777700 Done. | loss1: 0.0163 | loss_class: 0.0156 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:43:48] 	Iter 777800 Done. | loss1: 0.0203 | loss_class: 0.0198 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:44:44] 	Iter 777900 Done. | loss1: 0.0018 | loss_class: 0.0013 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:45:40] 	Iter 778000 Done. | loss1: 0.1291 | loss_class: 0.1285 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:46:35] 	Iter 778100 Done. | loss1: 0.0368 | loss_class: 0.0362 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:47:32] 	Iter 778200 Done. | loss1: 0.4187 | loss_class: 0.4180 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:48:29] 	Iter 778300 Done. | loss1: 0.0100 | loss_class: 0.0092 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|02:49:25] 	Iter 778400 Done. | loss1: 0.0153 | loss_class: 0.0147 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:50:21] 	Iter 778500 Done. | loss1: 0.3093 | loss_class: 0.3088 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:51:18] 	Iter 778600 Done. | loss1: 0.3478 | loss_class: 0.3472 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:52:14] 	Iter 778700 Done. | loss1: 0.0013 | loss_class: 0.0006 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:53:11] 	Iter 778800 Done. | loss1: 0.0287 | loss_class: 0.0280 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:54:07] 	Iter 778900 Done. | loss1: 0.0029 | loss_class: 0.0022 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:55:02] 	Iter 779000 Done. | loss1: 0.0046 | loss_class: 0.0038 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|02:55:58] 	Iter 779100 Done. | loss1: 0.0141 | loss_class: 0.0135 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|02:56:54] 	Iter 779200 Done. | loss1: 0.0105 | loss_class: 0.0097 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|02:57:50] 	Iter 779300 Done. | loss1: 0.0034 | loss_class: 0.0027 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|02:58:47] 	Iter 779400 Done. | loss1: 0.0325 | loss_class: 0.0320 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|02:59:43] 	Iter 779500 Done. | loss1: 0.0125 | loss_class: 0.0119 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:00:38] 	Iter 779600 Done. | loss1: 0.2189 | loss_class: 0.2181 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:01:34] 	Iter 779700 Done. | loss1: 0.0020 | loss_class: 0.0015 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|03:02:29] 	Iter 779800 Done. | loss1: 0.0473 | loss_class: 0.0465 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|03:03:26] 	Iter 779900 Done. | loss1: 0.1526 | loss_class: 0.1519 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:04:22] 	Iter 780000 Done. | loss1: 0.0016 | loss_class: 0.0010 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:05:19] 	Iter 780100 Done. | loss1: 0.0035 | loss_class: 0.0027 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|03:06:14] 	Iter 780200 Done. | loss1: 0.0858 | loss_class: 0.0852 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:07:11] 	Iter 780300 Done. | loss1: 0.0018 | loss_class: 0.0012 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:08:08] 	Iter 780400 Done. | loss1: 0.3090 | loss_class: 0.3084 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:09:01] 	Iter 780500 Done. | loss1: 0.0767 | loss_class: 0.0761 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:09:58] 	Iter 780600 Done. | loss1: 0.0880 | loss_class: 0.0873 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:10:55] 	Iter 780700 Done. | loss1: 0.0052 | loss_class: 0.0046 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:11:51] 	Iter 780800 Done. | loss1: 0.0436 | loss_class: 0.0429 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:12:46] 	Iter 780900 Done. | loss1: 0.1178 | loss_class: 0.1171 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:13:43] 	Iter 781000 Done. | loss1: 0.0064 | loss_class: 0.0058 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|03:14:39] 	Iter 781100 Done. | loss1: 0.0018 | loss_class: 0.0011 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:15:35] 	Iter 781200 Done. | loss1: 0.0011 | loss_class: 0.0006 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|03:16:31] 	Iter 781300 Done. | loss1: 0.0117 | loss_class: 0.0109 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|03:17:26] 	Iter 781400 Done. | loss1: 0.6131 | loss_class: 0.6124 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:18:21] 	Iter 781500 Done. | loss1: 0.0399 | loss_class: 0.0392 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:19:18] 	Iter 781600 Done. | loss1: 0.0935 | loss_class: 0.0928 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:20:14] 	Iter 781700 Done. | loss1: 0.0030 | loss_class: 0.0024 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:20:22] 	mean_loss1: 0.07133402368574539
[06.27.21|03:20:22] 	mean_loss_class: 0.07068956505925128
[06.27.21|03:20:22] 	mean_loss_recon: 0.0006444585804644072
[06.27.21|03:20:22] Time consumption:
[06.27.21|03:20:22] Done.
[06.27.21|03:20:23] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch87_model1.pt.
[06.27.21|03:20:23] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch87_model2.pt.
[06.27.21|03:20:23] Training epoch: 88
[06.27.21|03:21:11] 	Iter 781800 Done. | loss1: 0.0302 | loss_class: 0.0293 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|03:22:07] 	Iter 781900 Done. | loss1: 0.0018 | loss_class: 0.0011 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:23:03] 	Iter 782000 Done. | loss1: 0.0660 | loss_class: 0.0654 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:24:01] 	Iter 782100 Done. | loss1: 0.0834 | loss_class: 0.0826 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|03:24:59] 	Iter 782200 Done. | loss1: 0.0116 | loss_class: 0.0110 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:25:57] 	Iter 782300 Done. | loss1: 0.0194 | loss_class: 0.0186 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|03:26:56] 	Iter 782400 Done. | loss1: 0.0314 | loss_class: 0.0308 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:27:51] 	Iter 782500 Done. | loss1: 0.0131 | loss_class: 0.0123 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:28:47] 	Iter 782600 Done. | loss1: 0.0404 | loss_class: 0.0397 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|03:29:43] 	Iter 782700 Done. | loss1: 0.0023 | loss_class: 0.0014 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|03:30:38] 	Iter 782800 Done. | loss1: 0.3738 | loss_class: 0.3733 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|03:31:35] 	Iter 782900 Done. | loss1: 0.0041 | loss_class: 0.0036 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|03:32:32] 	Iter 783000 Done. | loss1: 0.0228 | loss_class: 0.0222 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:33:27] 	Iter 783100 Done. | loss1: 0.0245 | loss_class: 0.0240 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:34:26] 	Iter 783200 Done. | loss1: 0.0248 | loss_class: 0.0240 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|03:35:23] 	Iter 783300 Done. | loss1: 0.0216 | loss_class: 0.0209 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:36:19] 	Iter 783400 Done. | loss1: 0.2928 | loss_class: 0.2922 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:37:14] 	Iter 783500 Done. | loss1: 0.0404 | loss_class: 0.0398 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:38:10] 	Iter 783600 Done. | loss1: 0.0064 | loss_class: 0.0056 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:39:06] 	Iter 783700 Done. | loss1: 0.0032 | loss_class: 0.0024 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|03:40:04] 	Iter 783800 Done. | loss1: 0.0348 | loss_class: 0.0341 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:41:01] 	Iter 783900 Done. | loss1: 0.3337 | loss_class: 0.3329 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|03:41:57] 	Iter 784000 Done. | loss1: 0.0034 | loss_class: 0.0029 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|03:42:52] 	Iter 784100 Done. | loss1: 0.0056 | loss_class: 0.0050 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:43:46] 	Iter 784200 Done. | loss1: 0.0045 | loss_class: 0.0038 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:44:42] 	Iter 784300 Done. | loss1: 0.0913 | loss_class: 0.0905 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|03:45:37] 	Iter 784400 Done. | loss1: 0.0194 | loss_class: 0.0187 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:46:33] 	Iter 784500 Done. | loss1: 0.0088 | loss_class: 0.0080 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|03:47:28] 	Iter 784600 Done. | loss1: 0.0477 | loss_class: 0.0472 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|03:48:23] 	Iter 784700 Done. | loss1: 0.1508 | loss_class: 0.1502 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:49:19] 	Iter 784800 Done. | loss1: 0.0065 | loss_class: 0.0058 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:50:18] 	Iter 784900 Done. | loss1: 0.3197 | loss_class: 0.3190 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:51:13] 	Iter 785000 Done. | loss1: 0.0115 | loss_class: 0.0109 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:52:12] 	Iter 785100 Done. | loss1: 0.0150 | loss_class: 0.0142 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:53:07] 	Iter 785200 Done. | loss1: 0.0216 | loss_class: 0.0210 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:54:03] 	Iter 785300 Done. | loss1: 0.0820 | loss_class: 0.0815 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:54:58] 	Iter 785400 Done. | loss1: 0.0102 | loss_class: 0.0092 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|03:55:55] 	Iter 785500 Done. | loss1: 0.1505 | loss_class: 0.1498 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:56:53] 	Iter 785600 Done. | loss1: 0.0020 | loss_class: 0.0014 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:57:49] 	Iter 785700 Done. | loss1: 0.0724 | loss_class: 0.0717 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|03:58:44] 	Iter 785800 Done. | loss1: 0.0073 | loss_class: 0.0066 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|03:59:39] 	Iter 785900 Done. | loss1: 0.0231 | loss_class: 0.0224 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:00:34] 	Iter 786000 Done. | loss1: 0.1723 | loss_class: 0.1717 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:01:30] 	Iter 786100 Done. | loss1: 0.7528 | loss_class: 0.7522 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:02:27] 	Iter 786200 Done. | loss1: 0.0013 | loss_class: 0.0007 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:03:21] 	Iter 786300 Done. | loss1: 0.0176 | loss_class: 0.0170 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:04:16] 	Iter 786400 Done. | loss1: 0.0142 | loss_class: 0.0136 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:05:12] 	Iter 786500 Done. | loss1: 0.0484 | loss_class: 0.0479 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:06:08] 	Iter 786600 Done. | loss1: 0.1964 | loss_class: 0.1957 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:07:04] 	Iter 786700 Done. | loss1: 0.0576 | loss_class: 0.0572 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:08:00] 	Iter 786800 Done. | loss1: 0.0306 | loss_class: 0.0302 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:08:57] 	Iter 786900 Done. | loss1: 0.1514 | loss_class: 0.1507 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|04:09:53] 	Iter 787000 Done. | loss1: 0.0100 | loss_class: 0.0095 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:10:48] 	Iter 787100 Done. | loss1: 0.1161 | loss_class: 0.1155 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:11:43] 	Iter 787200 Done. | loss1: 0.0058 | loss_class: 0.0051 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:12:38] 	Iter 787300 Done. | loss1: 0.0221 | loss_class: 0.0214 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:13:32] 	Iter 787400 Done. | loss1: 0.0081 | loss_class: 0.0075 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:14:28] 	Iter 787500 Done. | loss1: 0.1132 | loss_class: 0.1123 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|04:15:26] 	Iter 787600 Done. | loss1: 0.3518 | loss_class: 0.3512 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:16:23] 	Iter 787700 Done. | loss1: 0.0070 | loss_class: 0.0064 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:17:19] 	Iter 787800 Done. | loss1: 0.0081 | loss_class: 0.0072 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|04:18:14] 	Iter 787900 Done. | loss1: 0.0013 | loss_class: 0.0004 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|04:19:10] 	Iter 788000 Done. | loss1: 0.0231 | loss_class: 0.0223 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|04:20:08] 	Iter 788100 Done. | loss1: 0.0340 | loss_class: 0.0334 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:21:04] 	Iter 788200 Done. | loss1: 0.5551 | loss_class: 0.5542 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|04:22:00] 	Iter 788300 Done. | loss1: 0.0054 | loss_class: 0.0047 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:22:58] 	Iter 788400 Done. | loss1: 0.0341 | loss_class: 0.0335 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:23:57] 	Iter 788500 Done. | loss1: 0.0178 | loss_class: 0.0174 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:24:55] 	Iter 788600 Done. | loss1: 0.0045 | loss_class: 0.0039 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:25:52] 	Iter 788700 Done. | loss1: 0.6168 | loss_class: 0.6163 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:26:49] 	Iter 788800 Done. | loss1: 0.0067 | loss_class: 0.0059 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|04:27:44] 	Iter 788900 Done. | loss1: 0.0029 | loss_class: 0.0023 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:28:41] 	Iter 789000 Done. | loss1: 0.1247 | loss_class: 0.1237 | loss_recon: 0.0010 | lr: 0.001000
[06.27.21|04:29:37] 	Iter 789100 Done. | loss1: 0.0085 | loss_class: 0.0078 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:30:34] 	Iter 789200 Done. | loss1: 0.0020 | loss_class: 0.0013 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:31:29] 	Iter 789300 Done. | loss1: 0.0243 | loss_class: 0.0238 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:32:25] 	Iter 789400 Done. | loss1: 0.5694 | loss_class: 0.5689 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:33:21] 	Iter 789500 Done. | loss1: 0.0013 | loss_class: 0.0007 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:34:17] 	Iter 789600 Done. | loss1: 0.0048 | loss_class: 0.0041 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:35:12] 	Iter 789700 Done. | loss1: 0.0054 | loss_class: 0.0048 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:36:08] 	Iter 789800 Done. | loss1: 0.0169 | loss_class: 0.0163 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:37:05] 	Iter 789900 Done. | loss1: 0.0030 | loss_class: 0.0024 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:38:04] 	Iter 790000 Done. | loss1: 0.0021 | loss_class: 0.0015 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:38:59] 	Iter 790100 Done. | loss1: 0.0044 | loss_class: 0.0037 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:39:55] 	Iter 790200 Done. | loss1: 0.0072 | loss_class: 0.0064 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|04:40:51] 	Iter 790300 Done. | loss1: 0.2004 | loss_class: 0.1999 | loss_recon: 0.0004 | lr: 0.001000
[06.27.21|04:41:47] 	Iter 790400 Done. | loss1: 0.0041 | loss_class: 0.0033 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|04:42:44] 	Iter 790500 Done. | loss1: 0.0837 | loss_class: 0.0830 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:43:38] 	Iter 790600 Done. | loss1: 0.0090 | loss_class: 0.0085 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:44:34] 	Iter 790700 Done. | loss1: 0.0013 | loss_class: 0.0005 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|04:45:29] 	Iter 790800 Done. | loss1: 0.3411 | loss_class: 0.3405 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:46:24] 	Iter 790900 Done. | loss1: 0.0938 | loss_class: 0.0932 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:47:20] 	Iter 791000 Done. | loss1: 0.0608 | loss_class: 0.0602 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:48:16] 	Iter 791100 Done. | loss1: 0.0578 | loss_class: 0.0574 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:49:12] 	Iter 791200 Done. | loss1: 0.0213 | loss_class: 0.0207 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:50:07] 	Iter 791300 Done. | loss1: 0.0612 | loss_class: 0.0605 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:51:04] 	Iter 791400 Done. | loss1: 0.0189 | loss_class: 0.0184 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:52:01] 	Iter 791500 Done. | loss1: 0.0079 | loss_class: 0.0072 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|04:52:57] 	Iter 791600 Done. | loss1: 0.0098 | loss_class: 0.0093 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:53:51] 	Iter 791700 Done. | loss1: 0.2379 | loss_class: 0.2373 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:54:12] 	mean_loss1: 0.06970599625106819
[06.27.21|04:54:12] 	mean_loss_class: 0.06906074532922503
[06.27.21|04:54:12] 	mean_loss_recon: 0.0006452509139504079
[06.27.21|04:54:12] Time consumption:
[06.27.21|04:54:12] Done.
[06.27.21|04:54:12] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch88_model1.pt.
[06.27.21|04:54:12] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch88_model2.pt.
[06.27.21|04:54:12] Training epoch: 89
[06.27.21|04:54:48] 	Iter 791800 Done. | loss1: 0.0016 | loss_class: 0.0009 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:55:43] 	Iter 791900 Done. | loss1: 0.0271 | loss_class: 0.0265 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|04:56:41] 	Iter 792000 Done. | loss1: 0.1547 | loss_class: 0.1543 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:57:36] 	Iter 792100 Done. | loss1: 0.0755 | loss_class: 0.0751 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|04:58:32] 	Iter 792200 Done. | loss1: 0.0041 | loss_class: 0.0034 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|04:59:28] 	Iter 792300 Done. | loss1: 0.0066 | loss_class: 0.0059 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:00:24] 	Iter 792400 Done. | loss1: 0.0425 | loss_class: 0.0418 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:01:19] 	Iter 792500 Done. | loss1: 0.0064 | loss_class: 0.0058 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:02:13] 	Iter 792600 Done. | loss1: 0.0433 | loss_class: 0.0426 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:03:11] 	Iter 792700 Done. | loss1: 0.0390 | loss_class: 0.0383 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:04:07] 	Iter 792800 Done. | loss1: 0.0624 | loss_class: 0.0616 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|05:05:03] 	Iter 792900 Done. | loss1: 0.4471 | loss_class: 0.4465 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:06:00] 	Iter 793000 Done. | loss1: 0.0240 | loss_class: 0.0233 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:06:56] 	Iter 793100 Done. | loss1: 0.0026 | loss_class: 0.0022 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|05:07:51] 	Iter 793200 Done. | loss1: 0.0060 | loss_class: 0.0054 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:08:47] 	Iter 793300 Done. | loss1: 0.0074 | loss_class: 0.0067 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:09:42] 	Iter 793400 Done. | loss1: 0.0460 | loss_class: 0.0454 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:10:37] 	Iter 793500 Done. | loss1: 0.0640 | loss_class: 0.0631 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|05:11:32] 	Iter 793600 Done. | loss1: 0.2722 | loss_class: 0.2716 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:12:28] 	Iter 793700 Done. | loss1: 0.1347 | loss_class: 0.1340 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:13:25] 	Iter 793800 Done. | loss1: 0.0349 | loss_class: 0.0343 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:14:22] 	Iter 793900 Done. | loss1: 0.0248 | loss_class: 0.0240 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|05:15:18] 	Iter 794000 Done. | loss1: 0.0017 | loss_class: 0.0011 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:16:14] 	Iter 794100 Done. | loss1: 0.0066 | loss_class: 0.0059 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|05:17:08] 	Iter 794200 Done. | loss1: 0.0022 | loss_class: 0.0015 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:18:05] 	Iter 794300 Done. | loss1: 0.0455 | loss_class: 0.0447 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|05:18:59] 	Iter 794400 Done. | loss1: 0.0084 | loss_class: 0.0075 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|05:19:55] 	Iter 794500 Done. | loss1: 0.0246 | loss_class: 0.0239 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:20:51] 	Iter 794600 Done. | loss1: 0.0157 | loss_class: 0.0149 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|05:21:46] 	Iter 794700 Done. | loss1: 0.0233 | loss_class: 0.0227 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:22:42] 	Iter 794800 Done. | loss1: 0.1858 | loss_class: 0.1853 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:23:38] 	Iter 794900 Done. | loss1: 0.4045 | loss_class: 0.4038 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:24:34] 	Iter 795000 Done. | loss1: 0.0083 | loss_class: 0.0078 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|05:25:29] 	Iter 795100 Done. | loss1: 0.0041 | loss_class: 0.0034 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:26:25] 	Iter 795200 Done. | loss1: 0.0009 | loss_class: 0.0004 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|05:27:21] 	Iter 795300 Done. | loss1: 0.0023 | loss_class: 0.0016 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:28:18] 	Iter 795400 Done. | loss1: 0.0234 | loss_class: 0.0228 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:29:15] 	Iter 795500 Done. | loss1: 0.0136 | loss_class: 0.0129 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:30:10] 	Iter 795600 Done. | loss1: 0.0282 | loss_class: 0.0277 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|05:31:06] 	Iter 795700 Done. | loss1: 0.0230 | loss_class: 0.0222 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:32:01] 	Iter 795800 Done. | loss1: 0.0027 | loss_class: 0.0020 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|05:32:56] 	Iter 795900 Done. | loss1: 0.0085 | loss_class: 0.0079 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:33:51] 	Iter 796000 Done. | loss1: 0.0087 | loss_class: 0.0081 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:34:48] 	Iter 796100 Done. | loss1: 0.0089 | loss_class: 0.0082 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:35:43] 	Iter 796200 Done. | loss1: 0.0566 | loss_class: 0.0560 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:36:38] 	Iter 796300 Done. | loss1: 0.0596 | loss_class: 0.0588 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|05:37:34] 	Iter 796400 Done. | loss1: 0.1417 | loss_class: 0.1410 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:38:31] 	Iter 796500 Done. | loss1: 0.0045 | loss_class: 0.0038 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:39:27] 	Iter 796600 Done. | loss1: 0.3713 | loss_class: 0.3707 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:40:22] 	Iter 796700 Done. | loss1: 0.0753 | loss_class: 0.0746 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:41:17] 	Iter 796800 Done. | loss1: 0.0365 | loss_class: 0.0358 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:42:12] 	Iter 796900 Done. | loss1: 0.0045 | loss_class: 0.0037 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|05:43:09] 	Iter 797000 Done. | loss1: 0.0032 | loss_class: 0.0027 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|05:44:06] 	Iter 797100 Done. | loss1: 0.0022 | loss_class: 0.0016 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:45:01] 	Iter 797200 Done. | loss1: 0.0414 | loss_class: 0.0408 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:45:58] 	Iter 797300 Done. | loss1: 0.0222 | loss_class: 0.0218 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|05:46:53] 	Iter 797400 Done. | loss1: 0.0482 | loss_class: 0.0475 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:47:50] 	Iter 797500 Done. | loss1: 0.0193 | loss_class: 0.0187 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:48:46] 	Iter 797600 Done. | loss1: 0.1768 | loss_class: 0.1761 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:49:41] 	Iter 797700 Done. | loss1: 0.0238 | loss_class: 0.0233 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:50:37] 	Iter 797800 Done. | loss1: 0.0368 | loss_class: 0.0362 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:51:33] 	Iter 797900 Done. | loss1: 0.1919 | loss_class: 0.1912 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:52:31] 	Iter 798000 Done. | loss1: 0.0105 | loss_class: 0.0099 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:53:26] 	Iter 798100 Done. | loss1: 0.0022 | loss_class: 0.0016 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:54:21] 	Iter 798200 Done. | loss1: 0.0070 | loss_class: 0.0062 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|05:55:17] 	Iter 798300 Done. | loss1: 0.4517 | loss_class: 0.4513 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|05:56:11] 	Iter 798400 Done. | loss1: 0.1178 | loss_class: 0.1171 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|05:57:07] 	Iter 798500 Done. | loss1: 0.0044 | loss_class: 0.0034 | loss_recon: 0.0010 | lr: 0.001000
[06.27.21|05:58:04] 	Iter 798600 Done. | loss1: 0.0226 | loss_class: 0.0218 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|05:59:00] 	Iter 798700 Done. | loss1: 0.1685 | loss_class: 0.1679 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|05:59:56] 	Iter 798800 Done. | loss1: 0.0010 | loss_class: 0.0003 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|06:00:53] 	Iter 798900 Done. | loss1: 0.0341 | loss_class: 0.0336 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|06:01:49] 	Iter 799000 Done. | loss1: 0.0213 | loss_class: 0.0206 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|06:02:45] 	Iter 799100 Done. | loss1: 0.0059 | loss_class: 0.0053 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|06:03:39] 	Iter 799200 Done. | loss1: 0.0011 | loss_class: 0.0006 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|06:04:35] 	Iter 799300 Done. | loss1: 0.0187 | loss_class: 0.0183 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|06:05:32] 	Iter 799400 Done. | loss1: 0.1465 | loss_class: 0.1455 | loss_recon: 0.0010 | lr: 0.001000
[06.27.21|06:06:27] 	Iter 799500 Done. | loss1: 0.0055 | loss_class: 0.0048 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|06:07:22] 	Iter 799600 Done. | loss1: 0.0184 | loss_class: 0.0177 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|06:08:17] 	Iter 799700 Done. | loss1: 0.0112 | loss_class: 0.0105 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|06:09:13] 	Iter 799800 Done. | loss1: 0.0083 | loss_class: 0.0079 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|06:10:08] 	Iter 799900 Done. | loss1: 0.0027 | loss_class: 0.0020 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|06:11:02] 	Iter 800000 Done. | loss1: 0.0504 | loss_class: 0.0498 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|06:11:57] 	Iter 800100 Done. | loss1: 0.2779 | loss_class: 0.2773 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|06:12:50] 	Iter 800200 Done. | loss1: 0.3471 | loss_class: 0.3463 | loss_recon: 0.0008 | lr: 0.001000
[06.27.21|06:13:46] 	Iter 800300 Done. | loss1: 0.0092 | loss_class: 0.0086 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|06:14:41] 	Iter 800400 Done. | loss1: 0.0827 | loss_class: 0.0819 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|06:15:35] 	Iter 800500 Done. | loss1: 0.0099 | loss_class: 0.0091 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|06:16:32] 	Iter 800600 Done. | loss1: 0.0362 | loss_class: 0.0357 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|06:17:26] 	Iter 800700 Done. | loss1: 0.0029 | loss_class: 0.0023 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|06:18:23] 	Iter 800800 Done. | loss1: 0.0109 | loss_class: 0.0105 | loss_recon: 0.0004 | lr: 0.001000
[06.27.21|06:19:20] 	Iter 800900 Done. | loss1: 0.0249 | loss_class: 0.0240 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|06:20:15] 	Iter 801000 Done. | loss1: 0.0010 | loss_class: 0.0006 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|06:21:11] 	Iter 801100 Done. | loss1: 0.0738 | loss_class: 0.0729 | loss_recon: 0.0009 | lr: 0.001000
[06.27.21|06:22:06] 	Iter 801200 Done. | loss1: 0.0352 | loss_class: 0.0345 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|06:23:04] 	Iter 801300 Done. | loss1: 0.0226 | loss_class: 0.0219 | loss_recon: 0.0006 | lr: 0.001000
[06.27.21|06:24:01] 	Iter 801400 Done. | loss1: 0.0844 | loss_class: 0.0839 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|06:24:59] 	Iter 801500 Done. | loss1: 0.7543 | loss_class: 0.7536 | loss_recon: 0.0007 | lr: 0.001000
[06.27.21|06:25:54] 	Iter 801600 Done. | loss1: 0.0255 | loss_class: 0.0250 | loss_recon: 0.0005 | lr: 0.001000
[06.27.21|06:26:51] 	Iter 801700 Done. | loss1: 0.0268 | loss_class: 0.0263 | loss_recon: 0.0004 | lr: 0.001000
[06.27.21|06:27:24] 	mean_loss1: 0.06504687102751465
[06.27.21|06:27:24] 	mean_loss_class: 0.06440033657528123
[06.27.21|06:27:24] 	mean_loss_recon: 0.0006465344956191898
[06.27.21|06:27:24] Time consumption:
[06.27.21|06:27:24] Done.
[06.27.21|06:27:24] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch89_model1.pt.
[06.27.21|06:27:24] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch89_model2.pt.
[06.27.21|06:27:24] Eval epoch: 89
[06.27.21|06:34:07] 	mean_loss1: 0.7115437911549048
[06.27.21|06:34:07] 	mean_loss_class: 0.7111583929508924
[06.27.21|06:34:07] 	mean_loss_recon: 0.038539790714433954
[06.27.21|06:34:07] 

[06.27.21|06:34:07] 	Top1: 84.06%
[06.27.21|06:34:07] 

[06.27.21|06:34:07] 	Top5: 96.87%
[06.27.21|06:34:07] Done.
[06.27.21|06:34:07] Training epoch: 90
[06.27.21|06:34:30] 	Iter 801800 Done. | loss1: 0.0016 | loss_class: 0.0009 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|06:35:25] 	Iter 801900 Done. | loss1: 0.0292 | loss_class: 0.0285 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|06:36:22] 	Iter 802000 Done. | loss1: 0.0029 | loss_class: 0.0023 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|06:37:18] 	Iter 802100 Done. | loss1: 0.0020 | loss_class: 0.0012 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|06:38:14] 	Iter 802200 Done. | loss1: 0.0073 | loss_class: 0.0066 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|06:39:10] 	Iter 802300 Done. | loss1: 0.2647 | loss_class: 0.2642 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|06:40:06] 	Iter 802400 Done. | loss1: 0.0277 | loss_class: 0.0271 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|06:41:01] 	Iter 802500 Done. | loss1: 0.0165 | loss_class: 0.0157 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|06:41:56] 	Iter 802600 Done. | loss1: 0.0468 | loss_class: 0.0463 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|06:42:52] 	Iter 802700 Done. | loss1: 0.0135 | loss_class: 0.0131 | loss_recon: 0.0004 | lr: 0.000100
[06.27.21|06:43:48] 	Iter 802800 Done. | loss1: 0.0304 | loss_class: 0.0297 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|06:44:44] 	Iter 802900 Done. | loss1: 0.0072 | loss_class: 0.0067 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|06:45:40] 	Iter 803000 Done. | loss1: 0.0048 | loss_class: 0.0041 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|06:46:36] 	Iter 803100 Done. | loss1: 0.0091 | loss_class: 0.0083 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|06:47:33] 	Iter 803200 Done. | loss1: 0.0008 | loss_class: 0.0001 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|06:48:29] 	Iter 803300 Done. | loss1: 0.0398 | loss_class: 0.0393 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|06:49:25] 	Iter 803400 Done. | loss1: 0.0273 | loss_class: 0.0265 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|06:50:21] 	Iter 803500 Done. | loss1: 0.0037 | loss_class: 0.0031 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|06:51:18] 	Iter 803600 Done. | loss1: 0.1623 | loss_class: 0.1616 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|06:52:14] 	Iter 803700 Done. | loss1: 0.0737 | loss_class: 0.0731 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|06:53:09] 	Iter 803800 Done. | loss1: 0.0058 | loss_class: 0.0051 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|06:54:04] 	Iter 803900 Done. | loss1: 0.8416 | loss_class: 0.8410 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|06:54:59] 	Iter 804000 Done. | loss1: 0.0438 | loss_class: 0.0431 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|06:55:54] 	Iter 804100 Done. | loss1: 0.0648 | loss_class: 0.0643 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|06:56:50] 	Iter 804200 Done. | loss1: 0.0026 | loss_class: 0.0018 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|06:57:46] 	Iter 804300 Done. | loss1: 0.0593 | loss_class: 0.0586 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|06:58:41] 	Iter 804400 Done. | loss1: 0.0266 | loss_class: 0.0261 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|06:59:38] 	Iter 804500 Done. | loss1: 0.2033 | loss_class: 0.2027 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:00:32] 	Iter 804600 Done. | loss1: 0.0940 | loss_class: 0.0931 | loss_recon: 0.0009 | lr: 0.000100
[06.27.21|07:01:29] 	Iter 804700 Done. | loss1: 0.0381 | loss_class: 0.0374 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:02:25] 	Iter 804800 Done. | loss1: 0.0357 | loss_class: 0.0350 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:03:21] 	Iter 804900 Done. | loss1: 0.0688 | loss_class: 0.0679 | loss_recon: 0.0009 | lr: 0.000100
[06.27.21|07:04:15] 	Iter 805000 Done. | loss1: 0.1312 | loss_class: 0.1306 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:05:11] 	Iter 805100 Done. | loss1: 0.2185 | loss_class: 0.2179 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:06:09] 	Iter 805200 Done. | loss1: 0.0012 | loss_class: 0.0005 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:07:04] 	Iter 805300 Done. | loss1: 0.0028 | loss_class: 0.0022 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:07:59] 	Iter 805400 Done. | loss1: 0.0010 | loss_class: 0.0005 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|07:08:55] 	Iter 805500 Done. | loss1: 0.0083 | loss_class: 0.0078 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|07:09:50] 	Iter 805600 Done. | loss1: 0.0086 | loss_class: 0.0079 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:10:47] 	Iter 805700 Done. | loss1: 0.0140 | loss_class: 0.0132 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|07:11:42] 	Iter 805800 Done. | loss1: 0.0268 | loss_class: 0.0263 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|07:12:38] 	Iter 805900 Done. | loss1: 0.0034 | loss_class: 0.0026 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:13:33] 	Iter 806000 Done. | loss1: 0.0008 | loss_class: 0.0005 | loss_recon: 0.0004 | lr: 0.000100
[06.27.21|07:14:29] 	Iter 806100 Done. | loss1: 0.0021 | loss_class: 0.0016 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|07:15:26] 	Iter 806200 Done. | loss1: 0.0145 | loss_class: 0.0139 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:16:21] 	Iter 806300 Done. | loss1: 0.0687 | loss_class: 0.0681 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:17:18] 	Iter 806400 Done. | loss1: 0.0836 | loss_class: 0.0830 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:18:13] 	Iter 806500 Done. | loss1: 0.0360 | loss_class: 0.0354 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:19:09] 	Iter 806600 Done. | loss1: 0.0053 | loss_class: 0.0046 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:20:05] 	Iter 806700 Done. | loss1: 0.0666 | loss_class: 0.0662 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|07:21:00] 	Iter 806800 Done. | loss1: 0.0018 | loss_class: 0.0009 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|07:21:55] 	Iter 806900 Done. | loss1: 0.0111 | loss_class: 0.0104 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:22:52] 	Iter 807000 Done. | loss1: 0.0009 | loss_class: 0.0002 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:23:47] 	Iter 807100 Done. | loss1: 0.0015 | loss_class: 0.0008 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:24:42] 	Iter 807200 Done. | loss1: 0.0501 | loss_class: 0.0494 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:25:37] 	Iter 807300 Done. | loss1: 0.3893 | loss_class: 0.3885 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:26:32] 	Iter 807400 Done. | loss1: 0.0105 | loss_class: 0.0101 | loss_recon: 0.0004 | lr: 0.000100
[06.27.21|07:27:27] 	Iter 807500 Done. | loss1: 0.2946 | loss_class: 0.2939 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:28:23] 	Iter 807600 Done. | loss1: 0.0689 | loss_class: 0.0682 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:29:19] 	Iter 807700 Done. | loss1: 0.1381 | loss_class: 0.1373 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|07:30:16] 	Iter 807800 Done. | loss1: 0.0031 | loss_class: 0.0025 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:31:13] 	Iter 807900 Done. | loss1: 0.0021 | loss_class: 0.0013 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|07:32:08] 	Iter 808000 Done. | loss1: 0.0025 | loss_class: 0.0020 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|07:33:04] 	Iter 808100 Done. | loss1: 0.0125 | loss_class: 0.0119 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:34:02] 	Iter 808200 Done. | loss1: 0.0902 | loss_class: 0.0896 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:34:58] 	Iter 808300 Done. | loss1: 0.0019 | loss_class: 0.0010 | loss_recon: 0.0009 | lr: 0.000100
[06.27.21|07:35:54] 	Iter 808400 Done. | loss1: 0.0034 | loss_class: 0.0029 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|07:36:50] 	Iter 808500 Done. | loss1: 0.0103 | loss_class: 0.0097 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:37:45] 	Iter 808600 Done. | loss1: 0.0276 | loss_class: 0.0270 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:38:41] 	Iter 808700 Done. | loss1: 0.0124 | loss_class: 0.0119 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|07:39:37] 	Iter 808800 Done. | loss1: 0.0073 | loss_class: 0.0065 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|07:40:33] 	Iter 808900 Done. | loss1: 0.1781 | loss_class: 0.1774 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:41:28] 	Iter 809000 Done. | loss1: 0.0029 | loss_class: 0.0023 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:42:26] 	Iter 809100 Done. | loss1: 0.0055 | loss_class: 0.0048 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:43:24] 	Iter 809200 Done. | loss1: 0.1736 | loss_class: 0.1729 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|07:44:20] 	Iter 809300 Done. | loss1: 0.0031 | loss_class: 0.0025 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:45:16] 	Iter 809400 Done. | loss1: 0.0888 | loss_class: 0.0883 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|07:46:13] 	Iter 809500 Done. | loss1: 0.0145 | loss_class: 0.0137 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:47:09] 	Iter 809600 Done. | loss1: 0.1410 | loss_class: 0.1404 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:48:05] 	Iter 809700 Done. | loss1: 0.0145 | loss_class: 0.0139 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:49:02] 	Iter 809800 Done. | loss1: 0.0163 | loss_class: 0.0158 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|07:49:59] 	Iter 809900 Done. | loss1: 0.0792 | loss_class: 0.0783 | loss_recon: 0.0009 | lr: 0.000100
[06.27.21|07:50:55] 	Iter 810000 Done. | loss1: 0.0131 | loss_class: 0.0125 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:51:51] 	Iter 810100 Done. | loss1: 0.0054 | loss_class: 0.0047 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:52:48] 	Iter 810200 Done. | loss1: 0.0023 | loss_class: 0.0018 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|07:53:45] 	Iter 810300 Done. | loss1: 0.0123 | loss_class: 0.0115 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|07:54:41] 	Iter 810400 Done. | loss1: 0.0116 | loss_class: 0.0110 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|07:55:36] 	Iter 810500 Done. | loss1: 0.0147 | loss_class: 0.0141 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:56:31] 	Iter 810600 Done. | loss1: 0.0896 | loss_class: 0.0890 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:57:26] 	Iter 810700 Done. | loss1: 0.1992 | loss_class: 0.1984 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|07:58:22] 	Iter 810800 Done. | loss1: 0.0052 | loss_class: 0.0046 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|07:59:18] 	Iter 810900 Done. | loss1: 0.0019 | loss_class: 0.0013 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:00:14] 	Iter 811000 Done. | loss1: 0.0266 | loss_class: 0.0262 | loss_recon: 0.0004 | lr: 0.000100
[06.27.21|08:01:11] 	Iter 811100 Done. | loss1: 0.0186 | loss_class: 0.0180 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:02:09] 	Iter 811200 Done. | loss1: 0.0590 | loss_class: 0.0581 | loss_recon: 0.0009 | lr: 0.000100
[06.27.21|08:03:06] 	Iter 811300 Done. | loss1: 0.0343 | loss_class: 0.0338 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:04:02] 	Iter 811400 Done. | loss1: 0.0431 | loss_class: 0.0426 | loss_recon: 0.0004 | lr: 0.000100
[06.27.21|08:04:59] 	Iter 811500 Done. | loss1: 0.0064 | loss_class: 0.0056 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|08:05:54] 	Iter 811600 Done. | loss1: 0.0013 | loss_class: 0.0006 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|08:06:50] 	Iter 811700 Done. | loss1: 0.1401 | loss_class: 0.1395 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:07:35] 	mean_loss1: 0.05150935301564548
[06.27.21|08:07:35] 	mean_loss_class: 0.05086275024581748
[06.27.21|08:07:35] 	mean_loss_recon: 0.000646602769247181
[06.27.21|08:07:35] Time consumption:
[06.27.21|08:07:35] Done.
[06.27.21|08:07:36] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch90_model1.pt.
[06.27.21|08:07:36] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch90_model2.pt.
[06.27.21|08:07:36] Training epoch: 91
[06.27.21|08:07:47] 	Iter 811800 Done. | loss1: 0.0312 | loss_class: 0.0306 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:08:46] 	Iter 811900 Done. | loss1: 0.0042 | loss_class: 0.0036 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:09:43] 	Iter 812000 Done. | loss1: 0.1061 | loss_class: 0.1054 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|08:10:39] 	Iter 812100 Done. | loss1: 0.0039 | loss_class: 0.0032 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|08:11:34] 	Iter 812200 Done. | loss1: 0.1812 | loss_class: 0.1806 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:12:30] 	Iter 812300 Done. | loss1: 0.4642 | loss_class: 0.4629 | loss_recon: 0.0013 | lr: 0.000100
[06.27.21|08:13:26] 	Iter 812400 Done. | loss1: 0.0019 | loss_class: 0.0011 | loss_recon: 0.0009 | lr: 0.000100
[06.27.21|08:14:21] 	Iter 812500 Done. | loss1: 0.0040 | loss_class: 0.0035 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:15:18] 	Iter 812600 Done. | loss1: 0.0325 | loss_class: 0.0318 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:16:17] 	Iter 812700 Done. | loss1: 0.0257 | loss_class: 0.0251 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:17:14] 	Iter 812800 Done. | loss1: 0.0526 | loss_class: 0.0520 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:18:09] 	Iter 812900 Done. | loss1: 0.0014 | loss_class: 0.0006 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|08:19:04] 	Iter 813000 Done. | loss1: 0.2140 | loss_class: 0.2135 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:19:59] 	Iter 813100 Done. | loss1: 0.0009 | loss_class: 0.0004 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:20:54] 	Iter 813200 Done. | loss1: 0.0065 | loss_class: 0.0059 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:21:51] 	Iter 813300 Done. | loss1: 0.0089 | loss_class: 0.0084 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:22:46] 	Iter 813400 Done. | loss1: 0.0025 | loss_class: 0.0019 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:23:42] 	Iter 813500 Done. | loss1: 0.0583 | loss_class: 0.0579 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:24:39] 	Iter 813600 Done. | loss1: 0.0077 | loss_class: 0.0071 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|08:25:35] 	Iter 813700 Done. | loss1: 0.0224 | loss_class: 0.0218 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:26:31] 	Iter 813800 Done. | loss1: 0.0019 | loss_class: 0.0013 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:27:29] 	Iter 813900 Done. | loss1: 0.0286 | loss_class: 0.0280 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:28:27] 	Iter 814000 Done. | loss1: 0.0114 | loss_class: 0.0106 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|08:29:23] 	Iter 814100 Done. | loss1: 0.0040 | loss_class: 0.0035 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:30:18] 	Iter 814200 Done. | loss1: 0.0020 | loss_class: 0.0015 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:31:13] 	Iter 814300 Done. | loss1: 0.0021 | loss_class: 0.0016 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:32:10] 	Iter 814400 Done. | loss1: 0.0029 | loss_class: 0.0022 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:33:07] 	Iter 814500 Done. | loss1: 0.0106 | loss_class: 0.0099 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|08:34:04] 	Iter 814600 Done. | loss1: 0.0087 | loss_class: 0.0081 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:35:01] 	Iter 814700 Done. | loss1: 0.0183 | loss_class: 0.0177 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:35:58] 	Iter 814800 Done. | loss1: 0.0077 | loss_class: 0.0072 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:36:57] 	Iter 814900 Done. | loss1: 0.0222 | loss_class: 0.0216 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:37:56] 	Iter 815000 Done. | loss1: 0.0287 | loss_class: 0.0282 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:38:54] 	Iter 815100 Done. | loss1: 0.0706 | loss_class: 0.0700 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:39:50] 	Iter 815200 Done. | loss1: 0.0135 | loss_class: 0.0130 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:40:48] 	Iter 815300 Done. | loss1: 0.0136 | loss_class: 0.0131 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:41:46] 	Iter 815400 Done. | loss1: 0.0131 | loss_class: 0.0123 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|08:42:42] 	Iter 815500 Done. | loss1: 0.0322 | loss_class: 0.0315 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|08:43:38] 	Iter 815600 Done. | loss1: 0.0176 | loss_class: 0.0170 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:44:36] 	Iter 815700 Done. | loss1: 0.0267 | loss_class: 0.0260 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|08:45:32] 	Iter 815800 Done. | loss1: 0.0062 | loss_class: 0.0056 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:46:28] 	Iter 815900 Done. | loss1: 0.4132 | loss_class: 0.4123 | loss_recon: 0.0009 | lr: 0.000100
[06.27.21|08:47:23] 	Iter 816000 Done. | loss1: 0.0063 | loss_class: 0.0055 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|08:48:19] 	Iter 816100 Done. | loss1: 0.1484 | loss_class: 0.1478 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:49:13] 	Iter 816200 Done. | loss1: 0.0209 | loss_class: 0.0203 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:50:09] 	Iter 816300 Done. | loss1: 0.0138 | loss_class: 0.0130 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|08:51:06] 	Iter 816400 Done. | loss1: 0.0188 | loss_class: 0.0183 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:52:02] 	Iter 816500 Done. | loss1: 0.0008 | loss_class: 0.0002 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:52:57] 	Iter 816600 Done. | loss1: 0.0109 | loss_class: 0.0104 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:53:54] 	Iter 816700 Done. | loss1: 0.1024 | loss_class: 0.1021 | loss_recon: 0.0003 | lr: 0.000100
[06.27.21|08:54:51] 	Iter 816800 Done. | loss1: 0.0261 | loss_class: 0.0254 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|08:55:46] 	Iter 816900 Done. | loss1: 0.0007 | loss_class: 0.0001 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:56:42] 	Iter 817000 Done. | loss1: 0.0045 | loss_class: 0.0039 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|08:57:39] 	Iter 817100 Done. | loss1: 0.0189 | loss_class: 0.0182 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|08:58:35] 	Iter 817200 Done. | loss1: 0.0025 | loss_class: 0.0019 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|08:59:31] 	Iter 817300 Done. | loss1: 0.0029 | loss_class: 0.0024 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|09:00:26] 	Iter 817400 Done. | loss1: 0.0133 | loss_class: 0.0128 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|09:01:20] 	Iter 817500 Done. | loss1: 0.0162 | loss_class: 0.0157 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|09:02:16] 	Iter 817600 Done. | loss1: 0.0106 | loss_class: 0.0100 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:03:11] 	Iter 817700 Done. | loss1: 0.0013 | loss_class: 0.0007 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:04:06] 	Iter 817800 Done. | loss1: 0.0011 | loss_class: 0.0005 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:05:02] 	Iter 817900 Done. | loss1: 0.3495 | loss_class: 0.3489 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:05:57] 	Iter 818000 Done. | loss1: 0.0038 | loss_class: 0.0030 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|09:06:53] 	Iter 818100 Done. | loss1: 0.0046 | loss_class: 0.0039 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:07:49] 	Iter 818200 Done. | loss1: 0.0281 | loss_class: 0.0272 | loss_recon: 0.0009 | lr: 0.000100
[06.27.21|09:08:46] 	Iter 818300 Done. | loss1: 0.0645 | loss_class: 0.0639 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|09:09:40] 	Iter 818400 Done. | loss1: 0.0033 | loss_class: 0.0028 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|09:10:37] 	Iter 818500 Done. | loss1: 0.1883 | loss_class: 0.1876 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:11:34] 	Iter 818600 Done. | loss1: 0.0817 | loss_class: 0.0810 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:12:29] 	Iter 818700 Done. | loss1: 0.0996 | loss_class: 0.0989 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:13:25] 	Iter 818800 Done. | loss1: 0.0021 | loss_class: 0.0013 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|09:14:20] 	Iter 818900 Done. | loss1: 0.0139 | loss_class: 0.0133 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:15:15] 	Iter 819000 Done. | loss1: 0.0043 | loss_class: 0.0035 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|09:16:11] 	Iter 819100 Done. | loss1: 0.0041 | loss_class: 0.0034 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:17:06] 	Iter 819200 Done. | loss1: 0.0037 | loss_class: 0.0031 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:18:01] 	Iter 819300 Done. | loss1: 0.0879 | loss_class: 0.0872 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:18:57] 	Iter 819400 Done. | loss1: 0.0775 | loss_class: 0.0770 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|09:19:54] 	Iter 819500 Done. | loss1: 0.0080 | loss_class: 0.0071 | loss_recon: 0.0009 | lr: 0.000100
[06.27.21|09:20:50] 	Iter 819600 Done. | loss1: 0.0011 | loss_class: 0.0005 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:21:44] 	Iter 819700 Done. | loss1: 0.1029 | loss_class: 0.1023 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:22:39] 	Iter 819800 Done. | loss1: 0.0017 | loss_class: 0.0010 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:23:34] 	Iter 819900 Done. | loss1: 0.4053 | loss_class: 0.4047 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:24:31] 	Iter 820000 Done. | loss1: 0.0039 | loss_class: 0.0031 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|09:25:26] 	Iter 820100 Done. | loss1: 0.0404 | loss_class: 0.0398 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:26:21] 	Iter 820200 Done. | loss1: 0.0228 | loss_class: 0.0220 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|09:27:17] 	Iter 820300 Done. | loss1: 0.0053 | loss_class: 0.0046 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:28:14] 	Iter 820400 Done. | loss1: 0.0836 | loss_class: 0.0831 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|09:29:10] 	Iter 820500 Done. | loss1: 0.0350 | loss_class: 0.0344 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:30:05] 	Iter 820600 Done. | loss1: 0.0383 | loss_class: 0.0376 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:31:00] 	Iter 820700 Done. | loss1: 0.0033 | loss_class: 0.0028 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|09:31:57] 	Iter 820800 Done. | loss1: 0.0020 | loss_class: 0.0014 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|09:32:54] 	Iter 820900 Done. | loss1: 0.0023 | loss_class: 0.0017 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:33:51] 	Iter 821000 Done. | loss1: 0.0076 | loss_class: 0.0069 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:34:50] 	Iter 821100 Done. | loss1: 0.0361 | loss_class: 0.0353 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|09:35:46] 	Iter 821200 Done. | loss1: 0.0028 | loss_class: 0.0021 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:36:43] 	Iter 821300 Done. | loss1: 0.0224 | loss_class: 0.0216 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:37:38] 	Iter 821400 Done. | loss1: 0.0049 | loss_class: 0.0045 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|09:38:33] 	Iter 821500 Done. | loss1: 0.0171 | loss_class: 0.0164 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:39:29] 	Iter 821600 Done. | loss1: 0.1107 | loss_class: 0.1099 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|09:40:25] 	Iter 821700 Done. | loss1: 0.0056 | loss_class: 0.0049 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:41:21] 	Iter 821800 Done. | loss1: 0.0455 | loss_class: 0.0448 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:41:23] 	mean_loss1: 0.04608612154601209
[06.27.21|09:41:23] 	mean_loss_class: 0.0454399524953285
[06.27.21|09:41:23] 	mean_loss_recon: 0.0006461690991653777
[06.27.21|09:41:23] Time consumption:
[06.27.21|09:41:23] Done.
[06.27.21|09:41:23] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch91_model1.pt.
[06.27.21|09:41:23] The model has been saved as ./work_dir/recognition/ntu-xsub/AS_GCN/max_hop_4/lamda_05/epoch91_model2.pt.
[06.27.21|09:41:23] Training epoch: 92
[06.27.21|09:42:21] 	Iter 821900 Done. | loss1: 0.0213 | loss_class: 0.0207 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:43:20] 	Iter 822000 Done. | loss1: 0.2766 | loss_class: 0.2760 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:44:17] 	Iter 822100 Done. | loss1: 0.0077 | loss_class: 0.0070 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:45:16] 	Iter 822200 Done. | loss1: 0.0832 | loss_class: 0.0826 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:46:14] 	Iter 822300 Done. | loss1: 0.0261 | loss_class: 0.0255 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:47:12] 	Iter 822400 Done. | loss1: 0.0094 | loss_class: 0.0087 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:48:11] 	Iter 822500 Done. | loss1: 0.0129 | loss_class: 0.0123 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:49:08] 	Iter 822600 Done. | loss1: 0.0591 | loss_class: 0.0583 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|09:50:06] 	Iter 822700 Done. | loss1: 0.0156 | loss_class: 0.0151 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|09:51:03] 	Iter 822800 Done. | loss1: 0.0011 | loss_class: 0.0005 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:52:00] 	Iter 822900 Done. | loss1: 0.0080 | loss_class: 0.0073 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:52:58] 	Iter 823000 Done. | loss1: 0.0047 | loss_class: 0.0040 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:53:54] 	Iter 823100 Done. | loss1: 0.0559 | loss_class: 0.0550 | loss_recon: 0.0009 | lr: 0.000100
[06.27.21|09:54:49] 	Iter 823200 Done. | loss1: 0.0049 | loss_class: 0.0042 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:55:46] 	Iter 823300 Done. | loss1: 0.0101 | loss_class: 0.0094 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:56:42] 	Iter 823400 Done. | loss1: 0.0269 | loss_class: 0.0263 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:57:38] 	Iter 823500 Done. | loss1: 0.1077 | loss_class: 0.1071 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|09:58:34] 	Iter 823600 Done. | loss1: 0.0035 | loss_class: 0.0028 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|09:59:30] 	Iter 823700 Done. | loss1: 0.0204 | loss_class: 0.0198 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|10:00:25] 	Iter 823800 Done. | loss1: 0.0145 | loss_class: 0.0139 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|10:01:21] 	Iter 823900 Done. | loss1: 0.0122 | loss_class: 0.0114 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|10:02:17] 	Iter 824000 Done. | loss1: 0.0769 | loss_class: 0.0762 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|10:03:12] 	Iter 824100 Done. | loss1: 0.0011 | loss_class: 0.0005 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|10:04:08] 	Iter 824200 Done. | loss1: 0.0037 | loss_class: 0.0032 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|10:05:05] 	Iter 824300 Done. | loss1: 0.0257 | loss_class: 0.0248 | loss_recon: 0.0009 | lr: 0.000100
[06.27.21|10:06:01] 	Iter 824400 Done. | loss1: 0.0264 | loss_class: 0.0256 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|10:06:58] 	Iter 824500 Done. | loss1: 0.0032 | loss_class: 0.0026 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|10:07:54] 	Iter 824600 Done. | loss1: 0.0021 | loss_class: 0.0014 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|10:08:51] 	Iter 824700 Done. | loss1: 0.0107 | loss_class: 0.0101 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|10:09:46] 	Iter 824800 Done. | loss1: 0.0103 | loss_class: 0.0098 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|10:10:41] 	Iter 824900 Done. | loss1: 0.0114 | loss_class: 0.0108 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|10:11:38] 	Iter 825000 Done. | loss1: 0.0016 | loss_class: 0.0009 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|10:12:35] 	Iter 825100 Done. | loss1: 0.0083 | loss_class: 0.0079 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|10:13:31] 	Iter 825200 Done. | loss1: 0.0047 | loss_class: 0.0039 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|10:14:28] 	Iter 825300 Done. | loss1: 0.0072 | loss_class: 0.0068 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|10:15:27] 	Iter 825400 Done. | loss1: 0.0127 | loss_class: 0.0122 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|10:16:24] 	Iter 825500 Done. | loss1: 0.0049 | loss_class: 0.0041 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|10:17:19] 	Iter 825600 Done. | loss1: 0.0707 | loss_class: 0.0700 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|10:18:18] 	Iter 825700 Done. | loss1: 0.0147 | loss_class: 0.0140 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|10:19:15] 	Iter 825800 Done. | loss1: 0.0080 | loss_class: 0.0075 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|10:20:11] 	Iter 825900 Done. | loss1: 0.0030 | loss_class: 0.0025 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|10:21:07] 	Iter 826000 Done. | loss1: 0.0061 | loss_class: 0.0055 | loss_recon: 0.0005 | lr: 0.000100
[06.27.21|10:22:04] 	Iter 826100 Done. | loss1: 0.4124 | loss_class: 0.4117 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|10:23:02] 	Iter 826200 Done. | loss1: 0.2527 | loss_class: 0.2520 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|10:23:59] 	Iter 826300 Done. | loss1: 0.0111 | loss_class: 0.0104 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|10:24:58] 	Iter 826400 Done. | loss1: 0.0086 | loss_class: 0.0079 | loss_recon: 0.0008 | lr: 0.000100
[06.27.21|10:25:56] 	Iter 826500 Done. | loss1: 0.0058 | loss_class: 0.0054 | loss_recon: 0.0004 | lr: 0.000100
[06.27.21|10:26:53] 	Iter 826600 Done. | loss1: 0.0677 | loss_class: 0.0667 | loss_recon: 0.0009 | lr: 0.000100
[06.27.21|10:27:52] 	Iter 826700 Done. | loss1: 0.0025 | loss_class: 0.0019 | loss_recon: 0.0006 | lr: 0.000100
[06.27.21|10:28:52] 	Iter 826800 Done. | loss1: 0.0045 | loss_class: 0.0039 | loss_recon: 0.0007 | lr: 0.000100
[06.27.21|10:29:54] 	Iter 826900 Done. | loss1: 0.0252 | loss_class: 0.0245 | loss_recon: 0.0007 | lr: 0.000100
Traceback (most recent call last):
  File "/root/AS-GCN/main.py", line 24, in <module>
    p.start()
  File "/root/AS-GCN/processor/processor.py", line 111, in start
    self.train(training_A=False)
  File "/root/AS-GCN/processor/recognition.py", line 168, in train
    self.optimizer1.step()
  File "/root/anaconda3/envs/asgcn/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/root/anaconda3/envs/asgcn/lib/python3.6/site-packages/torch/optim/sgd.py", line 95, in step
    if p.grad is None:
  File "/root/anaconda3/envs/asgcn/lib/python3.6/site-packages/torch/tensor.py", line 942, in grad
    from torch.overrides import has_torch_function, handle_torch_function
  File "<frozen importlib._bootstrap>", line 1007, in _handle_fromlist
KeyboardInterrupt

Process finished with exit code 1

```
